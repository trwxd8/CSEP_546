Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: False
train0 0.11362192958039061 greater than 1
test0 0.09916885225671472
train5 0.060120966477193824 greater than 0.06290907162948257
test5 0.06783277099451447
train10 0.05262092697371998 greater than 0.05365963918960336
test10 0.05293643687958985
train15 0.04879231765016945 greater than 0.04942990688738643
test15 0.04590431326353579
train20 0.04597402508852353 greater than 0.046521104478829976
test20 0.04144553124653822
train25 0.04317991187003822 greater than 0.04374705980025453
test25 0.03747450987598496
train30 0.04047515092527197 greater than 0.040983820502062784
test30 0.03547228995354684
train35 0.03806162589963933 greater than 0.03852984965770146
test35 0.03426156140501025
train40 0.03611879344384053 greater than 0.03645866767129946
test40 0.032428326578672305
train45 0.03464381701776997 greater than 0.034906248398438984
test45 0.03168537045790979
train50 0.03376825574527935 greater than 0.03389435970774264
test50 0.031984420604502746
train55 0.03270681496706358 greater than 0.03289258911971735
test55 0.03247638540353772
train60 0.03171271071307206 greater than 0.031893176850955385
test60 0.0326612802261135
train65 0.031080513151136625 greater than 0.031202475172012877
test65 0.0325910834321947
train70 0.030633094494227877 greater than 0.03075086352292146
test70 0.0337158273404368
train75 0.02974490224569285 greater than 0.029889884524218632
test75 0.03401364559757597
train80 0.0290709400965133 greater than 0.02923797551803861
test80 0.034488473842608676
train85 0.028108470808490063 greater than 0.028292961182721083
test85 0.03576919977385557
train90 0.027408691330274616 greater than 0.02751248974521019
test90 0.03595497644896122
train95 0.02678973448876701 greater than 0.026945621770306852
test95 0.03558017533291811
train100 0.02614343051835713 greater than 0.02625781157408724
test100 0.03526970425822092
train105 0.025734128634336814 greater than 0.02585242881673071
test105 0.03474074176479188
train110 0.02507164689308139 greater than 0.025239600923144628
test110 0.033586349162158406
train114 0.025005581636674414 greater than 0.024753519802142618
test114 0.031100111016537854
0 Accuracy:0.9240924092409241 Lower Bound:0.9091814677315778 Upper Bound:0.9390033507502703
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: True
train0 0.11362192958039061 greater than 1
test0 0.09916885225671472
train5 0.060381291024811455 greater than 0.06360880514656549
test5 0.07314060333304212
train10 0.05216098534891284 greater than 0.0534077056802823
test10 0.04852321618079239
train15 0.0472756578180607 greater than 0.048191343760747886
test15 0.040149900958644257
train20 0.04256681709020597 greater than 0.04338958263004726
test20 0.03718453653000284
train25 0.04010107636941584 greater than 0.04060306118832656
test25 0.032003672221885014
train30 0.03731965274696716 greater than 0.037815074001046126
test30 0.031464210862927215
train35 0.03553642430046112 greater than 0.035902356627349065
test35 0.03343864573167523
train40 0.03376656974098353 greater than 0.0342051233901009
test40 0.03326549160035103
train45 0.03179614612567316 greater than 0.03218991055990182
test45 0.03496018758184326
train50 0.03044432954670912 greater than 0.030765807473721218
test50 0.03009137681878763
train53 0.030259403973503213 greater than 0.030108166759288152
test53 0.03343076987650184
0 Accuracy:0.9141914191419142 Lower Bound:0.8984229805385732 Upper Bound:0.9299598577452551
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: False
train0 0.11888053901788531 greater than 1
test0 0.1021323467573416
train5 0.06385331507834865 greater than 0.06712629405976088
test5 0.07481112696455144
train10 0.05523891779984716 greater than 0.056439949160909095
test10 0.060308135034055285
train15 0.050741178711895686 greater than 0.05150114192282642
test15 0.0515114876766764
train20 0.04753323197170757 greater than 0.04811152878810227
test20 0.04579500267248506
train25 0.04473876628794281 greater than 0.04531083037075008
test25 0.040482986909325114
train30 0.04186280001068411 greater than 0.042419826696901315
test30 0.03669235827617147
train35 0.03929816509915859 greater than 0.039779012610533675
test35 0.0350171076592828
train40 0.037092941164772915 greater than 0.037453868320225625
test40 0.03432789354104589
train45 0.035505224184838556 greater than 0.035793587700038806
test45 0.03414302236338923
train50 0.034117196314820145 greater than 0.03440043399517459
test50 0.033765817315735545
train55 0.03291821557706565 greater than 0.03314870545014109
test55 0.03324283732452228
train60 0.03185929249404081 greater than 0.032054046620955415
test60 0.03285808852198031
train65 0.030898240352836125 greater than 0.031076821643829647
test65 0.03251019414567226
train70 0.03000602411411954 greater than 0.030194156683593352
test70 0.032291826329571144
train75 0.029106300017118423 greater than 0.029276709115760167
test75 0.0324033828170211
train80 0.02830918609083098 greater than 0.028452651430772358
test80 0.03264458115859129
train85 0.02772479956360038 greater than 0.027792624889156983
test85 0.03272660255160361
train88 0.027646843928062357 greater than 0.027638120382472327
test88 0.0323529775110812
1 Accuracy:0.9183168316831684 Lower Bound:0.902897437139886 Upper Bound:0.9337362262264507
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: True
train0 0.11888053901788531 greater than 1
test0 0.1021323467573416
train5 0.0711829688844932 greater than 0.07504146052962671
test5 0.07918142750781934
train10 0.05957006120622366 greater than 0.06099037171550381
test10 0.06157594322383523
train15 0.054166041706652214 greater than 0.05493882359130236
test15 0.05052652588056739
train20 0.05094978321784205 greater than 0.05130299955562008
test20 0.04546396493168758
train25 0.04854010300071287 greater than 0.04906435189068543
test25 0.045280162505089934
train30 0.04716260495874708 greater than 0.047351261119729295
test30 0.05255201788700396
train35 0.04580597595753494 greater than 0.045829520613076956
test35 0.06116842731765584
train40 0.04357935098217023 greater than 0.0444750488149472
test40 0.04802789680160734
train45 0.042166020625249705 greater than 0.042207588410837625
test45 0.04669504384682031
train50 0.039983132407004046 greater than 0.040029836771026825
test50 0.04970832007667892
train52 0.039874391056113664 greater than 0.03977734957585655
test52 0.05045326622047279
1 Accuracy:0.8696369636963697 Lower Bound:0.8506807764245813 Upper Bound:0.888593150968158
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: False
train0 0.12430222725292164 greater than 1
test0 0.1129436251929761
train5 0.0668932372557985 greater than 0.07118747087168598
test5 0.08061909380898417
train10 0.057020381806782 greater than 0.05832551557801149
test10 0.06984448605963209
train15 0.052298979268157846 greater than 0.05309070639523756
test15 0.05957956700203294
train20 0.04887659563469635 greater than 0.049512495471741184
test20 0.051707917280252126
train25 0.04586281643623496 greater than 0.04645178739662502
test25 0.04578108357700266
train30 0.0429045427374631 greater than 0.04345806941854838
test30 0.04099552616480138
train35 0.04042037913396842 greater than 0.040912219666489676
test35 0.03760758073526037
train40 0.038216753478668555 greater than 0.038629682903393914
test40 0.03529923430415792
train45 0.036265620244493015 greater than 0.03663507594702184
test45 0.033664541431115885
train50 0.03449990021709189 greater than 0.03486788625850126
test50 0.03248183185996222
train55 0.032818495424729706 greater than 0.03310138242768629
test55 0.031428175999932764
train60 0.031704792673923 greater than 0.0318877960938208
test60 0.030899020940401627
train65 0.030939324526562756 greater than 0.031096236884573993
test65 0.030763112815223797
train70 0.03033206544261663 greater than 0.030446572200319596
test70 0.030608368390571693
train75 0.02901123541659907 greater than 0.029242899470892113
test75 0.030616141137883204
train80 0.02825340284287561 greater than 0.028376755261159257
test80 0.030462233300457702
train85 0.027284926245159335 greater than 0.027487084430442742
test85 0.030509070842549578
train90 0.026310291280863183 greater than 0.026436911362342018
test90 0.03103148582147356
train91 0.026370751593229286 greater than 0.026310291280863183
test91 0.031133662104721155
2 Accuracy:0.91996699669967 Lower Bound:0.9046904411913306 Upper Bound:0.9352435522080094
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: True
train0 0.12430222725292164 greater than 1
test0 0.1129436251929761
train5 0.06833202586693402 greater than 0.07364199746174695
test5 0.085109574147706
train10 0.05710465570704982 greater than 0.058344403785032875
test10 0.06581914945938186
train15 0.051976924935468904 greater than 0.052849696133227195
test15 0.058870561147526014
train20 0.048589044818007895 greater than 0.04924661524112685
test20 0.05298923628373462
train25 0.04517277507044906 greater than 0.04587792672824588
test25 0.04809228583180437
train30 0.042591678831996764 greater than 0.04301856330928661
test30 0.04574069476781608
train35 0.03992643232445392 greater than 0.04052870835081301
test35 0.04168379736213893
train40 0.03766724837768887 greater than 0.037948540479040595
test40 0.036958053957623055
train45 0.035847452864824526 greater than 0.03631756237121036
test45 0.03428640320492003
train50 0.03361503202682206 greater than 0.033948418277101614
test50 0.0323289223547702
train55 0.03102205576441195 greater than 0.03160502209944422
test55 0.03106567548898307
train58 0.030489730357951862 greater than 0.030390786684439983
test58 0.03114555976462305
2 Accuracy:0.91996699669967 Lower Bound:0.9046904411913306 Upper Bound:0.9352435522080094
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: False
train0 0.12418000586121143 greater than 1
test0 0.11158490463462199
train5 0.06809821403369258 greater than 0.07241033237665885
test5 0.08538919911700513
train10 0.05826258044169735 greater than 0.05948777894048256
test10 0.07209608010297795
train15 0.05400967575428156 greater than 0.05470460642120043
test15 0.06367699043637684
train20 0.05123340357943124 greater than 0.051711790300535035
test20 0.057818527999555794
train25 0.04925621391915096 greater than 0.04960537119645316
test25 0.05413329114602164
train30 0.0477487988946472 greater than 0.048024308841178275
test30 0.05170337106981959
train35 0.04650008059589566 greater than 0.04673583052807622
test35 0.0499727549486146
train40 0.045391372734872024 greater than 0.045606065288253415
test40 0.048687549165251746
train45 0.044338473395848066 greater than 0.04454768616613922
test45 0.04767536245066012
train50 0.04330552381989341 greater than 0.04350994672507376
test50 0.04672380190865766
train55 0.04226517476769414 greater than 0.042479730500068834
test55 0.04569290495956671
train60 0.041114771953351635 greater than 0.04135444412293234
test60 0.04457988576360909
train65 0.03986609431018314 greater than 0.04012373535904597
test65 0.043360441680189266
train70 0.038507630913779824 greater than 0.038786180931856984
test70 0.042124545119542695
train75 0.03706849072446832 greater than 0.037366943851300315
test75 0.040810513293670844
train80 0.03547744741224464 greater than 0.03579340903740858
test80 0.03914042889714333
train85 0.034035976537903974 greater than 0.03429949953986014
test85 0.03775522783596167
train90 0.032625087890885654 greater than 0.032939091027036654
test90 0.037475898724750545
train95 0.030992951602893525 greater than 0.03131051071354195
test95 0.03759414264833683
train100 0.02949663009869685 greater than 0.02978647110266902
test100 0.037031482131152337
train105 0.028113743383721707 greater than 0.028373050783051655
test105 0.03665751127049558
train110 0.02703563872666356 greater than 0.02722853004747251
test110 0.0364840490357162
train115 0.02607844943569418 greater than 0.026273223988922425
test115 0.03645118334242202
train120 0.025136490531848764 greater than 0.025318548668845007
test120 0.03621888063504226
train125 0.024239174491762525 greater than 0.024424442155462724
test125 0.03570724121158522
train130 0.023198756724895655 greater than 0.02340908844139396
test130 0.03477916431329044
train135 0.022353116521682592 greater than 0.0225091558745286
test135 0.03443058683826595
train140 0.021576576029469708 greater than 0.021701168973526343
test140 0.03422656185142624
train145 0.021058612543411875 greater than 0.021148572143194152
test145 0.03446874078779021
train150 0.020499151384269252 greater than 0.02069363140027922
test150 0.03501240274315675
train155 0.01953600884944262 greater than 0.01975274099862851
test155 0.03601816853498749
train160 0.01905809674132701 greater than 0.019131008132761914
test160 0.037385603378881034
train165 0.018779741314615614 greater than 0.018802173522360414
test165 0.038918576694352405
train170 0.018434657195857126 greater than 0.018528405049284876
test170 0.03981346498119113
train173 0.01830976668130217 greater than 0.01827717504372457
test173 0.039765343512753314
3 Accuracy:0.9034653465346535 Lower Bound:0.886838796941569 Upper Bound:0.920091896127738
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: True
train0 0.12418000586121143 greater than 1
test0 0.11158490463462199
train5 0.06805864629593728 greater than 0.07237280485887683
test5 0.08573268021675912
train10 0.05821970954301138 greater than 0.059432324261055366
test10 0.07205829584856945
train15 0.05395654880475866 greater than 0.054653824561595475
test15 0.06364613113826614
train20 0.05118587992188146 greater than 0.051661624166293066
test20 0.0578448525601988
train25 0.04922333799838495 greater than 0.04956976673119542
test25 0.054214321351796865
train30 0.04772536618670906 greater than 0.04799959822432069
test30 0.05180897128129737
train35 0.0464789232128735 greater than 0.0467147023002509
test35 0.05008896569157476
train40 0.04536722342464128 greater than 0.04558283481021722
test40 0.04881758331357648
train45 0.04430819787764336 greater than 0.044518894655099374
test45 0.047829023341487635
train50 0.043265991022740695 greater than 0.04347231790021813
test50 0.046898321031456414
train55 0.04221154667977801 greater than 0.042430503412953784
test55 0.04584466941296455
train60 0.04101709876022084 greater than 0.041268105820748344
test60 0.04460602986904879
train65 0.03969601638304475 greater than 0.03997061089646481
test65 0.04336414708976488
train70 0.03826135230263172 greater than 0.0385548943584927
test70 0.042389120025086255
train75 0.0367379865713135 greater than 0.03705685498161651
test75 0.04106089035413955
train80 0.03509053408646419 greater than 0.035410753601572524
test80 0.03928275473180343
train85 0.03363933584096828 greater than 0.03392893967301795
test85 0.03820578097953989
train90 0.031969052846643684 greater than 0.03232726709552347
test90 0.038307157048537696
train95 0.030301096386398794 greater than 0.030617791276770903
test95 0.037606256604605856
train100 0.02883757251958464 greater than 0.029117141851896258
test100 0.03678058126614801
train105 0.027523805589786208 greater than 0.027772109209359665
test105 0.036213713775737545
train110 0.026406812878271752 greater than 0.02661719346107454
test110 0.03602425623294603
train115 0.025354910053327333 greater than 0.025566911879526148
test115 0.03604943231902915
train120 0.02432164689800436 greater than 0.024524083943426554
test120 0.03619558685604577
train125 0.023378891885537552 greater than 0.02355496145056805
test125 0.03623122268859485
train130 0.022570974289072367 greater than 0.02272769145714958
test130 0.0359778870332142
train135 0.02181220743562492 greater than 0.021954181239339184
test135 0.03547545607705849
train140 0.021083355344577857 greater than 0.02125932790468367
test140 0.03494183480845564
train145 0.02013778899059728 greater than 0.020281809988318685
test145 0.034428503064221676
train147 0.02053419299613801 greater than 0.020089916919890767
test147 0.034413531213885254
3 Accuracy:0.9158415841584159 Lower Bound:0.9002114136085113 Upper Bound:0.9314717547083204
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: False
train0 0.12620342862114994 greater than 1
test0 0.11836131463430115
train5 0.07506717280756935 greater than 0.08010315594435419
test5 0.06110862917499599
train10 0.06279604031503977 greater than 0.06428214133065865
test10 0.0515899163715206
train15 0.05786984729767588 greater than 0.05864521944880531
test15 0.04841234874286198
train20 0.054827878895880436 greater than 0.05534862720346738
test20 0.0466024768508534
train25 0.05263817728811991 greater than 0.053032504197094946
test25 0.04522553837971106
train30 0.05089078665342138 greater than 0.05121435287433095
test30 0.04412915140479889
train35 0.04943032687973126 greater than 0.04970346931821058
test35 0.0432537305829875
train40 0.04817497263483094 greater than 0.04841350355857898
test40 0.04256396247194548
train45 0.04704479686350656 greater than 0.04726454123155109
test45 0.042035611510742504
train50 0.04596428765856341 greater than 0.04617961113850795
test50 0.041626913673371985
train55 0.0448887678236376 greater than 0.045101579784938776
test55 0.04124749821310807
train60 0.04389779444968645 greater than 0.044086309427463145
test60 0.04082027455901803
train65 0.043007436949274225 greater than 0.04317976145823719
test65 0.040212160064078996
train70 0.04215458495332307 greater than 0.04232833575416739
test70 0.03913806577571495
train75 0.04113293153568247 greater than 0.04136225119488614
test75 0.037643743120030285
train80 0.03987722513991629 greater than 0.04013668926166568
test80 0.036157875201672046
train85 0.038620161750564305 greater than 0.03886714553933783
test85 0.03476006108538435
train90 0.03752531021058295 greater than 0.03770920572414199
test90 0.03368156970542234
train95 0.03633983810941289 greater than 0.03664949923380367
test95 0.03293972160035146
train100 0.0345793012115 greater than 0.034946058205872245
test100 0.03244261909465344
train105 0.03280164788653917 greater than 0.03310527751346768
test105 0.03213686950262526
train110 0.031457560379103085 greater than 0.03171750776024516
test110 0.03201331373173947
train115 0.03009777792348588 greater than 0.030374664537430384
test115 0.03182401836055239
train120 0.028730115007282703 greater than 0.029014038852248333
test120 0.03157711228756725
train125 0.027424313967855385 greater than 0.02765252129193322
test125 0.03147305248521482
train130 0.026396955957800866 greater than 0.026622281756540533
test130 0.031014177364348007
train135 0.025103190139683228 greater than 0.025380496363244884
test135 0.030594732850451088
train140 0.023628549698299126 greater than 0.023889266075336004
test140 0.03130805483735336
train145 0.022618981462554873 greater than 0.022803833069968433
test145 0.03337708347555938
train150 0.021912263615712697 greater than 0.021977979899053934
test150 0.03411835334508593
train155 0.021090944307439335 greater than 0.02125541401285787
test155 0.033760294819415386
train160 0.02026662192885314 greater than 0.020435902312622094
test160 0.03459647239059927
train165 0.019598399931735483 greater than 0.0196751334659023
test165 0.0353788778883842
train170 0.01918122671535034 greater than 0.01927739113338055
test170 0.03512678123646246
train175 0.01881233281751238 greater than 0.0188572454946911
test175 0.03495653234313693
train176 0.01898167875954134 greater than 0.01881233281751238
test176 0.0355688146876977
4 Accuracy:0.9051155115511551 Lower Bound:0.8886166349039726 Upper Bound:0.9216143881983376
Loss calculated for layer count:1 node count: 20 stepSize:0.2 Momentum: True
train0 0.12620342862114994 greater than 1
test0 0.11836131463430115
train5 0.07500778512754004 greater than 0.08039360923205557
test5 0.05871914953882647
train10 0.06239503521977925 greater than 0.06384139042048666
test10 0.05061240722161941
train15 0.05737993692021875 greater than 0.058176690885584983
test15 0.047645610905097786
train20 0.054174976452862844 greater than 0.054741468203824833
test20 0.045665196216716206
train25 0.05167194042631079 greater than 0.05214371162798142
test25 0.04387780031712777
train30 0.04925207464175632 greater than 0.0497639533431161
test30 0.04169168696340213
train35 0.04650925729930949 greater than 0.04705183727254122
test35 0.03878050265938896
train40 0.044070101133130875 greater than 0.0445449508251359
test40 0.03631488512368837
train45 0.04154144391141807 greater than 0.04200299330610976
test45 0.03463913873114359
train50 0.03991866278644763 greater than 0.0400933208505909
test50 0.03381285336169948
train55 0.03767220465141544 greater than 0.03823009332642657
test55 0.033198381288830285
train60 0.035952943668212806 greater than 0.03617877928394891
test60 0.03266318824367992
train65 0.03521333481168228 greater than 0.03540902384008659
test65 0.03225782010568531
train67 0.035435656238669476 greater than 0.03511213644305226
test67 0.03218530096653958
4 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
