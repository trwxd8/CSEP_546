Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: False
train0 0.1231279511496012 greater than 1
test0 0.11309029086263432
train5 0.06257565929310736 greater than 0.06603161919043365
test5 0.06830223066545338
train10 0.05472166520731114 greater than 0.0557460169743612
test10 0.05600328302403202
train15 0.051078162326785664 greater than 0.05166418856320116
test15 0.049188321564302584
train20 0.04881727654748855 greater than 0.04920390129501428
test20 0.04562446431862588
train25 0.04721021006271981 greater than 0.04749661419642787
test25 0.04367452664593643
train30 0.04592245623092829 greater than 0.04616744572240839
test30 0.04202969710356568
train35 0.04473645425527797 greater than 0.04496917998679205
test35 0.040514788674742605
train40 0.043638391351565935 greater than 0.04384633194208454
test40 0.039218586507044545
train45 0.04268465345017442 greater than 0.04286942801856999
test45 0.03816236223151027
train50 0.04172620454127733 greater than 0.04192119129608325
test50 0.03747974910489641
train55 0.04066909008574401 greater than 0.040900330756211016
test55 0.036725372673770414
train60 0.03925557032889588 greater than 0.03957000285327699
test60 0.03518305264776245
train65 0.03765116800632195 greater than 0.03796743835409418
test65 0.03359151877878362
train70 0.036155701407076545 greater than 0.03643591425197837
test70 0.032722268036089304
train75 0.03490055671128361 greater than 0.035144796084276654
test75 0.03219395686130714
train80 0.03379401243504732 greater than 0.03399721824803056
test80 0.031855728724075355
train85 0.03289563191136477 greater than 0.03305534946251687
test85 0.031596651249915744
train90 0.032197868398884895 greater than 0.03232797345776027
test90 0.0312546397420172
train95 0.031583367324921445 greater than 0.03170355174733259
test95 0.03106092722829929
train100 0.0309937630619477 greater than 0.031109733489352486
test100 0.031317879162659414
train105 0.03045833776068728 greater than 0.030559790299544188
test105 0.031919743202255324
train110 0.029973975132050565 greater than 0.03006519459657671
test110 0.03252795374228264
train115 0.02955737608285211 greater than 0.029638139493145137
test115 0.03301371490218292
train120 0.029163076980018063 greater than 0.029239261386937106
test120 0.033466854853932394
train125 0.028807930274642013 greater than 0.028876156771939696
test125 0.03395131820199725
train130 0.028480408247909428 greater than 0.028546950377564023
test130 0.03441778074105135
train135 0.02809821874085639 greater than 0.028175177052460122
test135 0.03487199151220567
train140 0.027697579313520623 greater than 0.02777843386789525
test140 0.035508582155456686
train145 0.027337729056386952 greater than 0.027402759550625735
test145 0.03607553139646236
train150 0.027026503362258522 greater than 0.02708956503094667
test150 0.03662816812816814
train155 0.02670329011145335 greater than 0.026768439957755075
test155 0.03718033738904422
train160 0.026378865986639004 greater than 0.02644330736942862
test160 0.03772768557899889
train165 0.02606449089792708 greater than 0.026126023314835576
test165 0.03826342700043315
train170 0.025769746693582243 greater than 0.025826878759445872
test170 0.0387840906244877
train175 0.025500081902499763 greater than 0.02555156019139371
test175 0.039309169127647224
train180 0.025266388188473845 greater than 0.02530982158842777
test180 0.039917148459462724
train185 0.025073620012241286 greater than 0.02510887114249718
test185 0.04057901737040007
train190 0.024918508420771493 greater than 0.024948453953404
test190 0.041199209333620486
train195 0.02470298816217034 greater than 0.024759222064363175
test195 0.04169877812229786
train200 0.024372014753335863 greater than 0.02443942254413887
test200 0.04175181513607083
train205 0.02405007555604486 greater than 0.02411185856964709
test205 0.04135302666741023
train210 0.023756992565933027 greater than 0.02381502906109746
test210 0.040760748106982714
train215 0.023464256020983055 greater than 0.02351970103955844
test215 0.040377255636999426
train220 0.023231763558561963 greater than 0.023274009237463526
test220 0.04060033407804496
train225 0.02304348238511921 greater than 0.023079330757728552
test225 0.04123431234100071
train230 0.02285198850553117 greater than 0.022892460788881918
test230 0.04193237554984359
train235 0.022648620951130734 greater than 0.022687937093914513
test235 0.042313397769311485
train240 0.022469344725637542 greater than 0.022503342484181364
test240 0.042154588071242845
train245 0.0222944831926394 greater than 0.022331715295099315
test245 0.0412418737583558
train250 0.02207736941199919 greater than 0.02212467459772347
test250 0.03947546981229141
train255 0.021831867632754118 greater than 0.021881145137765064
test255 0.037990557593251456
train260 0.02160177221849438 greater than 0.021642401985303
test260 0.03732685058326656
train265 0.021432395398970395 greater than 0.021464300114704345
test265 0.03729473198926108
train270 0.021261173073879043 greater than 0.02129929704605491
test270 0.038638505571725226
train275 0.02108382947510646 greater than 0.021114450150293968
test275 0.04197676696873748
train280 0.02098248145711949 greater than 0.0210094233005855
test280 0.04122693544542707
train281 0.020999756830985494 greater than 0.02098248145711949
test281 0.040312332934456546
0 Accuracy:0.8976897689768977 Lower Bound:0.880627867419156 Upper Bound:0.9147516705346393
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: True
train0 0.1231279511496012 greater than 1
test0 0.11309029086263432
train5 0.06339496067123537 greater than 0.067388377441261
test5 0.07929511910641665
train10 0.05413846120835506 greater than 0.05529339768883378
test10 0.05510909993540594
train15 0.04962487702520834 greater than 0.05053292810833575
test15 0.043337998080071057
train20 0.04478497232701744 greater than 0.04566011384374917
test20 0.03611056082301201
train25 0.04165594375740985 greater than 0.04220884809063978
test25 0.03343063827411891
train30 0.03931236950249737 greater than 0.03975056830913859
test30 0.03312823783402297
train35 0.037430291594943904 greater than 0.03775155461491279
test35 0.03182029879226259
train40 0.0360999994509218 greater than 0.036317291025854695
test40 0.0314870697489074
train45 0.03500101355887259 greater than 0.03519842339590032
test45 0.029866507770342474
train50 0.0341811159078429 greater than 0.03431358359143074
test50 0.031209102029955336
train55 0.033194626547329964 greater than 0.03342906644715313
test55 0.03215753252340467
train60 0.0323962421846376 greater than 0.03254693438303993
test60 0.032718126493602624
train65 0.031478024199544784 greater than 0.031666121598165516
test65 0.03222030435055388
train70 0.0308415580675026 greater than 0.030957314200090943
test70 0.03565542111608005
train75 0.030212787311562333 greater than 0.030333605216590422
test75 0.036117478724962605
train79 0.03004314572285887 greater than 0.030019715747161956
test79 0.035604783427442686
0 Accuracy:0.9125412541254125 Lower Bound:0.8966362920559813 Upper Bound:0.9284462161948438
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: False
train0 0.1235326466517989 greater than 1
test0 0.11563159754884604
train5 0.06548375919403443 greater than 0.0693146809219883
test5 0.07365352905037327
train10 0.056757516480953965 greater than 0.05785778345486668
test10 0.06410535782743032
train15 0.053073514956941334 greater than 0.05365619056195805
test15 0.056376266437468084
train20 0.050658130111361194 greater than 0.051090926108790014
test20 0.050723614263346914
train25 0.04876913146282405 greater than 0.04913133446003104
test25 0.04682188840407174
train30 0.04695409120926742 greater than 0.047321369515867885
test30 0.04377739123406461
train35 0.045009863373250976 greater than 0.04541316723491099
test35 0.040885899894336504
train40 0.04302732869738538 greater than 0.04340883712023478
test40 0.038423562036417135
train45 0.04124970443158159 greater than 0.041602257695423764
test45 0.03655323247635807
train50 0.03941827180352259 greater than 0.03978615553198375
test50 0.035193475145416694
train55 0.03764916204686535 greater than 0.038007388287756634
test55 0.03375563744777284
train60 0.03566883745483655 greater than 0.03608121986100729
test60 0.032354141757684717
train65 0.033807756129217145 greater than 0.034140033369689876
test65 0.03162910479289288
train70 0.03239608939282246 greater than 0.03265334864171833
test70 0.031431495282644724
train75 0.03134467450533425 greater than 0.031522292240614924
test75 0.03139309649420194
train80 0.030516786082938358 greater than 0.0306798395338926
test80 0.031409562056571416
train85 0.029634238632545478 greater than 0.02982436547186783
test85 0.031579734928149805
train90 0.028808797808959632 greater than 0.028966304374066266
test90 0.032016662901825206
train95 0.027962646930712175 greater than 0.028116620255781644
test95 0.03251152766554547
train100 0.027319130162328373 greater than 0.027436373769217124
test100 0.03307481268942049
train105 0.026780361841686758 greater than 0.026886120129935673
test105 0.03352993581966326
train110 0.02617361090543042 greater than 0.02629310184226849
test110 0.033582917288441885
train115 0.025686181344115946 greater than 0.02577294938597269
test115 0.03357669451281067
train120 0.025319646901701614 greater than 0.025377275893675895
test120 0.033541291209704505
train122 0.025382828456798383 greater than 0.02522474882387109
test122 0.03334105428150693
1 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: True
train0 0.1235326466517989 greater than 1
test0 0.11563159754884604
train5 0.06969716424946841 greater than 0.07371739735492801
test5 0.10089029847896935
train10 0.05677041291423263 greater than 0.05866626274869386
test10 0.0695879783820531
train15 0.05061594863483575 greater than 0.05172972897933466
test15 0.046101092440656334
train20 0.04542907127861838 greater than 0.046452930288858454
test20 0.04102272951493728
train25 0.042464265142243156 greater than 0.042739084522732076
test25 0.037016312227599685
train30 0.04063791687550353 greater than 0.04107517697403672
test30 0.03343527047326062
train35 0.03828486841470362 greater than 0.03872932064339913
test35 0.03379698332471298
train40 0.03670084162040888 greater than 0.036934888093427654
test40 0.03371733902767781
train45 0.03529573950103266 greater than 0.03542096276149767
test45 0.033766730393510365
train46 0.03542290131372448 greater than 0.03529573950103266
test46 0.03346052003345546
1 Accuracy:0.9075907590759076 Lower Bound:0.8912862586847559 Upper Bound:0.9238952594670593
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: False
train0 0.1251005768994789 greater than 1
test0 0.12011705010952495
train5 0.06761456884728811 greater than 0.07218695190267263
test5 0.08010131121501786
train10 0.057726172204293 greater than 0.05893675530499062
test10 0.07144973305490679
train15 0.05376467325020385 greater than 0.054370547133059124
test15 0.06509472559651625
train20 0.05135732361960905 greater than 0.051785592614303155
test20 0.05952335187709791
train25 0.04944260577802138 greater than 0.049797285769897226
test25 0.05440778160783775
train30 0.0478272393731491 greater than 0.04813430746038191
test30 0.05021584865860621
train35 0.04630197015344623 greater than 0.046614554817403085
test35 0.04679496103778749
train40 0.044605338301236565 greater than 0.04496066935966201
test40 0.043847811944635696
train45 0.04277065383819235 greater than 0.04314452530833161
test45 0.04106060928091835
train50 0.04084682569544046 greater than 0.041237428494655504
test50 0.03838894203553107
train55 0.038917843203942544 greater than 0.039289905922329886
test55 0.03615272057463329
train60 0.03723665196229287 greater than 0.0375517985011021
test60 0.0345412293532512
train65 0.03577194959686909 greater than 0.036057233847411045
test65 0.033482783673888614
train70 0.03434602758860182 greater than 0.03462309216224838
test70 0.03275598396736879
train75 0.03313273573391126 greater than 0.03335949261252672
test75 0.03208171318905823
train80 0.032086082369242076 greater than 0.03227956116235813
test80 0.03167105530328632
train85 0.031219802457051374 greater than 0.0313828960174219
test85 0.031528678079681856
train90 0.030489056007330238 greater than 0.030619657220549157
test90 0.03160831644814998
train95 0.029970585423558148 greater than 0.030059482151853466
test95 0.03176391499060223
train100 0.029580386099931875 greater than 0.029655201283468716
test100 0.0319419444569306
train105 0.02917522434233788 greater than 0.029263187680539114
test105 0.03216346172028898
train110 0.028683394310268288 greater than 0.028786600249560215
test110 0.03233095776478143
train115 0.02817012836526009 greater than 0.0282701475764538
test115 0.03244532400008622
train120 0.027701383433782603 greater than 0.027791751505629188
test120 0.03256589820208311
train125 0.027283929919878422 greater than 0.02735819312402825
test125 0.032702189384076305
train130 0.0269534074391129 greater than 0.027019989432308735
test130 0.03284700170485592
train135 0.026676068345061386 greater than 0.02670618806247188
test135 0.0328095760369753
train140 0.026530195919509683 greater than 0.02657172724331418
test140 0.03263429972609582
train145 0.026276181991243965 greater than 0.026291974573550488
test145 0.03255769921327779
train146 0.02630378253429436 greater than 0.026276181991243965
test146 0.032482681562582036
2 Accuracy:0.9133663366336634 Lower Bound:0.897529421057252 Upper Bound:0.9292032522100748
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: True
train0 0.1251005768994789 greater than 1
test0 0.12011705010952495
train5 0.06804470087216935 greater than 0.07346139866514308
test5 0.08907634114015471
train10 0.05705077935670446 greater than 0.058388760973372124
test10 0.06865679336254302
train15 0.05191695501403517 greater than 0.05278157019459117
test15 0.0591867771634952
train20 0.04809602597250805 greater than 0.048838774792807536
test20 0.05327319522531306
train25 0.04483967267166901 greater than 0.045429483534389956
test25 0.04773574854186789
train30 0.04229283443657315 greater than 0.04277023532660868
test30 0.042447298819790064
train35 0.03964211996060333 greater than 0.04014146573035054
test35 0.037347314318599716
train40 0.037681957756674225 greater than 0.03805306689042394
test40 0.03443356191417471
train45 0.035644310792652324 greater than 0.036033281272277895
test45 0.03236165500127055
train50 0.033942469280049264 greater than 0.034295784662721186
test50 0.030913889875848546
train55 0.032581213304027606 greater than 0.03281990066256995
test55 0.03134614492695358
train60 0.03150498871901685 greater than 0.031723836501252156
test60 0.032643406531242376
train65 0.03020514202214117 greater than 0.030427698990206503
test65 0.03376766640156555
train70 0.029300584472413883 greater than 0.02947193162026389
test70 0.033952365880779774
train75 0.02831770453058399 greater than 0.028553528435346625
test75 0.03383571004919785
train80 0.027218012616760295 greater than 0.02732732723028636
test80 0.03384321071221287
train85 0.02649334741677112 greater than 0.026671014248329794
test85 0.033554950850611526
train90 0.026024684396518116 greater than 0.02595399905476967
test90 0.032786454974059416
2 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: False
train0 0.12559277954271972 greater than 1
test0 0.12201023832094571
train5 0.07042803557620071 greater than 0.07556168965695317
test5 0.08615958078222025
train10 0.059299303405529705 greater than 0.06053764987273085
test10 0.07333259791509099
train15 0.05525750012324672 greater than 0.055898175923540815
test15 0.06611018638157612
train20 0.05264304913408786 greater than 0.053103955561058784
test20 0.059842475559446356
train25 0.05070077557008072 greater than 0.051050486350321105
test25 0.05514700675558968
train30 0.049155216452188746 greater than 0.049441492698040185
test30 0.05175677989169871
train35 0.047843492831212746 greater than 0.048092892924649876
test35 0.04928889872514962
train40 0.046653732188388294 greater than 0.046886353528038764
test40 0.04744498786639537
train45 0.045513095468774675 greater than 0.045738182051223326
test45 0.04600871809545174
train50 0.044421498162759296 greater than 0.044634622502653085
test50 0.04485624480321774
train55 0.04339882195725807 greater than 0.04359772656972931
test55 0.04386537417551555
train60 0.042431933600782616 greater than 0.042623508670942986
test60 0.04284685859040016
train65 0.04145749251870452 greater than 0.04165568984302014
test65 0.04166418802230396
train70 0.040440632556758026 greater than 0.04064755840835111
test70 0.04038686731569099
train75 0.03934524162281186 greater than 0.03957560779864061
test75 0.03915541931899267
train80 0.03813318899462922 greater than 0.03837755109618212
test80 0.037974719237699746
train85 0.036851437806982586 greater than 0.03711928088484872
test85 0.03691845345446572
train90 0.035525352987543064 greater than 0.03578416250261246
test90 0.03597895410497666
train95 0.03428655949676009 greater than 0.034525657516336565
test95 0.03516131457731594
train100 0.03317829126673915 greater than 0.03338815946488082
test100 0.03447072770478079
train105 0.03219329448899889 greater than 0.03238436239552996
test105 0.033950712700321226
train110 0.031260034040723174 greater than 0.03144357875151331
test110 0.03363074989532086
train115 0.030375802952673066 greater than 0.030547910106443858
test115 0.03349158235309804
train120 0.029547916843635242 greater than 0.029709427852173686
test120 0.03346549630804573
train125 0.02877659624556175 greater than 0.028924931949957836
test125 0.03348833275829575
train130 0.02808371520663811 greater than 0.028216223218923395
test130 0.03351128346247013
train135 0.027458115061014192 greater than 0.02757900092740147
test135 0.03353291460010291
train140 0.02687089787801411 greater than 0.026987176012775622
test140 0.033566310413759466
train145 0.026290415370926613 greater than 0.02640614515877511
test145 0.03355848702435258
train150 0.025719567103440567 greater than 0.025832689956786337
test150 0.0334682649539059
train155 0.025163644521646978 greater than 0.025273590502099354
test155 0.03331569383299439
train160 0.02461610490062666 greater than 0.02472567808349031
test160 0.033239293125806445
train165 0.02407445763433506 greater than 0.024180945001680727
test165 0.03326875750689526
train170 0.023568725409515608 greater than 0.023665808490458462
test170 0.03335673719036928
train175 0.02311251022487235 greater than 0.023200356139291754
test175 0.03345947451560756
train180 0.022688022975400998 greater than 0.022771723768506027
test180 0.033549841487476945
train185 0.022268831639940395 greater than 0.022353181878167188
test185 0.03362031099572521
train190 0.02184349091064682 greater than 0.021928699732514562
test190 0.03368063713783203
train195 0.021422836542733668 greater than 0.02150584279723547
test195 0.03374116554840497
train200 0.02102185791791377 greater than 0.021099917145757496
test200 0.03380931127491939
train205 0.020647004174479056 greater than 0.02072019634378722
test205 0.03389631843764498
train210 0.02028751088818778 greater than 0.020359075977284317
test210 0.034018001298784135
train215 0.019923421406220627 greater than 0.019997702481382263
test215 0.03418564411992371
train220 0.01953909642079034 greater than 0.01961698896513786
test220 0.03437402333691387
train225 0.019153666903675943 greater than 0.019229426250090605
test225 0.034531924735782746
train230 0.018795356678995505 greater than 0.018864051274033053
test230 0.034664403169298856
train235 0.0184697760344464 greater than 0.018532996978304492
test235 0.034755845592216526
train240 0.018164395874369278 greater than 0.018224021794868598
test240 0.03480733679261521
train245 0.01787763235294742 greater than 0.017933398878540684
test245 0.03485156624253483
train250 0.017617580244093226 greater than 0.017666481740474114
test250 0.034859742661756496
train255 0.017390954055497163 greater than 0.017435148181930486
test255 0.03482833250363504
train260 0.01717298107235117 greater than 0.01721506866283158
test260 0.034816602454539346
train265 0.016989518006080593 greater than 0.01702339473425007
test265 0.034817702823842035
train270 0.016809691435965085 greater than 0.016847956109422097
test270 0.03483241563204161
train275 0.016608190681836284 greater than 0.01664921479461966
test275 0.03486512265420274
train280 0.01640254725347654 greater than 0.016443449078640614
test280 0.03487884774158968
train285 0.016198773499785837 greater than 0.016240047113799227
test285 0.03484126850645455
train290 0.01597844974367384 greater than 0.01602481968907857
test290 0.0347452750889507
train295 0.01575165589055697 greater than 0.015792132248404592
test295 0.03471170850139982
train300 0.015569936199254536 greater than 0.015603336433754354
test300 0.0346973950063497
train305 0.015431422244734008 greater than 0.015456316672379303
test305 0.034693482022209746
train310 0.015309688950644076 greater than 0.015334449352866512
test310 0.03471677327451234
train315 0.01518139022283915 greater than 0.015207344983768719
test315 0.0347599522954306
train320 0.015057158876079522 greater than 0.015080720103464763
test320 0.03481420168910715
train325 0.014954050027157582 greater than 0.014972461788300957
test325 0.03489190279967213
train330 0.014901981940622349 greater than 0.014902389872846967
test330 0.035107114265702385
train331 0.014908620049269267 greater than 0.014901981940622349
test331 0.03515931380630581
3 Accuracy:0.9141914191419142 Lower Bound:0.8984229805385732 Upper Bound:0.9299598577452551
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: True
train0 0.12559277954271972 greater than 1
test0 0.12201023832094571
train5 0.07056295076190473 greater than 0.07613642911756846
test5 0.09565601368126793
train10 0.059211671977805275 greater than 0.06046632973303855
test10 0.07342737577834647
train15 0.05481884733646432 greater than 0.055522901497399094
test15 0.0651652341964942
train20 0.05208056923883417 greater than 0.052552996408897865
test20 0.059166884643042456
train25 0.05000782409915259 greater than 0.050402990103865904
test25 0.05466148701194994
train30 0.04791400643414429 greater than 0.04835222880534307
test30 0.05067846698220207
train35 0.04569345175595347 greater than 0.0461389519849642
test35 0.04722207186722424
train40 0.0433934260569782 greater than 0.043881734962286485
test40 0.04470873696097853
train45 0.04068357900865388 greater than 0.04121762436959831
test45 0.04221398263877704
train50 0.03873520411502843 greater than 0.03926282873041431
test50 0.039568758438805016
train55 0.036438061305865924 greater than 0.03677477878775712
test55 0.03913392749886374
train60 0.03479495491342252 greater than 0.03515325826283964
test60 0.03920087477344533
train65 0.03310632278865738 greater than 0.03344705832733619
test65 0.03799973074077493
train70 0.03148688176679654 greater than 0.03188018186842742
test70 0.04020925095574448
train75 0.029932626127377734 greater than 0.030152448924513586
test75 0.04129394173528914
train80 0.028515842998579934 greater than 0.028822281944143716
test80 0.037393995225947836
train85 0.027503887616121214 greater than 0.027665597544661604
test85 0.03383706940082225
train90 0.02664320174349384 greater than 0.026864189784851042
test90 0.03252493956435204
train95 0.026261607180604014 greater than 0.026213602474591507
test95 0.032371701079740224
3 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: False
train0 0.12600892110495285 greater than 1
test0 0.12371331621742375
train5 0.08253093415523123 greater than 0.08893746727754599
test5 0.07203924141565882
train10 0.06567383121239369 greater than 0.06776835751265001
test10 0.05670668242954329
train15 0.059592254687766716 greater than 0.060454453926365184
test15 0.051163101148822825
train20 0.05641735291204773 greater than 0.05694836840901913
test20 0.04872171892427824
train25 0.054215671244202006 greater than 0.05460675776209217
test25 0.04728375061953123
train30 0.052514708555023215 greater than 0.05282651932405787
test30 0.04625592452134546
train35 0.05111305958338365 greater than 0.05137501250600747
test35 0.04541548556168992
train40 0.04991521055080269 greater than 0.05014103652537296
test40 0.04466650888766984
train45 0.04887540918546191 greater than 0.04907242324839755
test45 0.04397284238721735
train50 0.047953341680077845 greater than 0.04813078371768886
test50 0.043322482279883806
train55 0.04710027819169715 greater than 0.0472673025447562
test55 0.042716717774239917
train60 0.04627798673421777 greater than 0.04644172689070906
test60 0.04216510011936112
train65 0.04545339366933332 greater than 0.04561943520957708
test65 0.04166904257357031
train70 0.04461647927721078 greater than 0.04478451642347572
test70 0.04120448003886824
train75 0.04377250854399116 greater than 0.0439418767777552
test75 0.04073057528783626
train80 0.042918551554458215 greater than 0.04309064289520769
test80 0.040193781236259274
train85 0.04203733262147419 greater than 0.04221740636947433
test85 0.03949909973305655
train90 0.04105952661887198 greater than 0.041272836132090894
test90 0.038522385633620704
train95 0.03985701241926856 greater than 0.04010344887252446
test95 0.03742347025327135
train100 0.038604447583623774 greater than 0.03886206260297996
test100 0.036302031456197065
train105 0.037260242020679735 greater than 0.03753300624093928
test105 0.0351879339999828
train110 0.035936587064535605 greater than 0.03619223074850294
test110 0.03423108472649334
train115 0.03474520833858355 greater than 0.03497258853959309
test115 0.03346458772553149
train120 0.03366619331953524 greater than 0.033875651849219514
test120 0.03285366524464964
train125 0.03265000175938845 greater than 0.03285051816678384
test125 0.03239321568907273
train130 0.03165831720852072 greater than 0.031854172069977996
test130 0.03209308953824522
train135 0.03071912328515988 greater than 0.030901223183215924
test135 0.03190188097388012
train140 0.029848973000699622 greater than 0.030017682099499194
test140 0.031749070142324226
train145 0.029027359979888773 greater than 0.02919151160154585
test145 0.03160892782485794
train150 0.0281979925378518 greater than 0.028361212485723285
test150 0.03147224050440644
train155 0.027432551530785387 greater than 0.027580556912704578
test155 0.03128105414580685
train160 0.026663082631203074 greater than 0.026824752529681143
test160 0.031100103228163412
train165 0.025809973958384063 greater than 0.02598418658796186
test165 0.03102553010401015
train170 0.024953953536857556 greater than 0.025120559981759848
test170 0.03110721506763566
train175 0.02418398161374831 greater than 0.024328623750074385
test175 0.03139057141316234
train180 0.02353130457957872 greater than 0.0236532567674569
test180 0.031838249776622154
train185 0.02296698601007687 greater than 0.023074010445803612
test185 0.032352542345334905
train190 0.02249755880326159 greater than 0.022582487558373016
test190 0.032809780841307416
train195 0.022057798741541516 greater than 0.022152932989231405
test195 0.03316489640088614
train200 0.021555101948594495 greater than 0.021649421367405088
test200 0.03356054076028129
train205 0.021117995638008143 greater than 0.02121552135951916
test205 0.03408559397165559
train210 0.02056565602945969 greater than 0.020678390306721625
test210 0.03445946218396819
train215 0.020002382224885033 greater than 0.020120810999441636
test215 0.03448320015939968
train220 0.019292294567932803 greater than 0.01943887820130739
test220 0.03435403032930834
train225 0.018596779522331876 greater than 0.018743282656093277
test225 0.03452456184145396
train230 0.017888871688461105 greater than 0.017999942914639536
test230 0.03457105287095349
train235 0.017360019918638694 greater than 0.017466843177580085
test235 0.034574165425671995
train240 0.016852617433492312 greater than 0.0169515668451617
test240 0.03466748976641531
train245 0.016343173675708103 greater than 0.016448856224868795
test245 0.03484199018695105
train250 0.01582696703738675 greater than 0.01591390076294795
test250 0.03503415868652468
train255 0.015425621491584187 greater than 0.015510419338976232
test255 0.03490491670632097
train260 0.014985705542876449 greater than 0.015074042827706163
test260 0.03462431111796397
train265 0.014585856029631066 greater than 0.014654225832079414
test265 0.03439274659658984
train270 0.014306795897643842 greater than 0.014365348445956575
test270 0.03424795407233314
train275 0.013898447598289902 greater than 0.01398738864578006
test275 0.03419431461725022
train280 0.013611354841685932 greater than 0.01364049185529338
test280 0.0340189323405757
train285 0.013531489768710317 greater than 0.01354729404568147
test285 0.03371371208365354
train290 0.013385256564025564 greater than 0.013422552605715211
test290 0.03346788469293097
train295 0.013161395734216532 greater than 0.013213146205273028
test295 0.03334382191544207
train300 0.012954952575593556 greater than 0.012990250546345226
test300 0.03334445249923573
train305 0.012787073172577124 greater than 0.012820290055281186
test305 0.033387603332497164
train310 0.012624208186897927 greater than 0.012655998173138918
test310 0.033416437960660406
train315 0.012523986212691036 greater than 0.012519242873261094
test315 0.03355246089230078
4 Accuracy:0.9166666666666666 Lower Bound:0.9011062988741652 Upper Bound:0.9322270344591681
Loss calculated for layer count:1 node count: 5 stepSize:0.15 Momentum: True
train0 0.12600892110495285 greater than 1
test0 0.12371331621742375
train5 0.08218007001673863 greater than 0.08853592958638073
test5 0.07128037977054581
train10 0.06548421982054195 greater than 0.06752577705431666
test10 0.05622668764493523
train15 0.05955522783908755 greater than 0.06040271923375037
test15 0.0509261909593687
train20 0.056410064956767834 greater than 0.056938428791546786
test20 0.04861415967453838
train25 0.05421677427616512 greater than 0.05460668838666463
test25 0.047240022388048857
train30 0.052516214135408266 greater than 0.05282865219652212
test30 0.04624169050434205
train35 0.05110677284423747 greater than 0.051370858607858984
test35 0.045409230651222585
train40 0.04989527077465428 greater than 0.05012410161064205
test40 0.04465784946590224
train45 0.04884008161469232 greater than 0.04904016682011148
test45 0.043958175933951536
train50 0.04790195415703692 greater than 0.048082801913587166
test50 0.0433018963761897
train55 0.047029085136693224 greater than 0.04720053385606216
test55 0.04269403186436695
train60 0.046179745400227734 greater than 0.0463495839966181
test60 0.04214713283246379
train65 0.04532123602366443 greater than 0.04549436046728301
test65 0.04165857944808989
train70 0.044445677288394624 greater than 0.044622082551457706
test70 0.041199426235001814
train75 0.043558475042510335 greater than 0.04373591234333309
test75 0.040737946928748034
train80 0.04267700667380856 greater than 0.04285289569653522
test80 0.040219821832023724
train85 0.04178345495562495 greater than 0.04196633057843226
test85 0.03950863288284649
train90 0.040717281421077445 greater than 0.040959630828214764
test90 0.03849937227509535
train95 0.03947860848537797 greater than 0.039723575081813726
test95 0.03743565639239277
train100 0.03820599785817675 greater than 0.03846423148521661
test100 0.03629815133963745
train105 0.03694684536276272 greater than 0.03719409386650537
test105 0.035210502855055316
train110 0.03575212375654087 greater than 0.035983243190434756
test110 0.034269191170910776
train115 0.03467477563403618 greater than 0.03487978776166032
test115 0.03348627197888159
train120 0.033705856484391776 greater than 0.03389462000941584
test120 0.03282786337628875
train125 0.03277055765178573 greater than 0.03295743957888326
test125 0.03225579566526275
train130 0.031819562608189675 greater than 0.03201453767347488
test130 0.031759158958584825
train135 0.03077537665173173 greater than 0.030991537589523423
test135 0.031355100728758696
train140 0.02971151878915108 greater than 0.02992066371474975
test140 0.03106205147029122
train145 0.028704152183951084 greater than 0.028895881162386793
test145 0.030871866661878692
train150 0.02779656903519831 greater than 0.02797589101808825
test150 0.030798690896941438
train155 0.02685512239774828 greater than 0.027058849901980236
test155 0.030805517771954484
train160 0.02586774317000786 greater than 0.02605084314333546
test160 0.03078207560311111
train165 0.024993460505124226 greater than 0.02516549085164215
test165 0.030794900342318195
train170 0.02417245971669061 greater than 0.02433014463251249
test170 0.030907254637870685
train175 0.023438418408864205 greater than 0.023578208349293292
test175 0.03111137317380183
train180 0.022784535848585568 greater than 0.022909846981775922
test180 0.03138350405804557
train185 0.022192174359366067 greater than 0.022306441276397467
test185 0.03170575674677027
train190 0.021649500729775884 greater than 0.021754285624200728
test190 0.03206183216196152
train195 0.02112799758043174 greater than 0.02123909901110373
test195 0.03234711773866953
train200 0.020557441412683862 greater than 0.02062674299921441
test200 0.032577187058520314
train205 0.02003859434752804 greater than 0.020157089076276336
test205 0.03297633091062572
train210 0.01948003925121596 greater than 0.01959688989945442
test210 0.03345502556714976
train215 0.018871175886916364 greater than 0.018990975574067568
test215 0.03385594049381318
train220 0.01833704261355156 greater than 0.018434342228715008
test220 0.03409059654966923
train225 0.01789190730085752 greater than 0.017977858559801885
test225 0.034081846778436106
train230 0.017460160269558902 greater than 0.017547754282451884
test230 0.033972097433859066
train235 0.017027941244956928 greater than 0.017111229578885164
test235 0.033824877751931326
train240 0.01665621028217368 greater than 0.016724847187487974
test240 0.03373694579169718
train245 0.016354267832788965 greater than 0.016403720289511592
test245 0.03387399995693169
train250 0.016153808211020467 greater than 0.01620259549217827
test250 0.03371539194182131
train255 0.015919313102030073 greater than 0.01596963474248554
test255 0.03348820598709769
train260 0.015635865821266495 greater than 0.01569680529522193
test260 0.03347993864135848
train265 0.015383757509836801 greater than 0.015421407153025748
test265 0.034199811820813254
train270 0.014865621532341536 greater than 0.014931085155911124
test270 0.034009197154688496
train275 0.01451357886829252 greater than 0.014585961137904713
test275 0.034070533166247065
train280 0.01424367098312921 greater than 0.014303928755170474
test280 0.03427976206982112
train283 0.014142431076820283 greater than 0.014138533544138764
test283 0.03460832228278029
4 Accuracy:0.9092409240924092 Lower Bound:0.8930679726938885 Upper Bound:0.9254138754909299
