Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: False
train0 0.11856662147250899 greater than 1
test0 0.09973747597729968
train5 0.06117674744381913 greater than 0.06427882642463184
test5 0.06700862918273134
train10 0.05309251705296119 greater than 0.05419423993667978
test10 0.0526588235059913
train15 0.04921631113561416 greater than 0.04983773496399265
test15 0.045638242431129045
train20 0.04665790162725895 greater than 0.04713004809439083
test20 0.041203610574489555
train25 0.04419276795613078 greater than 0.044710085650169554
test25 0.037622589264544384
train30 0.041571843276017144 greater than 0.04210216710611559
test30 0.034685399055165406
train35 0.03898212338581822 greater than 0.03947284938920408
test35 0.032773396365853316
train40 0.03686114808361676 greater than 0.0372395074452451
test40 0.03165175752762977
train45 0.03523635062263198 greater than 0.03552735439107098
test45 0.03096061879121967
train50 0.03402834845553348 greater than 0.034242526967697466
test50 0.030622268631156285
train55 0.033105700804961044 greater than 0.03327213727947962
test55 0.030536051022062312
train60 0.03239990530004989 greater than 0.032541343821115364
test60 0.03101057251286336
train65 0.03154477108927274 greater than 0.031672647670243824
test65 0.03191680978841609
train70 0.0307517781748912 greater than 0.030911532659925776
test70 0.03251116278343513
train75 0.03005552256656245 greater than 0.03018594121450152
test75 0.032948054967309674
train80 0.02940304194952701 greater than 0.029529678551881898
test80 0.033118632318102637
train85 0.028849298028132693 greater than 0.02895650691949634
test85 0.03305829123676446
train90 0.028300705785134366 greater than 0.02840991476736432
test90 0.032946197674375934
train95 0.02778413071600704 greater than 0.027882528691799607
test95 0.03288150144859928
train100 0.027337521661962745 greater than 0.027420731378772677
test100 0.0327948883571199
train105 0.02689103904597599 greater than 0.026983562874021427
test105 0.032657197211135656
train110 0.02623585787879645 greater than 0.02640168263131622
test110 0.03285905910719016
train115 0.02533261459275358 greater than 0.02546374847994446
test115 0.03370149934235011
train120 0.024817272949619332 greater than 0.02491899241078278
test120 0.034386327830937743
train125 0.02436832373886204 greater than 0.02445218324764373
test125 0.034700150755889274
train130 0.02398885147443648 greater than 0.024062002290313918
test130 0.03490778982545683
train135 0.0236831671535864 greater than 0.02373891274003356
test135 0.035136018641703966
train140 0.02333090806284785 greater than 0.023418868635386156
test140 0.03567809454246243
train145 0.02299788329870735 greater than 0.02303407827721311
test145 0.03644155692359105
train150 0.022763471927422056 greater than 0.022843239979082105
test150 0.03683308113534116
train155 0.02238032587349979 greater than 0.022519549592360036
test155 0.03714990220125059
train157 0.022279205431170863 greater than 0.022254777833005734
test157 0.03662505925238959
0 Accuracy:0.9051155115511551 Lower Bound:0.8886166349039726 Upper Bound:0.9216143881983376
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: True
train0 0.11856662147250899 greater than 1
test0 0.09973747597729968
train5 0.060759553396665236 greater than 0.06460442976261306
test5 0.06670409416214679
train10 0.052453418562027315 greater than 0.05361893985769966
test10 0.048555260413590595
train15 0.04828531149200446 greater than 0.0489568452268926
test15 0.042356702056474994
train20 0.04513437966787759 greater than 0.04582393491898385
test20 0.038944173514882025
train25 0.04179469557690187 greater than 0.04248040798305046
test25 0.036107543816233274
train30 0.03844798503347272 greater than 0.03907024092403385
test30 0.03354098381348805
train35 0.03582846263054843 greater than 0.036267894297345886
test35 0.032720219160753336
train40 0.034191824872170526 greater than 0.034435456024443735
test40 0.03128713114999175
train45 0.03277476303999737 greater than 0.03313219212264497
test45 0.03098725675974121
train50 0.031309298116618436 greater than 0.03164359512338635
test50 0.030660122888823835
train55 0.02966796890661628 greater than 0.02984408348175572
test55 0.030694693416085383
train60 0.029186345667088158 greater than 0.029341928244905162
test60 0.03328358579086631
train65 0.02772587628460276 greater than 0.02787957740628941
test65 0.03384097733318402
train70 0.026746848702612243 greater than 0.026859061545339617
test70 0.03431125774555323
train75 0.02592521701406578 greater than 0.02599215449485131
test75 0.0385004435447865
train76 0.026052420531410273 greater than 0.02592521701406578
test76 0.03872565973022434
0 Accuracy:0.9092409240924092 Lower Bound:0.8930679726938885 Upper Bound:0.9254138754909299
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: False
train0 0.12019701310176784 greater than 1
test0 0.10361438788831433
train5 0.06428166185334822 greater than 0.06782091680428103
test5 0.0721804936524267
train10 0.05535747930995094 greater than 0.056528423740120355
test10 0.06053986358011953
train15 0.051262975398091264 greater than 0.0519203632083703
test15 0.05294362742393711
train20 0.04868567398652834 greater than 0.04912599133680541
test20 0.048095650738302624
train25 0.04682196311426937 greater than 0.04716208965769024
test25 0.044576462109400326
train30 0.04524486567599492 greater than 0.045550894537480575
test30 0.041733407226183586
train35 0.04364235341527111 greater than 0.04397921556845203
test35 0.03898240438225786
train40 0.041811246732576204 greater than 0.042196287956261194
test40 0.03638151813687006
train45 0.03984188358010904 greater than 0.04022911930394086
test45 0.034583123608513726
train50 0.038088087471335526 greater than 0.03842056380167889
test50 0.033519615944469956
train55 0.03659249632741705 greater than 0.03687340559521622
test55 0.03280988407408897
train60 0.035217336830449836 greater than 0.03548794451882592
test60 0.03224000437519252
train65 0.03394085794384657 greater than 0.03418447567215075
test65 0.03172987880944805
train70 0.032852693345575674 greater than 0.033049515799322915
test70 0.03129228850345955
train75 0.03199685162314509 greater than 0.03215734494022054
test75 0.031033940690997283
train80 0.03118415286744387 greater than 0.03135123621297915
test80 0.03094661646959248
train85 0.03032540936143264 greater than 0.030493808175464297
test85 0.03100392450830889
train90 0.029535709085856773 greater than 0.0296855996009272
test90 0.031128856860263572
train95 0.02883254483875426 greater than 0.02896785703599697
test95 0.031172715681936622
train100 0.02823333273309754 greater than 0.02833445869041447
test100 0.03134206731414237
train105 0.027688100411719378 greater than 0.02780659535057177
test105 0.03166186236291638
train110 0.027051853779201055 greater than 0.027165056220936398
test110 0.03181879244796261
train115 0.02657048356635931 greater than 0.026660593562467076
test115 0.031930229531197826
train120 0.026152576258328487 greater than 0.026232348047334715
test120 0.0321413394006036
train125 0.025798932312455142 greater than 0.025861223601890613
test125 0.032613177329551754
train130 0.025477773321645844 greater than 0.025548916809679016
test130 0.03328168972381852
train135 0.025078938215597105 greater than 0.02516074197792274
test135 0.03385794975920832
train140 0.024682180089640664 greater than 0.024767224228500723
test140 0.03426705997956878
train145 0.024220204722127128 greater than 0.024300618021288883
test145 0.03446348868538223
train150 0.02370370254863049 greater than 0.0238095369203752
test150 0.03422887541674848
train155 0.02317288096490034 greater than 0.02326026643404826
test155 0.03465414648801486
train160 0.022650846080524284 greater than 0.022754320089022388
test160 0.03536171874529614
train165 0.02218857807346475 greater than 0.022270500930364635
test165 0.035988242032735276
train170 0.021585196322945078 greater than 0.02169309806213009
test170 0.036146276693304585
train175 0.02105718968445992 greater than 0.021163600629145372
test175 0.035806613933504025
train180 0.020619763282466407 greater than 0.02069682555120532
test180 0.03552193092006112
train185 0.02025587220287508 greater than 0.020324975396448085
test185 0.03529188227674615
train190 0.01993790948814239 greater than 0.020004609906776674
test190 0.0350845205158398
train195 0.019511736001697918 greater than 0.019593491201348626
test195 0.03459820242875783
train200 0.019193310764989438 greater than 0.019258879687259773
test200 0.035065941255633816
train205 0.018791412243949097 greater than 0.01887419254643972
test205 0.03512008784872913
train208 0.018666745724085262 greater than 0.01866409174108382
test208 0.03475030064069991
1 Accuracy:0.9174917491749175 Lower Bound:0.9020016379834073 Upper Bound:0.9329818603664277
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: True
train0 0.12019701310176784 greater than 1
test0 0.10361438788831433
train5 0.06420276231617959 greater than 0.06834333846193606
test5 0.07809826174683072
train10 0.05485276533170297 greater than 0.056092145807900826
test10 0.05757151929371566
train15 0.05010598622889068 greater than 0.050859296174107685
test15 0.048215789673961576
train20 0.046898519495372805 greater than 0.047571102933204976
test20 0.04220271198386629
train25 0.04254444500466697 greater than 0.04339539014849748
test25 0.037599653053498286
train30 0.0394785699219291 greater than 0.039972663523430374
test30 0.035675700541533135
train35 0.0366276864588836 greater than 0.03716316596230268
test35 0.03990058581799686
train40 0.03482212047931625 greater than 0.03505104980296743
test40 0.03941256724673711
train45 0.033379307261141786 greater than 0.03360794928837878
test45 0.03151269880878123
train50 0.03159933645588202 greater than 0.03198957143697126
test50 0.030604592453431487
train55 0.030009159588062877 greater than 0.030227714506420117
test55 0.03376907837490002
train60 0.02868878545774831 greater than 0.02888671512042219
test60 0.03541243649032869
train63 0.028546182095576283 greater than 0.028527736039744213
test63 0.04189897652124526
1 Accuracy:0.8960396039603961 Lower Bound:0.8788564719422342 Upper Bound:0.913222735978558
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: False
train0 0.12334546968089267 greater than 1
test0 0.11072019464878416
train5 0.06697968572108637 greater than 0.07138794728969688
test5 0.07787126562042314
train10 0.05696631631054609 greater than 0.05822818109010113
test10 0.06871941420534343
train15 0.05262281574736671 greater than 0.053309620063551943
test15 0.06178960954971424
train20 0.049916018796365036 greater than 0.05038399517079387
test20 0.05669457712901884
train25 0.047940045337040736 greater than 0.04829381744354931
test25 0.05283755220297947
train30 0.04639902429040526 greater than 0.04668245014365221
test30 0.04980959616240625
train35 0.04509676140545532 greater than 0.04534525644671547
test35 0.047260227135025305
train40 0.04391147308427682 greater than 0.04414272131295329
test40 0.04491261887550613
train45 0.04276469669749503 greater than 0.042995548039749885
test45 0.042812320577376425
train50 0.041552297334172886 greater than 0.041805946216084716
test50 0.04080701132374891
train55 0.04016798501130571 greater than 0.040460066565285766
test55 0.03876380752509408
train60 0.03863760681177214 greater than 0.03894969782687778
test60 0.036794092018113116
train65 0.03708112709327685 greater than 0.03738730792910213
test65 0.03510276061831835
train70 0.03560001166613382 greater than 0.03589235896787955
test70 0.0337764046398396
train75 0.03413674870367136 greater than 0.034433269154041525
test75 0.0328381273676617
train80 0.03275000869378318 greater than 0.03301015623383026
test80 0.03219005284722715
train85 0.0315473439487229 greater than 0.03177577666977951
test85 0.03163286051902633
train90 0.030495649457243337 greater than 0.03069560149085505
test90 0.031145340056576577
train95 0.0295339107579272 greater than 0.029699616106749458
test95 0.030720530088497615
train100 0.029020165136420327 greater than 0.029125609979809428
test100 0.03034759678586092
train105 0.02841112120205167 greater than 0.02852980763636856
test105 0.030027507212506046
train110 0.027856410330716105 greater than 0.02796490705235406
test110 0.029792379514344484
train115 0.02733000377264161 greater than 0.02743267819474079
test115 0.029670016209905514
train120 0.026887935947350297 greater than 0.02696617491445026
test120 0.029668957236790236
train125 0.026545157885322277 greater than 0.02660922348108572
test125 0.02978886054344193
train130 0.026220122231075615 greater than 0.026289654079906594
test130 0.030084214510369803
train135 0.025715370759954496 greater than 0.02584572710615009
test135 0.030699252501005782
train140 0.025123003105751673 greater than 0.025231167517185307
test140 0.03144019427223022
train145 0.024642516405486342 greater than 0.02473339153444718
test145 0.031942018788062215
train150 0.02420223146320145 greater than 0.02426533263717857
test150 0.03205643627458997
train152 0.024289875703884238 greater than 0.024193526561935557
test152 0.03203305923883231
2 Accuracy:0.9191419141914191 Lower Bound:0.9037937027025817 Upper Bound:0.9344901256802565
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: True
train0 0.12334546968089267 greater than 1
test0 0.11072019464878416
train5 0.07935510921423586 greater than 0.08171098093144437
test5 0.08440060307789628
train10 0.07003098976509285 greater than 0.07077666528007964
test10 0.08153408828579833
train15 0.06450071972010417 greater than 0.06614476401018604
test15 0.06694410711806588
train20 0.06115045502767456 greater than 0.062314915267834566
test20 0.07307554015613896
train22 0.06077101874720301 greater than 0.06021620204996326
test22 0.07391979821482374
2 Accuracy:0.8044554455445545 Lower Bound:0.7821259648785998 Upper Bound:0.8267849262105093
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: False
train0 0.12548374509445692 greater than 1
test0 0.115398751992852
train5 0.07012940704358682 greater than 0.0750004880774631
test5 0.08377733159671695
train10 0.05877034811506159 greater than 0.06011678630236664
test10 0.0707869871857016
train15 0.05444017948109788 greater than 0.055124553487756245
test15 0.06314249520267981
train20 0.0516877408172232 greater than 0.05216708614688792
test20 0.05734971962877755
train25 0.0496771618246737 greater than 0.05003496880333585
test25 0.05344388242413493
train30 0.04813359406955656 greater than 0.048414256004942854
test30 0.0508110933363233
train35 0.046885661076940234 greater than 0.04711758187180496
test35 0.04893971095789387
train40 0.045825950569880265 greater than 0.046026001683580384
test40 0.04753679513454194
train45 0.04490105735099061 greater than 0.04507674263162152
test45 0.046464694983260044
train50 0.044080967789231686 greater than 0.044237979574952124
test50 0.04564727069233859
train55 0.043337589373902474 greater than 0.043481333688795594
test55 0.045020902471887156
train60 0.042649939431576495 greater than 0.04278350933015212
test60 0.04453023895970458
train65 0.042011055720231456 greater than 0.042135031667619914
test65 0.044115282240535895
train70 0.04141601835522102 greater than 0.041532136498532025
test70 0.0437100879748538
train75 0.040848183779079277 greater than 0.04096074732814185
test75 0.04325997346418787
train80 0.040280481535239325 greater than 0.04039565013881678
test80 0.042741597896917054
train85 0.039673786592726606 greater than 0.03980070336460155
test85 0.042175297191324915
train90 0.03897347845444472 greater than 0.03912386156430973
test90 0.04159516162422986
train95 0.03812597006826738 greater than 0.03830896323176519
test95 0.04098594894951077
train100 0.037112099630073454 greater than 0.03732713885874791
test100 0.04015808440974328
train105 0.03596316222924946 greater than 0.03620193364017183
test105 0.03883843108310707
train110 0.03470275447451764 greater than 0.03496357117822189
test110 0.03711335669898507
train115 0.03337147311929326 greater than 0.033635660953795526
test115 0.035455063286493387
train120 0.0321160520385131 greater than 0.03236167935919957
test120 0.034248187963343966
train125 0.030878481747281028 greater than 0.03112634118265465
test125 0.03329762374414478
train130 0.029677152341232856 greater than 0.029910581750030196
test130 0.032539409568322714
train135 0.028561110192462293 greater than 0.028779128549112763
test135 0.03201579124899745
train140 0.027494821617308214 greater than 0.027704010406757185
test140 0.031655315551119736
train145 0.02647746041058043 greater than 0.026671738897994637
test145 0.0314224731229374
train150 0.025637866364514206 greater than 0.025789400651244073
test150 0.03128728901886019
train155 0.02495909115078625 greater than 0.02508592973845583
test155 0.03120785314038221
train160 0.024357642489985978 greater than 0.024478931050151913
test160 0.031144737581947017
train165 0.02370611732322471 greater than 0.023857089435499385
test165 0.03103477283425595
train170 0.022912600425201834 greater than 0.023058888690379267
test170 0.030892509660446148
train175 0.022137143874593215 greater than 0.022273142769300463
test175 0.03082578516942883
train180 0.021609546915265627 greater than 0.021707520722661487
test180 0.030867313225943722
train185 0.021128786927843684 greater than 0.021225593540384145
test185 0.030970150976090117
train190 0.020626200940538665 greater than 0.02072931438293998
test190 0.031062225757073066
train195 0.02009693485130132 greater than 0.02020401721360276
test195 0.03111206936945242
train200 0.019562231430238438 greater than 0.019668386962991394
test200 0.031128716107473337
train205 0.019047759289728846 greater than 0.01914787700013813
test205 0.031143628493924328
train210 0.01857102512921499 greater than 0.018663416635896605
test210 0.031193701079132424
train215 0.01812140627635576 greater than 0.018210573256935206
test215 0.0313005625353534
train220 0.017666094175758852 greater than 0.01775943232020691
test220 0.031459707807787056
train225 0.017175487340903715 greater than 0.01727628650172689
test225 0.03163667651743532
train230 0.016673648670388425 greater than 0.01677310120833393
test230 0.03177693944217656
train235 0.01617234036822296 greater than 0.016273358080800545
test235 0.03188466743648782
train240 0.015643239291368318 greater than 0.015754182411172418
test240 0.032049590020881154
train245 0.015113542631533531 greater than 0.015209851018730272
test245 0.03249121989934485
train250 0.014716461556258284 greater than 0.01478587889447214
test250 0.03292792421846618
train255 0.014420212646679104 greater than 0.014473352158524763
test255 0.03311911492433707
train258 0.014369282779113972 greater than 0.014346880656636692
test258 0.033353478032751335
3 Accuracy:0.9191419141914191 Lower Bound:0.9037937027025817 Upper Bound:0.9344901256802565
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: True
train0 0.12548374509445692 greater than 1
test0 0.115398751992852
train5 0.06955721389427146 greater than 0.07456732608556164
test5 0.08407800616948552
train10 0.05850217347380717 greater than 0.05976024759456856
test10 0.07113729858639907
train15 0.05421725154234328 greater than 0.05491109045018683
test15 0.06307922692535066
train20 0.05145931940399696 greater than 0.05193381620111868
test20 0.05729664304357184
train25 0.049489045605451816 greater than 0.0498384280475133
test25 0.05356016447783184
train30 0.047979941584862094 greater than 0.048254877612572133
test30 0.0510243306968147
train35 0.046749950016677574 greater than 0.04697976416965997
test35 0.04919099282996533
train40 0.045692190604254605 greater than 0.04589274966178583
test40 0.04780641606956492
train45 0.04475989283584411 greater than 0.0449377449704394
test45 0.0467635725645312
train50 0.043920206705683626 greater than 0.044082663681302814
test50 0.0459716237358069
train55 0.04313437850912311 greater than 0.04328863823773475
test55 0.04532678726216671
train60 0.042383680815549926 greater than 0.04253091229712934
test60 0.04471515792270633
train65 0.041667373465960174 greater than 0.041808579037302555
test65 0.04402787693460492
train70 0.04095852311260048 greater than 0.04110236030183172
test70 0.043221633252054754
train75 0.04018876395137788 greater than 0.04035241688745703
test75 0.04232015140952191
train80 0.039245963465282876 greater than 0.03945432147241401
test80 0.041346338331822156
train85 0.03804063509581294 greater than 0.03830218095375356
test85 0.040235154122645236
train90 0.03662468877521577 greater than 0.03691917960504412
test90 0.038612341021441995
train95 0.03512406005353408 greater than 0.035420094328265074
test95 0.03660791642636068
train100 0.033790615384483816 greater than 0.034039030364342374
test100 0.035056705338347635
train105 0.03256907044469142 greater than 0.03281705323402569
test105 0.033880158671712485
train110 0.03132751203746498 greater than 0.031572828093888346
test110 0.03297560478609857
train115 0.030035125754423093 greater than 0.030307735534214546
test115 0.032416654684088005
train120 0.028657813366146062 greater than 0.028923328665930243
test120 0.032100636894678784
train125 0.027502032590790572 greater than 0.02771263922530112
test125 0.03183525630635226
train130 0.02654222749284566 greater than 0.026723820713602353
test130 0.031611063162779514
train135 0.02569059986340672 greater than 0.025857474189466868
test135 0.03139830661189114
train140 0.02484301389162571 greater than 0.024993388599817073
test140 0.0312874851204267
train145 0.02421951457075163 greater than 0.02430016946863527
test145 0.03125247597623564
train150 0.023798809887777977 greater than 0.023937011555977206
test150 0.031120111255987595
train155 0.02295335759931206 greater than 0.02306075749341653
test155 0.031075800356129137
train160 0.02267940297394387 greater than 0.022756316400954817
test160 0.031084715393323383
train165 0.022172494274558367 greater than 0.02225173816936268
test165 0.031180140907022716
train170 0.02178486374949552 greater than 0.021871146953460416
test170 0.03138722630849486
train175 0.021059210013934293 greater than 0.021219315241976292
test175 0.0316087381016772
train180 0.020366015695979723 greater than 0.020494926837574888
test180 0.031779310404172534
train185 0.019784406040475048 greater than 0.019900242992502357
test185 0.03192878108994011
train190 0.019128947716597268 greater than 0.019253366911245955
test190 0.03201988670490516
train195 0.018576706910175492 greater than 0.01868692211692492
test195 0.03209165010584264
train200 0.018026334791575224 greater than 0.018135787790362224
test200 0.03221596633006409
train205 0.01749316064830117 greater than 0.017598231493446897
test205 0.032406499735660295
train210 0.016968288100994267 greater than 0.017073173030344172
test210 0.03257397019424547
train215 0.0164604516620623 greater than 0.01655763230968426
test215 0.03266789311458483
train220 0.0160339115412368 greater than 0.016111387981469893
test220 0.032753284504648955
train225 0.015656085083687266 greater than 0.015734187039763425
test225 0.03297073303519841
train230 0.015277632574524639 greater than 0.015348544612125621
test230 0.033223147021384486
train235 0.014910328527664687 greater than 0.01499159119095898
test235 0.033467036569893194
train240 0.014417131525383023 greater than 0.01452586973920202
test240 0.033657468480946785
train245 0.01401906513269257 greater than 0.014083575834211343
test245 0.0338032290124828
train250 0.013737235057308787 greater than 0.01378919943279294
test250 0.03396235091870208
train255 0.013475926697114707 greater than 0.01352877373226013
test255 0.0341213854194869
train260 0.013242358167684638 greater than 0.013282507458251642
test260 0.03434471256146563
train265 0.013134588015952858 greater than 0.013142287920057496
test265 0.03470089575944164
train267 0.013131375437682362 greater than 0.013131369339527538
test267 0.034820760862507995
3 Accuracy:0.9158415841584159 Lower Bound:0.9002114136085113 Upper Bound:0.9314717547083204
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: False
train0 0.12657542292432003 greater than 1
test0 0.11931818584798565
train5 0.07726159013651991 greater than 0.08246732459979914
test5 0.06504131091959689
train10 0.06350321166768534 greater than 0.0651670639438674
test10 0.05324301637286386
train15 0.05832056161458419 greater than 0.059103113048967836
test15 0.04926427551189498
train20 0.05524976930808048 greater than 0.05577995739432616
test20 0.04728764671767457
train25 0.05303178316252235 greater than 0.05342744085868412
test25 0.045886053274273364
train30 0.05130934387825154 greater than 0.051624642837948195
test30 0.04478301606830075
train35 0.049898647926781003 greater than 0.05016144925048696
test35 0.043883648655533895
train40 0.04869642986860605 greater than 0.04892387590349226
test40 0.04312839658687777
train45 0.04763183673567432 greater than 0.04783654138349476
test45 0.04246682549170307
train50 0.046650555456413646 greater than 0.04684237679225528
test50 0.0418496622790199
train55 0.0457067931200293 greater than 0.04589472757440049
test55 0.041224008955905646
train60 0.04475459407206961 greater than 0.04494791034910042
test60 0.040535232562266474
train65 0.043751241216295 greater than 0.043957573982033274
test65 0.03973569273013993
train70 0.042671954767161145 greater than 0.04289420575690953
test70 0.03878570236533872
train75 0.04151153612888488 greater than 0.04175067992129903
test75 0.0376621650046235
train80 0.04023807201971941 greater than 0.04050550157878972
test80 0.03638682186752181
train85 0.03877814370215544 greater than 0.03908774796532555
test85 0.03499969743571201
train90 0.037083808535749725 greater than 0.03744242593763694
test90 0.033545926215312626
train95 0.03514552144982147 greater than 0.03553818983251979
test95 0.03222649605085834
train100 0.03342970689226205 greater than 0.03374373551595159
test100 0.031314818048199376
train105 0.031969744274878954 greater than 0.0322520993076784
test105 0.030770741378140266
train110 0.030619175346439785 greater than 0.0308795153750685
test110 0.030449005380505094
train115 0.029386339232192766 greater than 0.029621879931325028
test115 0.0302460087535819
train120 0.028388524221717597 greater than 0.028564630034981716
test120 0.03012692669938453
train125 0.02760250992727123 greater than 0.027751147833736084
test125 0.030088066017839886
train130 0.02690555735271224 greater than 0.027038365833450904
test130 0.030062225965480468
train135 0.026290657683457654 greater than 0.026408059504161318
test135 0.030015715036554688
train140 0.025714044068600127 greater than 0.025831108018602698
test140 0.029946170977814775
train145 0.025105492258669374 greater than 0.025226852608748157
test145 0.02989204723232481
train150 0.02451430129213102 greater than 0.024632484023042387
test150 0.02988688763420182
train155 0.023922661227173327 greater than 0.024039686820611392
test155 0.029939867486040897
train160 0.023375225921280884 greater than 0.023476789846648288
test160 0.03002170055556037
train165 0.023073854492318874 greater than 0.023104840034700088
test165 0.030017849096320298
train170 0.022766160304686724 greater than 0.022838872856923736
test170 0.029829722971445576
train175 0.02243629701325462 greater than 0.022494918365386644
test175 0.029761314759478692
train180 0.022119658673091805 greater than 0.022196331612487914
test180 0.02964907281739934
train185 0.021752556131237718 greater than 0.021693562514962118
test185 0.02938469628890311
4 Accuracy:0.9273927392739274 Lower Bound:0.9127835325902323 Upper Bound:0.9420019459576225
Loss calculated for layer count:1 node count: 25 stepSize:0.15 Momentum: True
train0 0.12657542292432003 greater than 1
test0 0.11931818584798565
train5 0.07698169243280288 greater than 0.08223015503899822
test5 0.0642877459031027
train10 0.06331360504284834 greater than 0.06493050067160609
test10 0.05282384294289208
train15 0.058216815997021866 greater than 0.058999538408979216
test15 0.04906663796687753
train20 0.055141423744500845 greater than 0.055669531604479634
test20 0.047145684592714544
train25 0.05293106845713199 greater than 0.053326215933000916
test25 0.04575424628652295
train30 0.05120595933483595 greater than 0.05152235440386958
test30 0.044652680410570696
train35 0.0497856710440697 greater than 0.05005099897294039
test35 0.04375220948201405
train40 0.048564610697215335 greater than 0.04879679729439681
test40 0.04298885355575476
train45 0.047466052148472625 greater than 0.047679179086985345
test45 0.04229941517324011
train50 0.04642510994337018 greater than 0.046631616026536114
test50 0.041619875867981965
train55 0.045381809689077277 greater than 0.045593369500780474
test55 0.04088819607967884
train60 0.04428493256794434 greater than 0.04451028503482356
test60 0.04005098935355439
train65 0.043111081110689176 greater than 0.04335187655620218
test65 0.03906415576489107
train70 0.04186400802574317 greater than 0.04211952315167376
test70 0.0379019653148887
train75 0.0405245679400475 greater than 0.04080174685529291
test75 0.036581472759892576
train80 0.03906172224907646 greater than 0.03936583012742279
test80 0.0351141727581326
train85 0.037404925956901376 greater than 0.0377577145462974
test85 0.03354554194116393
train90 0.03549506473569097 greater than 0.03589842449486246
test90 0.03218392365712944
train95 0.033541375672818695 greater than 0.033888483954898144
test95 0.03127589581704198
train100 0.03199931749306918 greater than 0.032293666587950985
test100 0.03074576711294415
train105 0.030559711112271466 greater than 0.03084213499233745
test105 0.03044441015576104
train110 0.029228396372612125 greater than 0.029476128935946748
test110 0.030295008679689563
train115 0.028223202746524886 greater than 0.02839930332191506
test115 0.030279296965236117
train120 0.027415424766663667 greater than 0.027571783446726936
test120 0.03033736704290643
train125 0.02666569055778667 greater than 0.02680953667599647
test125 0.030415380025676085
train130 0.02600620446127613 greater than 0.026130648781137942
test130 0.030478336505541798
train135 0.025419618851166767 greater than 0.025533627381053487
test135 0.03050970190147764
train140 0.02486105026918378 greater than 0.024971551340360793
test140 0.030564909950612004
train145 0.024324094497440656 greater than 0.024428292120465253
test145 0.030750861168424237
train150 0.023844863689137023 greater than 0.023934115531450254
test150 0.03114719335439731
train155 0.02344792304165763 greater than 0.0235224778602783
test155 0.03173663599854855
train160 0.023065893711960803 greater than 0.0231465251149337
test160 0.032483634706487914
train165 0.022608485352451593 greater than 0.022706912089468463
test165 0.03331145856313252
train170 0.022090844892736362 greater than 0.02219650022789737
test170 0.03415973951119314
train175 0.02153859534460863 greater than 0.02165306515875987
test175 0.034946504434210664
train180 0.021097267095681275 greater than 0.021100706251932683
test180 0.036465415359425636
train185 0.02075235148944948 greater than 0.020856178972147248
test185 0.03709944554695495
train190 0.020386898307432413 greater than 0.020441395206141046
test190 0.03672834780731116
train195 0.020069874431521164 greater than 0.020149034083296817
test195 0.03636472027991495
train200 0.01961037463931905 greater than 0.019696867648430403
test200 0.03607639848698718
train205 0.01924257160545786 greater than 0.01931008244610904
test205 0.036013746432312754
train210 0.01880208637012248 greater than 0.018923207342759714
test210 0.03619711544847105
train215 0.01825697263155339 greater than 0.01833420118243499
test215 0.03650364151165898
train220 0.017888514309095643 greater than 0.017957344359104496
test220 0.03677963670354131
train225 0.017539472090435514 greater than 0.017613202895439814
test225 0.03686018127185758
train229 0.017327158820222503 greater than 0.01730705115774053
test229 0.037455753834342034
4 Accuracy:0.9084158415841584 Lower Bound:0.8921769160428032 Upper Bound:0.9246547671255136
