Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: False
train0 0.11953882973274317 greater than 1
test0 0.10111542987534493
train5 0.06137666333369153 greater than 0.06443454790519793
test5 0.06726265748036124
train10 0.053380243754568325 greater than 0.05447814485195035
test10 0.05310583437227271
train15 0.04939595581501345 greater than 0.05005419399514229
test15 0.046459282048172734
train20 0.04655963985713982 greater than 0.04709953304759782
test20 0.04221910455129024
train25 0.043710668872491865 greater than 0.044302549879111426
test25 0.038300257677268065
train30 0.040747037670574235 greater than 0.04132539556025374
test30 0.03461580944775638
train35 0.03814784663960084 greater than 0.03862657439110998
test35 0.032390208335501186
train40 0.036195546442672956 greater than 0.03653017325708878
test40 0.031602802147435785
train45 0.034818562575783966 greater than 0.035061910972965814
test45 0.03185422417113425
train50 0.0337204218898675 greater than 0.03392963213283297
test50 0.03313855992038152
train55 0.03269249523416393 greater than 0.03288979726345214
test55 0.03514771315914406
train60 0.03181676572950376 greater than 0.03200074545667182
test60 0.036520395157929327
train65 0.031060673196399602 greater than 0.031187265279196758
test65 0.0376613462820201
train70 0.03047362176183329 greater than 0.030586941754806366
test70 0.03850077670595597
train75 0.029922937914966728 greater than 0.030030084911522485
test75 0.038889537610998115
train80 0.029396995873562898 greater than 0.029501089275734152
test80 0.038725752766999945
train85 0.028915991478570057 greater than 0.02900881884572106
test85 0.03811755569507668
train90 0.028461297012692104 greater than 0.028550481676307256
test90 0.03752131892202368
train95 0.02803656754653418 greater than 0.02811869099834729
test95 0.03704073644936721
train100 0.027639974753765412 greater than 0.02771801272827663
test100 0.03658889272466595
train105 0.027254574277603364 greater than 0.027330993624186707
test105 0.03616850141975146
train110 0.026882136821128316 greater than 0.02695519003653958
test110 0.03583110927299642
train115 0.026505050235297166 greater than 0.026583870560647784
test115 0.035607085565339286
train120 0.026089016925585788 greater than 0.02617450378708581
test120 0.03556620928626151
train125 0.025652562030548446 greater than 0.025740675644408035
test125 0.03571558981893473
train130 0.02519485675726269 greater than 0.02528862755815912
test130 0.03598220424583299
train135 0.024716800453375064 greater than 0.024813496050302846
test135 0.036284314866392664
train140 0.024228448071337904 greater than 0.02432667559105055
test140 0.03659735023776465
train145 0.02374140873780273 greater than 0.023834913475634045
test145 0.036912070591488655
train150 0.023320637952120932 greater than 0.02340020247233781
test150 0.03718598550781423
train155 0.02292445881498148 greater than 0.023004814968246762
test155 0.037557975439932265
train160 0.02250820866551736 greater than 0.02260663248819085
test160 0.03801075744439042
train165 0.022095391635249245 greater than 0.022167898273894503
test165 0.038258192762410395
train170 0.021780953581306686 greater than 0.02183978516325266
test170 0.03837689117701239
train175 0.02152467216099454 greater than 0.021576795316818415
test175 0.03857511936622422
train180 0.02118599570103896 greater than 0.02125222313122232
test180 0.03915050822257964
train185 0.02089259724549352 greater than 0.020850065944705863
test185 0.03989020279939571
0 Accuracy:0.9001650165016502 Lower Bound:0.8832875520445865 Upper Bound:0.9170424809587138
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: True
train0 0.11953882973274317 greater than 1
test0 0.10111542987534493
train5 0.060772125649932986 greater than 0.06398414585272688
test5 0.06607475365062707
train10 0.05278231874764586 greater than 0.053878776289555244
test10 0.05166248351366478
train15 0.04868745105232059 greater than 0.049384682620806745
test15 0.04527018538320969
train20 0.04547613096524666 greater than 0.046116462667561975
test20 0.040413778481328745
train25 0.04224635680864716 greater than 0.04287899540114379
test25 0.035839952931514554
train30 0.03934795388561155 greater than 0.03990569664383875
test30 0.03271407751470309
train35 0.036940100052490675 greater than 0.037338634067954936
test35 0.03153569983269794
train40 0.03528825042911794 greater than 0.03558643482614278
test40 0.03150113427606532
train45 0.03403072432130718 greater than 0.03426156084873253
test45 0.03297269181893337
train50 0.032907503577195685 greater than 0.03313995404515097
test50 0.03515968295153651
train55 0.031922302426423875 greater than 0.03211630337729157
test55 0.03588635216783658
train60 0.031125923946141585 greater than 0.03122951705347477
test60 0.036369029189476015
train65 0.03049038692319521 greater than 0.030623591739179175
test65 0.035784440989219374
train70 0.029852313310987665 greater than 0.029977034790473808
test70 0.035119518050230954
train75 0.029240572061711222 greater than 0.029360514662122243
test75 0.035220005759458314
train80 0.028636748281126615 greater than 0.028755260428465206
test80 0.03588332808182288
train85 0.028100541946116362 greater than 0.028200457281744983
test85 0.035881027532200016
train90 0.027631264257250994 greater than 0.02772143431699571
test90 0.035203660193754496
train95 0.027171319535641888 greater than 0.027265810080360966
test95 0.03577458791479214
train100 0.026759379250823872 greater than 0.026791478144269812
test100 0.03662708827084098
train105 0.026299597154824043 greater than 0.026487438717935553
test105 0.03613843632293099
train110 0.025640054987796774 greater than 0.02577312602855173
test110 0.03609766441052998
train115 0.025052753254800376 greater than 0.0251563057606836
test115 0.03660731706936041
train120 0.024692425365151587 greater than 0.024731612372995905
test120 0.0371007554648895
train125 0.024403147805287146 greater than 0.02448279279047375
test125 0.03712497722141246
train129 0.024375340808386946 greater than 0.024287210413528906
test129 0.037152390301956224
0 Accuracy:0.9067656765676567 Lower Bound:0.8903959958211952 Upper Bound:0.9231353573141182
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: False
train0 0.12026386356170993 greater than 1
test0 0.10392530332908129
train5 0.0643715280176062 greater than 0.06789392008935657
test5 0.07255367520756055
train10 0.055714138960911104 greater than 0.056897736497102
test10 0.061599082301990134
train15 0.05152825071797937 greater than 0.05221116435217462
test15 0.053209198119020244
train20 0.048740579992256484 greater than 0.04923361315051493
test20 0.04795077102645375
train25 0.046572271714037224 greater than 0.046976032963166436
test25 0.044358061189644665
train30 0.0446748868069049 greater than 0.04504328269637621
test30 0.041274662513832834
train35 0.04284919734552325 greater than 0.04321658670806909
test35 0.03830165015198649
train40 0.04093530948388363 greater than 0.0413305345304575
test40 0.03587552725344936
train45 0.03895341696789072 greater than 0.03933782604207708
test45 0.034229142499369584
train50 0.03724209121610431 greater than 0.037562063737222665
test50 0.032994347152147045
train55 0.03559960233985479 greater than 0.03593808680959983
test55 0.03200039217763691
train60 0.03412803042355402 greater than 0.034395102400306694
test60 0.03110922867407249
train65 0.0328231191676398 greater than 0.03307858688841745
test65 0.030382899631124786
train70 0.031633248316960814 greater than 0.03185718715764672
test70 0.029958261148163728
train75 0.030650641949741483 greater than 0.030827686384675125
test75 0.02969888140302711
train80 0.029859456974582213 greater than 0.030010113710663016
test80 0.029495709097568067
train85 0.02914077118369075 greater than 0.0292790031514496
test85 0.029291239744861898
train90 0.028492887121198983 greater than 0.02861983353837208
test90 0.029107403458619372
train95 0.0278772838261799 greater than 0.027997606449478452
test95 0.029036036035313458
train100 0.02729556356804595 greater than 0.02740967414242841
test100 0.029104508627290616
train105 0.026680772251844714 greater than 0.02681061903390089
test105 0.029251429229674903
train110 0.026065142869985524 greater than 0.026174063494231008
test110 0.029347583428768732
train115 0.025540822447936498 greater than 0.02564383110860021
test115 0.02929949911456731
train120 0.02500853828478422 greater than 0.025127037544758417
test120 0.029162582702104523
train125 0.024413990176006654 greater than 0.024526277699735772
test125 0.029082570282044206
train130 0.023903953262895104 greater than 0.024000473440435696
test130 0.029129030535737135
train135 0.023483623311783153 greater than 0.02355939307943806
test135 0.029262268285213035
train140 0.023120463012204134 greater than 0.023194088454905212
test140 0.029508454353290112
train145 0.022747241735184925 greater than 0.022822364363470553
test145 0.02954432334484146
train150 0.02235671233823858 greater than 0.022453560626083672
test150 0.02915183439058232
train155 0.02188448851863227 greater than 0.021986221823526397
test155 0.028893892390020652
train160 0.02134484847711909 greater than 0.021446480898977653
test160 0.02872136568550511
train165 0.020904331494377565 greater than 0.0209891857045595
test165 0.02865040075778559
train170 0.020492992662384114 greater than 0.02057337329667496
test170 0.02875565266090786
train175 0.020040842507579128 greater than 0.020151554993004618
test175 0.02902716445662615
train180 0.019568807323785915 greater than 0.019651357344430593
test180 0.029346951233824102
train185 0.01922792830560732 greater than 0.01928823698140576
test185 0.02993793496844287
train190 0.018966565542988673 greater than 0.01902276255795372
test190 0.031188138412122614
train193 0.01885159406620494 greater than 0.018840515484961472
test193 0.031097835064717493
1 Accuracy:0.9232673267326733 Lower Bound:0.9082822607648923 Upper Bound:0.9382523927004544
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: True
train0 0.12026386356170993 greater than 1
test0 0.10392530332908129
train5 0.06383111552591966 greater than 0.0680870113530268
test5 0.07392748165386945
train10 0.05478685225278157 greater than 0.055902320051232635
test10 0.056947963104615286
train15 0.050735155877634 greater than 0.0514090687698372
test15 0.049884965499337706
train20 0.04801914866730315 greater than 0.048508072648178024
test20 0.046272416948209175
train25 0.04544802230908817 greater than 0.045980072649377875
test25 0.04272063737791213
train30 0.04268614221509156 greater than 0.043195016024167227
test30 0.039395603903322196
train35 0.040041499832970534 greater than 0.04063739873721477
test35 0.03863921419038368
train40 0.03705128724745027 greater than 0.03758704243336838
test40 0.03455461963706659
train45 0.0350490170325281 greater than 0.035372685349829604
test45 0.03550698322470737
train50 0.033803250065916 greater than 0.03400684432749987
test50 0.03426779774900263
train55 0.032819646205234664 greater than 0.033014357504506625
test55 0.03338665247564932
train60 0.03179658321646614 greater than 0.03200251335153967
test60 0.032649013285065885
train65 0.030832344720076506 greater than 0.031018987578904488
test65 0.032124431396219545
train70 0.02987042452624518 greater than 0.03006020991820089
test70 0.031872857554230964
train75 0.029069996244198624 greater than 0.029246389126889268
test75 0.032063782441939866
train80 0.02832795034021121 greater than 0.0285747759420833
test80 0.03280052452905873
train85 0.02732832728097148 greater than 0.027442276044740622
test85 0.032528497601178555
train90 0.026739584787265673 greater than 0.026786043764480898
test90 0.033618412047108345
train91 0.026801593871454026 greater than 0.026739584787265673
test91 0.033497055344001984
1 Accuracy:0.9174917491749175 Lower Bound:0.9020016379834073 Upper Bound:0.9329818603664277
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: False
train0 0.12371779538874197 greater than 1
test0 0.11242873568074321
train5 0.06729407133283512 greater than 0.0717296322015051
test5 0.0784373324874185
train10 0.05733210584201649 greater than 0.058574043365560075
test10 0.06963286662527961
train15 0.053078096587346524 greater than 0.053756205910672886
test15 0.06294074112112356
train20 0.05025391784738653 greater than 0.05076648373784326
test20 0.05632722972208621
train25 0.047927008034006055 greater than 0.04836797837313178
test25 0.05029967154708869
train30 0.04581533617808629 greater than 0.04622854306606011
test30 0.046130006552789246
train35 0.043800908766122165 greater than 0.04419854218161401
test35 0.042821687838977024
train40 0.04180815686202323 greater than 0.0422064741476294
test40 0.03979758172544587
train45 0.03983670317925212 greater than 0.040233338355845465
test45 0.037191009606442886
train50 0.03784949710548551 greater than 0.0382410992972505
test50 0.0351991840995892
train55 0.035936392382087776 greater than 0.036311985071840865
test55 0.03365780175526745
train60 0.0341041523580012 greater than 0.03444524882459229
test60 0.032677113928193426
train65 0.032667912589858426 greater than 0.03292470136044844
test65 0.032135657030353336
train70 0.03151656878829777 greater than 0.031733221135334465
test70 0.03168313619344621
train75 0.03049709968918328 greater than 0.030692888665685113
test75 0.03118826059256441
train80 0.029574174436479816 greater than 0.029754833338136204
test80 0.030798131959785723
train85 0.02865684976320436 greater than 0.028833567954101157
test85 0.03068789009831472
train90 0.027916328214931185 greater than 0.02805200598347991
test90 0.030815370115977493
train95 0.02728532966405693 greater than 0.027406534342388357
test95 0.030970899330180508
train100 0.026605169571332313 greater than 0.02673952200690102
test100 0.031117076313596805
train105 0.02605548647334911 greater than 0.02615905663315941
test105 0.03136029539087362
train110 0.02556609894423382 greater than 0.02566498411729933
test110 0.03157538500830546
train115 0.025081428088929266 greater than 0.025221386897922474
test115 0.0318095139074335
train120 0.024594471154423148 greater than 0.02467624148632309
test120 0.03223290657201603
train125 0.02423781710772979 greater than 0.02429515409468025
test125 0.0325897676569431
train130 0.02408684617087298 greater than 0.024138770148687455
test130 0.03257150984973318
train135 0.02379076117944728 greater than 0.02384668161092177
test135 0.032489711920210364
train140 0.023620922211667766 greater than 0.023651512556670723
test140 0.03251864096546116
train145 0.023446463749236597 greater than 0.023485395805623675
test145 0.032668946878576036
train150 0.023207256178221062 greater than 0.023264594800520396
test150 0.0328744669950955
train155 0.02287336729201179 greater than 0.022942215760914556
test155 0.032947038025066454
train160 0.022552719180767858 greater than 0.022617408106177794
test160 0.0329561058829092
train165 0.0222798162965547 greater than 0.02228503608441218
test165 0.03292206001092872
train166 0.02228265855211988 greater than 0.0222798162965547
test166 0.03279754913621735
2 Accuracy:0.9191419141914191 Lower Bound:0.9037937027025817 Upper Bound:0.9344901256802565
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: True
train0 0.12371779538874197 greater than 1
test0 0.11242873568074321
train5 0.07199740141379571 greater than 0.07634063841068821
test5 0.087467905424919
train10 0.059090570580131334 greater than 0.060547494183027864
test10 0.06941191242994
train15 0.05376987967781927 greater than 0.05470346824565992
test15 0.05672077408236588
train20 0.049284355682405086 greater than 0.05015133388068888
test20 0.052454635285543956
train25 0.046374691707757425 greater than 0.0467551574777553
test25 0.05015648740668981
train30 0.043500507108907176 greater than 0.04407229085951274
test30 0.046134509896372064
train35 0.04063700766341191 greater than 0.04099853127683504
test35 0.04319542521193909
train40 0.03924651022744439 greater than 0.039479309882427635
test40 0.03811691693103747
train45 0.03800890265648737 greater than 0.03814938349386161
test45 0.0384696431670218
train50 0.037323106084429264 greater than 0.03761504355588708
test50 0.03684448876399378
train55 0.03494214696444435 greater than 0.035500473725647665
test55 0.03306026854169169
train60 0.03329484440836947 greater than 0.033440231054122256
test60 0.0317968041825204
train61 0.033312457807501 greater than 0.03329484440836947
test61 0.031777996981511625
2 Accuracy:0.9166666666666666 Lower Bound:0.9011062988741652 Upper Bound:0.9322270344591681
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: False
train0 0.12496877481229351 greater than 1
test0 0.11505301120338425
train5 0.06963229453702131 greater than 0.07440267220423344
test5 0.08338874797738666
train10 0.0587687165472779 greater than 0.06002145829936917
test10 0.07159477119703749
train15 0.05450512054431441 greater than 0.055207891855050664
test15 0.0638012624848427
train20 0.051700403778328234 greater than 0.05218289255533417
test20 0.0575590552551404
train25 0.049674682704163614 greater than 0.05003629358572135
test25 0.05340419373363595
train30 0.048105475614413064 greater than 0.048392133807396244
test30 0.050636318005773916
train35 0.04681874796495476 greater than 0.04705980679497967
test35 0.04870864614253008
train40 0.04569991645348169 greater than 0.04591385999026347
test40 0.04726695612544811
train45 0.04468284826861731 greater than 0.04488059794463627
test45 0.04609889456583596
train50 0.0437189652651736 greater than 0.04390927759677851
test50 0.04509017682026561
train55 0.04278032911576854 greater than 0.042966409409706606
test55 0.04417878783337782
train60 0.04186218126755066 greater than 0.042044263231856775
test60 0.04329788381576305
train65 0.040958980212468875 greater than 0.04113913592273931
test65 0.04236693741045592
train70 0.04004782270374196 greater than 0.04023299918916181
test70 0.04129335158586467
train75 0.039069632864055924 greater than 0.039273985061728356
test75 0.040037084851674995
train80 0.03797167073782761 greater than 0.03820119850126521
test80 0.038696183323133994
train85 0.036776498171180096 greater than 0.03701821051186117
test85 0.03736138044194258
train90 0.035595304519512976 greater than 0.03582745991158088
test90 0.0361098875345986
train95 0.03443653151838396 greater than 0.03467061986662302
test95 0.03509008453216248
train100 0.03321365634413386 greater than 0.033464354816121075
test100 0.034423847752629244
train105 0.03201168279020305 greater than 0.03224084889311729
test105 0.034020307170051646
train110 0.030856199651824973 greater than 0.03109840017073365
test110 0.03367498252877577
train115 0.02964873480628714 greater than 0.02987971628050012
test115 0.033334063058632374
train120 0.028632511982776696 greater than 0.028820882161223797
test120 0.03301709263395513
train125 0.027808561754440287 greater than 0.027956846448732343
test125 0.0326914193793369
train130 0.027105247900732193 greater than 0.027244888381262024
test130 0.032379080184025064
train135 0.026428592250463907 greater than 0.026558908431760807
test135 0.0320499004099604
train140 0.02582582371626112 greater than 0.025940069351563344
test140 0.03170209920839864
train145 0.02528893158519763 greater than 0.025392694624498554
test145 0.03136061762857268
train150 0.024799095910487123 greater than 0.024892051120059752
test150 0.03106526865337465
train155 0.024370257363941047 greater than 0.024453298367733047
test155 0.030862332418126404
train160 0.023931632550726212 greater than 0.02402486343900938
test160 0.030802320044565686
train165 0.0234454050217482 greater than 0.023542287153204517
test165 0.030786503353994275
train170 0.022954781844669814 greater than 0.023055053951883387
test170 0.0308109310649437
train175 0.022440266128193838 greater than 0.02254555319459425
test175 0.03095736214484581
train180 0.02191202960458646 greater than 0.02198616091379881
test180 0.031229858111936744
train185 0.021473788875748235 greater than 0.02158371382178124
test185 0.03139922041558408
train190 0.020950536574483432 greater than 0.021060074859723596
test190 0.031448813570678476
train195 0.020373654653787758 greater than 0.020488687765191603
test195 0.03134636045829084
train200 0.01965632487338656 greater than 0.019801152396265686
test200 0.03118424868322619
train205 0.019096260429502347 greater than 0.019190418878498882
test205 0.03110508179909271
train210 0.01872666667703477 greater than 0.01880297162513419
test210 0.031037773016693512
train215 0.0184227587436667 greater than 0.01847079885291744
test215 0.031013817396020927
train220 0.018151071658304253 greater than 0.01820706421963068
test220 0.031042650761203355
train225 0.017881711057424563 greater than 0.0179343740938699
test225 0.031068184945620327
train230 0.017621129916948714 greater than 0.017673229128508626
test230 0.031089035150261544
train235 0.017363864206709517 greater than 0.017413864009094424
test235 0.031136239452463917
train240 0.01715469706735852 greater than 0.01719441979891901
test240 0.03116943872040376
train245 0.01689127575302555 greater than 0.016959826939599326
test245 0.031223310469500123
train250 0.01662988416963686 greater than 0.01668571104534976
test250 0.031376188626059454
train252 0.016575113569770674 greater than 0.016564899204910923
test252 0.031460150997770775
3 Accuracy:0.9216171617161716 Lower Bound:0.9064853638655583 Upper Bound:0.9367489595667848
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: True
train0 0.12496877481229351 greater than 1
test0 0.11505301120338425
train5 0.06906508743791077 greater than 0.07394842599635766
test5 0.08627292744504811
train10 0.058437148527732706 greater than 0.059731383107662316
test10 0.07162639729772437
train15 0.0540450124349673 greater than 0.05475186854289874
test15 0.06307422023078144
train20 0.05126329795331919 greater than 0.05173965283010228
test20 0.05717396070818611
train25 0.049293872378138114 greater than 0.04964199636723902
test25 0.053444161298080516
train30 0.04778971461249221 greater than 0.048064713305786294
test30 0.05098031228729738
train35 0.0465351303774681 greater than 0.04677441136264481
test35 0.049230926404776325
train40 0.04537917820685467 greater than 0.04560627992987363
test40 0.04793235325894927
train45 0.044290716935211985 greater than 0.044500326552676175
test45 0.04698314502150955
train50 0.04330433882322717 greater than 0.0434947790910117
test50 0.046288010566739604
train55 0.042386037314642466 greater than 0.04256567616273896
test55 0.04570700074475512
train60 0.041514294361114354 greater than 0.04168549317459069
test60 0.04516066669192311
train65 0.04067149107858812 greater than 0.040839270373382856
test65 0.04462671077768273
train70 0.039815081166791735 greater than 0.03999109437995883
test70 0.04398888182778708
train75 0.038828626908146716 greater than 0.039046206852460144
test75 0.04281856946179459
train80 0.03751753504873967 greater than 0.03781255009857161
test80 0.041553683488885766
train85 0.03590756134682365 greater than 0.0362310176163593
test85 0.03999477802323155
train90 0.03438508148187388 greater than 0.034682924594228434
test90 0.03826432714596217
train95 0.032970909710240484 greater than 0.03324709754800143
test95 0.03713039075004787
train100 0.03155012600379932 greater than 0.03184434455915224
test100 0.036518119822580526
train105 0.030056372101566142 greater than 0.030351937789513096
test105 0.03604436651345848
train110 0.02866534367187607 greater than 0.028928667346084933
test110 0.03562839883389067
train115 0.02751194855284454 greater than 0.027718422782769803
test115 0.03510330244366732
train120 0.02671500751724498 greater than 0.02684371655826396
test120 0.03430855118526325
train125 0.025993652612058184 greater than 0.026174279817518693
test125 0.03363776679431738
train130 0.024894446313216276 greater than 0.025113636353238353
test130 0.033159709145767435
train135 0.0239923253561921 greater than 0.024149701700192667
test135 0.03284798577264853
train140 0.023288826497972842 greater than 0.023414506392288363
test140 0.032624648950752164
train145 0.022828238315857015 greater than 0.022920882194267998
test145 0.03239972580087077
train150 0.02239055230895249 greater than 0.022486463490432245
test150 0.03234234701476063
train155 0.021953405633352224 greater than 0.022023998228026115
test155 0.03263238812419968
train159 0.021700102622738814 greater than 0.021660250852391448
test159 0.03260308385607679
3 Accuracy:0.9183168316831684 Lower Bound:0.902897437139886 Upper Bound:0.9337362262264507
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: False
train0 0.12676660515527727 greater than 1
test0 0.12140065747958718
train5 0.07810699192157936 greater than 0.08351844863205307
test5 0.06581386052811602
train10 0.06386607527034681 greater than 0.06555559100127918
test10 0.05343989776415895
train15 0.05862707856551846 greater than 0.05941852268287972
test15 0.049423777121150474
train20 0.05554901382927538 greater than 0.05607745238632448
test20 0.04748094322404444
train25 0.05333391506845493 greater than 0.05373036639240231
test25 0.0461240190700077
train30 0.05159395675269122 greater than 0.05191460013664006
test30 0.04504654782048563
train35 0.050148213554234 greater than 0.0504185063204495
test35 0.04415078755755087
train40 0.048914602043708434 greater than 0.049147040521129524
test40 0.043392612403875326
train45 0.04783607290068973 greater than 0.04804229256784063
test45 0.04274211122810532
train50 0.046852221960055604 greater than 0.047044139456630586
test50 0.04216504181099243
train55 0.04590915439882578 greater than 0.046096883236543845
test55 0.041612320552038486
train60 0.04495584680531897 greater than 0.04514995649282164
test60 0.04101983867154417
train65 0.0439381324602029 greater than 0.04414921266671431
test65 0.040328781412854536
train70 0.042811922817939614 greater than 0.04304734359046917
test70 0.03949415175202258
train75 0.04154998616755116 greater than 0.04181411863382105
test75 0.038478487810244016
train80 0.04013801572955392 greater than 0.040432786899941044
test80 0.03726723601829478
train85 0.038546713499697266 greater than 0.03888392075181898
test85 0.035876044544316185
train90 0.03671434602378705 greater than 0.037094420883737565
test90 0.03435832980991643
train95 0.034874609744746904 greater than 0.03522508865590232
test95 0.03293561231507174
train100 0.03329176014660145 greater than 0.033588076876340495
test100 0.031957969469183646
train105 0.03191163291770036 greater than 0.032177991536913676
test105 0.031406614536490976
train110 0.030632974640546866 greater than 0.03087193925885049
test110 0.031031902646216435
train115 0.02952514106993544 greater than 0.029745658205433557
test115 0.030727260759842133
train120 0.028477009010365668 greater than 0.028670648398515214
test120 0.03053134093277815
train125 0.027673160603088262 greater than 0.02781126043477077
test125 0.030420267010864057
train130 0.027156296507570066 greater than 0.02724073234182059
test130 0.03031466354165272
train135 0.02669372402311718 greater than 0.02679994306747977
test135 0.030196797088374975
train140 0.026058531649709553 greater than 0.026195458458006877
test140 0.030106973879611938
train145 0.02537419635442262 greater than 0.025508166169842354
test145 0.030077262855263624
train150 0.024718112536403658 greater than 0.024848477043837754
test150 0.03004478934722993
train155 0.024069549243720432 greater than 0.02419900685334456
test155 0.030054241335027605
train160 0.023420975243049914 greater than 0.02355054235514579
test160 0.030134672285499108
train165 0.022792246525809464 greater than 0.022914609360356282
test165 0.0302707771375317
train170 0.022174968126957916 greater than 0.022302875257492468
test170 0.03042743968167971
train175 0.021481949763617707 greater than 0.021625758085582848
test175 0.03059369123940038
train180 0.020764191127320277 greater than 0.020906647130954206
test180 0.03078608714113307
train185 0.02005994272852408 greater than 0.020199064681096543
test185 0.031065087124495516
train190 0.019414124567247996 greater than 0.019538860426670374
test190 0.031482025484876774
train195 0.018781175611600646 greater than 0.018902740532893682
test195 0.03174785026721753
train200 0.018158167287055136 greater than 0.01827914730519856
test200 0.031948157602309485
train205 0.01762855424136247 greater than 0.017729644692371246
test205 0.03218103577075884
train210 0.017138414600274616 greater than 0.01723131456378938
test210 0.032509453290658734
train215 0.01678668149189444 greater than 0.016835151757422933
test215 0.03251545634253727
train220 0.016659770960613454 greater than 0.016719283289288036
test220 0.032237592912943355
train225 0.016197490923313438 greater than 0.016293119754218353
test225 0.0320629389366704
train230 0.01568830353820117 greater than 0.01576659826302502
test230 0.032121914350492824
train235 0.01548397996311165 greater than 0.015504014117835179
test235 0.03192504163206407
train240 0.015142813313967149 greater than 0.0152268487679285
test240 0.03181757622309499
train245 0.014860037508648352 greater than 0.014922791024122698
test245 0.0316775710232663
train250 0.014498010994090512 greater than 0.014576602136910664
test250 0.03174696506152466
train255 0.014114188390511552 greater than 0.014204774981346502
test255 0.03187112569495127
train260 0.013619287113986871 greater than 0.01366007575111551
test260 0.03212208397095463
train262 0.013614628362515377 greater than 0.013608967801485659
test262 0.03219003828323281
4 Accuracy:0.9166666666666666 Lower Bound:0.9011062988741652 Upper Bound:0.9322270344591681
Loss calculated for layer count:1 node count: 20 stepSize:0.15 Momentum: True
train0 0.12676660515527727 greater than 1
test0 0.12140065747958718
train5 0.07732675491513731 greater than 0.08302238462987223
test5 0.06250328285664455
train10 0.06331587190958013 greater than 0.06491085307013597
test10 0.0519332870712495
train15 0.05812026164818298 greater than 0.058928033245680175
test15 0.04865562470791339
train20 0.055016005779236966 greater than 0.05554323021467994
test20 0.046875719367063716
train25 0.05280545829526987 greater than 0.053202900796870886
test25 0.045531631174903844
train30 0.05104055528113673 greater than 0.05136884243266395
test30 0.0444473641050383
train35 0.049531356008134395 greater than 0.049818707513285765
test35 0.04354134099242676
train40 0.04815083052938343 greater than 0.04842334936316536
test40 0.04270612721411994
train45 0.046754386045962604 greater than 0.04704187368290101
test45 0.041810305487677675
train50 0.045223783794004165 greater than 0.04554262052226706
test50 0.04074973331956782
train55 0.0435563309131561 greater than 0.04389793953973257
test55 0.03947690956853731
train60 0.0418068095474143 greater than 0.04216042252761178
test60 0.03802177105635503
train65 0.040027948241814056 greater than 0.040385914068451
test65 0.036406949924206636
train70 0.03815514589602068 greater than 0.03854500015895119
test70 0.03469448441308169
train75 0.03616626093848598 greater than 0.03656091248580512
test75 0.03308821683753898
train80 0.034072616738416904 greater than 0.03451395451348684
test80 0.03199252019919769
train85 0.032156520078864874 greater than 0.03249620577838822
test85 0.03164025690107279
train90 0.03061157145067417 greater than 0.03089858895620939
test90 0.031774618389992876
train95 0.029426052733271257 greater than 0.029638903444088382
test95 0.03202927089441681
train100 0.028356590085032222 greater than 0.02857391619338297
test100 0.032446337452942266
train105 0.027234848264058843 greater than 0.027461490379849535
test105 0.03316882693250191
train110 0.026192851920679272 greater than 0.02637547326760171
test110 0.033861263740426376
train115 0.02571957649448846 greater than 0.02571258624051076
test115 0.03418770346408672
4 Accuracy:0.9125412541254125 Lower Bound:0.8966362920559813 Upper Bound:0.9284462161948438
