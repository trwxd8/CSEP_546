Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: False
train0 0.11950153151076288 greater than 1
test0 0.10142454336320472
train5 0.061534415657639376 greater than 0.06451549242785021
test5 0.06779163873547604
train10 0.0537109100401477 greater than 0.05478405246517083
test10 0.05348096742797385
train15 0.04982953797714006 greater than 0.050459845628621755
test15 0.04676504313301958
train20 0.0473442480444143 greater than 0.04777273436950011
test20 0.04280453809946165
train25 0.0454895193471405 greater than 0.04583316649186762
test25 0.04037287745390879
train30 0.04388564908091868 greater than 0.044194150698834846
test30 0.03889443919814852
train35 0.04236044186630815 greater than 0.04266750699248638
test35 0.037576828068705694
train40 0.040782136801824756 greater than 0.04110581590283522
test40 0.03582369713326991
train45 0.03923232382792344 greater than 0.03953047548009826
test45 0.03394649196187243
train50 0.03751990467850204 greater than 0.03789813608252075
test50 0.03220038565029772
train55 0.03550823731661392 greater than 0.035906283910173295
test55 0.03112550643602787
train60 0.03389843305739843 greater than 0.0341707896951664
test60 0.030918464652409137
train65 0.032693987022760114 greater than 0.0329250509298271
test65 0.030937036523401438
train70 0.03154670550376895 greater than 0.03176952379222817
test70 0.031188710623793484
train75 0.030543623530723114 greater than 0.030746495070045947
test75 0.031679257071017264
train80 0.029647950342679877 greater than 0.029811683362243744
test80 0.032782531015187674
train85 0.02887554405999318 greater than 0.029026352798318358
test85 0.03436428836292367
train90 0.02810702160769789 greater than 0.028275079275840224
test90 0.03519738191132301
train95 0.02730713313349365 greater than 0.027457457477603024
test95 0.03537433693982991
train100 0.0265991620658743 greater than 0.02673365520689209
test100 0.03536062068677909
train105 0.026011121523595433 greater than 0.026116866064322355
test105 0.03529680264778963
train110 0.02544369147310929 greater than 0.02556468528487006
test110 0.035234781192590096
train115 0.024882923041673204 greater than 0.024989276048941775
test115 0.03512481545027724
train120 0.024340803253988352 greater than 0.024452700313473207
test120 0.03511114175101097
train125 0.02375827559691926 greater than 0.02387525941225758
test125 0.03543699086699205
train130 0.02318381556050373 greater than 0.023297298578853338
test130 0.035820105239430716
train135 0.02264593019617671 greater than 0.022748419220389372
test135 0.036072330016428227
train140 0.02216553877779352 greater than 0.022258887649452733
test140 0.036195370304851364
train145 0.021753224894883943 greater than 0.021824189422925418
test145 0.03651159933467232
train148 0.021746658876646383 greater than 0.021685612954352094
test148 0.03681529034587876
0 Accuracy:0.9067656765676567 Lower Bound:0.8903959958211952 Upper Bound:0.9231353573141182
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: True
train0 0.11950153151076288 greater than 1
test0 0.10142454336320472
train5 0.0724625681332303 greater than 0.07523330755623017
test5 0.08643617311470249
train10 0.06682309512408556 greater than 0.0677075273469083
test10 0.09322043398006491
train13 0.06737908216461787 greater than 0.06563584977609511
test13 0.06581627839218256
0 Accuracy:0.8523102310231023 Lower Bound:0.8323355975987707 Upper Bound:0.8722848644474339
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: False
train0 0.12253963063942566 greater than 1
test0 0.10955990693365263
train5 0.0648940106454316 greater than 0.06860616804284755
test5 0.07256044729849602
train10 0.0559400664242179 greater than 0.05717431454968388
test10 0.06115350309056436
train15 0.05174522851184738 greater than 0.05242707765775576
test15 0.05314298267936267
train20 0.04904627352522007 greater than 0.049505114742186385
test20 0.04823098600608349
train25 0.04718290346555286 greater than 0.04750793517527571
test25 0.04507814695187913
train30 0.04580168951913783 greater than 0.04605193576824667
test30 0.04304113732995258
train35 0.04465387991635801 greater than 0.044874996950124875
test35 0.041690767097644466
train40 0.04358884593705412 greater than 0.04379469798634025
test40 0.0408291915296053
train45 0.04263618275796305 greater than 0.042816590632240664
test45 0.04023380288986402
train50 0.041782608190428644 greater than 0.041949908907507494
test50 0.03973591709413319
train55 0.04090916714757147 greater than 0.04109179149561917
test55 0.0392032938517902
train60 0.03989852251502529 greater than 0.04011820762674356
test60 0.03852776220648852
train65 0.038668353170747814 greater than 0.03893066721494227
test65 0.037646394192611524
train70 0.03725677958507665 greater than 0.037549289128357324
test70 0.03653267072947183
train75 0.035803335951102515 greater than 0.03608580016650042
test75 0.03548777463735254
train80 0.03450794701100404 greater than 0.034753067439596184
test80 0.03468924501421705
train85 0.033361202021203566 greater than 0.033579365471107084
test85 0.03405647873011715
train90 0.032294054437237124 greater than 0.03250769358270805
test90 0.033511450543934486
train95 0.03115126907855125 greater than 0.031383846702165735
test95 0.033008085414343954
train100 0.030138379223446445 greater than 0.030311081069004226
test100 0.032879749547087195
train105 0.029404442374297318 greater than 0.029542058498799293
test105 0.03289944497973911
train110 0.028718766417987863 greater than 0.02885588509319657
test110 0.032674757998409645
train115 0.028051869596061835 greater than 0.02818025327805927
test115 0.03221721928766969
train120 0.027483066480847543 greater than 0.027586625049877002
test120 0.03170566636989595
train125 0.027031950247511424 greater than 0.027112675949058744
test125 0.03115133281895042
train130 0.026657645226346137 greater than 0.026749214887193944
test130 0.030657869586807353
train135 0.02608924864142597 greater than 0.026203111017405873
test135 0.030592977496532198
train140 0.0255838886349692 greater than 0.025683047822490094
test140 0.03078892275143263
train145 0.025080036419723447 greater than 0.025172554935962918
test145 0.031069849541599307
train150 0.024677742250990585 greater than 0.024760936433794376
test150 0.031410517774499706
train155 0.024236786841801625 greater than 0.024309414243797844
test155 0.03172995841892884
train160 0.023922207801655774 greater than 0.02397555166966697
test160 0.031957062991303575
train163 0.023855562845344958 greater than 0.02385541139194487
test163 0.032033015783006866
1 Accuracy:0.9174917491749175 Lower Bound:0.9020016379834073 Upper Bound:0.9329818603664277
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: True
train0 0.12253963063942566 greater than 1
test0 0.10955990693365263
train5 0.06433712580933283 greater than 0.06883873177022357
test5 0.07386158764264095
train10 0.05524930978229139 greater than 0.05641121018061937
test10 0.05829909111235182
train15 0.05098444877580675 greater than 0.051676140701970005
test15 0.05048078481488835
train20 0.04806770382265842 greater than 0.04859301124557148
test20 0.046403565747391635
train25 0.045553060069232126 greater than 0.04607408064799473
test25 0.04332494807911661
train30 0.04231133541623431 greater than 0.04308137984108037
test30 0.03838264491682144
train35 0.038752886821998546 greater than 0.03947982485067047
test35 0.03566881684532431
train40 0.036200646675737844 greater than 0.036595228000401575
test40 0.04088292873627135
train45 0.03453780135338696 greater than 0.03479798922981037
test45 0.04069505386469705
train50 0.03330040253515328 greater than 0.03355602921180574
test50 0.035976399522127125
train55 0.03197818170252919 greater than 0.03222562358037212
test55 0.030620495814036974
train60 0.031089280162333217 greater than 0.03114791688061366
test60 0.03067434131807692
train65 0.0298084663336234 greater than 0.030069293571602096
test65 0.030804361829635505
train70 0.028766795592724666 greater than 0.02891543720714333
test70 0.03023485340096409
train75 0.02769946592764913 greater than 0.027904141623552293
test75 0.0306134433592756
train80 0.026765829433288816 greater than 0.02691526782813166
test80 0.03137038056951839
train85 0.02604745665167453 greater than 0.0262012995287749
test85 0.033065216017812225
train90 0.024850550314128615 greater than 0.02504182373571565
test90 0.03427402817381385
train95 0.02435727814593098 greater than 0.024379429100516416
test95 0.03297575712484076
train100 0.023396940046629507 greater than 0.023398440213374064
test100 0.03353830488698826
train101 0.023411399041154034 greater than 0.023396940046629507
test101 0.03346322354202409
1 Accuracy:0.9183168316831684 Lower Bound:0.902897437139886 Upper Bound:0.9337362262264507
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: False
train0 0.1235197834866726 greater than 1
test0 0.11298602670476737
train5 0.0672623852788488 greater than 0.07161457238706273
test5 0.07892755144641798
train10 0.05750198597666961 greater than 0.05871982508939199
test10 0.0708090573966519
train15 0.053352659526955656 greater than 0.05401345457112405
test15 0.06425592788069301
train20 0.05062137729625751 greater than 0.051111744273127944
test20 0.057842283346668105
train25 0.04843123264060338 greater than 0.048842369234017934
test25 0.05140533761260212
train30 0.046479100341526816 greater than 0.0468593923351164
test30 0.047191616073882385
train35 0.044652994852146514 greater than 0.04500737881530427
test35 0.04413209663716239
train40 0.042991724902023756 greater than 0.04330906476212106
test40 0.041417440019351846
train45 0.04144893643777939 greater than 0.04175396216827863
test45 0.038943610813013105
train50 0.039980695739968056 greater than 0.0402724053774101
test50 0.0368806484191803
train55 0.03836967324231714 greater than 0.038719113545346046
test55 0.035374848892137975
train60 0.03660039776413123 greater than 0.036945485850513035
test60 0.03427542125116385
train65 0.034974376450139684 greater than 0.03529565504905713
test65 0.03345252909195762
train70 0.03336020451127467 greater than 0.033672824258880414
test70 0.032979011046060464
train75 0.03209962243455307 greater than 0.0323181001203745
test75 0.032730579804297694
train80 0.03113283769201191 greater than 0.03131146075615755
test80 0.03253233349735772
train85 0.03037310283112476 greater than 0.030510370183740913
test85 0.032321830443507386
train90 0.0297492035152648 greater than 0.029871732046007425
test90 0.03211981256879971
train95 0.029094834072168754 greater than 0.02922918343163785
test95 0.03183664124375418
train100 0.028441856429805994 greater than 0.028567633360662984
test100 0.0315001702645037
train105 0.02785733140424827 greater than 0.02796899221443284
test105 0.031194379271465303
train110 0.027329102834325405 greater than 0.02743117597263808
test110 0.030966985848473284
train115 0.026840079838837155 greater than 0.026935259519968096
test115 0.030792400336863836
train120 0.026392924461383083 greater than 0.026477007017652366
test120 0.030652447464140514
train125 0.02601962123146263 greater than 0.02609057155599937
test125 0.030584954688922306
train130 0.02561210409207772 greater than 0.02570460836868914
test130 0.030609436888458744
train135 0.025145277818803977 greater than 0.025218322732260448
test135 0.030682933577061932
train140 0.024916603609406358 greater than 0.024982451392419114
test140 0.03079378618147635
train145 0.024361806036678778 greater than 0.02447014685920989
test145 0.03095980239599765
train150 0.024189691677027118 greater than 0.02424057477838905
test150 0.03106242382809843
train152 0.024524734279759028 greater than 0.02410558446547273
test152 0.031138985754298104
2 Accuracy:0.9232673267326733 Lower Bound:0.9082822607648923 Upper Bound:0.9382523927004544
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: True
train0 0.1235197834866726 greater than 1
test0 0.11298602670476737
train5 0.06698232441112474 greater than 0.07137911294797257
test5 0.08113843012409842
train10 0.05734438563694578 greater than 0.05858181418530071
test10 0.07188617548491472
train15 0.0531126889831312 greater than 0.0537885086909007
test15 0.06437420792539854
train20 0.05022826770728243 greater than 0.05075923554842293
test20 0.05576739164620872
train25 0.047738234763361614 greater than 0.04822353958761782
test25 0.04933689136230759
train30 0.04542575599409866 greater than 0.04587241326492127
test30 0.04500565758595385
train35 0.04329445974230622 greater than 0.04370701741793774
test35 0.041428154032843355
train40 0.04116521380940542 greater than 0.041615054994225464
test40 0.038542006295927946
train45 0.03905299562106272 greater than 0.03946542798245289
test45 0.03634801745283857
train50 0.03708656079732527 greater than 0.03745894802193891
test50 0.03479925545587038
train55 0.03547794105843868 greater than 0.03578283204632536
test55 0.03373036112246719
train60 0.033841100705276156 greater than 0.03415144487125429
test60 0.03321405359829377
train65 0.032456405523493194 greater than 0.03270683123587213
test65 0.03299830310327528
train70 0.03134600362020807 greater than 0.031552414383575776
test70 0.0327178223769583
train75 0.030442802833780282 greater than 0.030607818306069852
test75 0.032215992300572295
train80 0.029665043977013302 greater than 0.02981757008074687
test80 0.03161166676867816
train85 0.028930472846781265 greater than 0.029074304581971015
test85 0.03105341604729826
train90 0.028201491018228296 greater than 0.028350968620891145
test90 0.030636963985519163
train95 0.027475550358762316 greater than 0.027612965769843855
test95 0.03029666763149568
train100 0.026822117068233123 greater than 0.026950262325380923
test100 0.029946634695804268
train105 0.02620950615270913 greater than 0.026326203786488502
test105 0.029607051815465902
train110 0.025710623278723338 greater than 0.025783117269397755
test110 0.029373152203756907
train115 0.024993191023929117 greater than 0.025193937281963848
test115 0.029850002211889114
train120 0.024143259791131504 greater than 0.024269605560152778
test120 0.031184255282473077
train125 0.02361455521022303 greater than 0.023712379763681533
test125 0.03215636663716528
train130 0.02315819498775715 greater than 0.023248986783179996
test130 0.032541950462220096
train135 0.022678589613711882 greater than 0.022770471133679873
test135 0.03274430363857319
train140 0.02225528272931665 greater than 0.0223421368273348
test140 0.03289097349495462
train145 0.02171109270741275 greater than 0.02183829899825946
test145 0.033046535052429764
train150 0.021320099209122135 greater than 0.02135491451756742
test150 0.033144380358249174
train153 0.02128924036873111 greater than 0.021268344910026177
test153 0.03357647395660482
2 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: False
train0 0.12505426037387418 greater than 1
test0 0.1167529042343373
train5 0.06965578397746455 greater than 0.07439655281845103
test5 0.08481991730002524
train10 0.058894997009895075 greater than 0.060168341832428994
test10 0.07202165444302586
train15 0.05468785486675582 greater than 0.055358368428639095
test15 0.0644154071736275
train20 0.051979649026098494 greater than 0.05245416535744194
test20 0.05840839801582285
train25 0.0499534455485118 greater than 0.05031894470796586
test25 0.05412901180425136
train30 0.04835541103984177 greater than 0.04864845123161153
test30 0.051162620374342306
train35 0.0470396805773878 greater than 0.04728554288458469
test35 0.04907229045227436
train40 0.04591181979297642 greater than 0.04612519298845051
test40 0.04755475786857106
train45 0.0449224219232434 greater than 0.0451107611375821
test45 0.04640589185511633
train50 0.04403991502822335 greater than 0.04420941447337521
test50 0.04550801823183799
train55 0.043232082612085325 greater than 0.04338905992924399
test55 0.04479678226470559
train60 0.04247644511091552 greater than 0.0426237708962292
test60 0.04421941328564464
train65 0.04176675469001597 greater than 0.041905368658225625
test65 0.043715929451250704
train70 0.04109037258927836 greater than 0.041224181670595106
test70 0.043214017338728136
train75 0.04041786134497322 greater than 0.040553953008740234
test75 0.04262530473092209
train80 0.03970501424818403 greater than 0.03985350399696556
test80 0.04186657229415393
train85 0.03888927521253924 greater than 0.03906428111098862
test85 0.04089684762334835
train90 0.037904311450088775 greater than 0.038116040553213076
test90 0.03972293161014984
train95 0.03676294866377208 greater than 0.03699961078763404
test95 0.03841413191693319
train100 0.03554560904635626 greater than 0.035792571192502266
test100 0.03704353201852622
train105 0.0342973724671821 greater than 0.034547951158641885
test105 0.03573120607348911
train110 0.033041180445301896 greater than 0.03329184449872645
test110 0.03469211445436132
train115 0.031807254656224984 greater than 0.03205431387170423
test115 0.03399588801541889
train120 0.030621745735611484 greater than 0.03084590181520512
test120 0.03352367589534926
train125 0.02951903275245197 greater than 0.02974423717448185
test125 0.03309416768833161
train130 0.028363901467951962 greater than 0.02858661725591279
test130 0.032721564225588945
train135 0.027419914430893852 greater than 0.02759308874616302
test135 0.03247093890132345
train140 0.026631715853219818 greater than 0.026778608288698925
test140 0.03227815126305917
train145 0.025964292826759258 greater than 0.02609181112014875
test145 0.032047801051739044
train150 0.025328768458342928 greater than 0.02545703650679249
test150 0.031756011160199916
train155 0.02467472540506478 greater than 0.024807195977831348
test155 0.03145603153792176
train160 0.023994898294027144 greater than 0.024134089968118597
test160 0.03126631119677844
train165 0.02325567523780002 greater than 0.023409242214045417
test165 0.03128201501389502
train170 0.022519671352106065 greater than 0.02265765904027578
test170 0.031533345594949636
train175 0.02187707327649045 greater than 0.022002369281287792
test175 0.031969278955765124
train180 0.02124282718797413 greater than 0.021371874113373317
test180 0.03238661923513663
train185 0.02059041181872561 greater than 0.020720392289731014
test185 0.03274880501131943
train190 0.0199768886444557 greater than 0.020092956587158917
test190 0.03316078278681148
train195 0.019447694274222518 greater than 0.01954669824054733
test195 0.033429026683668296
train200 0.018990378321553942 greater than 0.01907866163290847
test200 0.03363219098244934
train205 0.01855324374395942 greater than 0.018640235594087683
test205 0.033792745259761066
train210 0.018134185343136714 greater than 0.01821492218129997
test210 0.03390277403353417
train215 0.017756355877434075 greater than 0.017828925764766544
test215 0.03394663050087863
train220 0.017407505299139803 greater than 0.017475753532401786
test220 0.03391590414234426
train225 0.01707689824870349 greater than 0.017141372658865277
test225 0.03384826728722048
train230 0.016775965077976938 greater than 0.0168338288385954
test230 0.03380340047011452
train235 0.016451657171145117 greater than 0.016525045783252132
test235 0.033821755411658355
train240 0.016037955315891014 greater than 0.016121397029906278
test240 0.033928696924716485
train245 0.01564789595091327 greater than 0.01572426913696511
test245 0.034080770818888016
train250 0.015256422405858634 greater than 0.015335382569701248
test250 0.03427686464416172
train255 0.014864238582381971 greater than 0.014942501523393822
test255 0.034491774137397395
train260 0.014461060085056179 greater than 0.014544469176512507
test260 0.03468764718928219
train265 0.013997612048726616 greater than 0.014097107481953466
test265 0.03492409919031345
train270 0.013510398916677642 greater than 0.013602298284547789
test270 0.03517127258714153
train275 0.013168582380475947 greater than 0.013202906679754151
test275 0.035574428575185446
train280 0.01303661992740591 greater than 0.013081599951978856
test280 0.035953716859424015
train285 0.01275808518222648 greater than 0.012785602152706264
test285 0.036393747893090835
train290 0.012483595462679582 greater than 0.012555103996520477
test290 0.03653875823240018
train294 0.012422690033867029 greater than 0.012288854296232275
test294 0.03638858421956266
3 Accuracy:0.9042904290429042 Lower Bound:0.8877275277644201 Upper Bound:0.9208533303213884
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: True
train0 0.12505426037387418 greater than 1
test0 0.1167529042343373
train5 0.06932895026616154 greater than 0.07443830758551297
test5 0.09402705868363244
train10 0.05836249869038208 greater than 0.059660072500616945
test10 0.07225817835401284
train15 0.0538303888293428 greater than 0.054552688788931165
test15 0.06372347985339827
train20 0.05100805082176849 greater than 0.05149609114951548
test20 0.05769379315234316
train25 0.048830622664293465 greater than 0.04924986979270755
test25 0.052265904442470805
train30 0.04655253418468312 greater than 0.04705107756790712
test30 0.047900506524940635
train35 0.043688054827875376 greater than 0.04430309817736599
test35 0.04498011797222512
train40 0.040715062752348416 greater than 0.04126420906275379
test40 0.042336069911410555
train45 0.038237748492326445 greater than 0.038710132659853076
test45 0.03899215433877416
train50 0.036118941253466756 greater than 0.036492808374217485
test50 0.03653072975246714
train55 0.03423009450507623 greater than 0.03459604055252758
test55 0.034683089272904664
train60 0.032290175456294236 greater than 0.0326595054143864
test60 0.03602032228128398
train65 0.03051415835341845 greater than 0.030885554954452332
test65 0.03806440250085706
train70 0.028833330835399917 greater than 0.0291363590208966
test70 0.038391200993064645
train75 0.027583399757591987 greater than 0.0278587226580968
test75 0.03560598706769788
train80 0.02611231009807247 greater than 0.02633688562142251
test80 0.03315999755737945
train85 0.025395994081879928 greater than 0.02548880075607575
test85 0.03208742865017311
train87 0.025350725037983607 greater than 0.025341461774210015
test87 0.03194342119562352
3 Accuracy:0.9158415841584159 Lower Bound:0.9002114136085113 Upper Bound:0.9314717547083204
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: False
train0 0.12675159024640797 greater than 1
test0 0.12221110966078455
train5 0.07865868418111538 greater than 0.08416199757290124
test5 0.06656417280538734
train10 0.06442904010650433 greater than 0.06610642089905229
test10 0.05408184069168529
train15 0.05918033839619985 greater than 0.059981317867886276
test15 0.050020328991691926
train20 0.05607335388504262 greater than 0.05660390844159164
test20 0.04805924788839545
train25 0.053837079791365726 greater than 0.05424118803991699
test25 0.04662701038265396
train30 0.05200025044478745 greater than 0.052350678765959106
test30 0.04535788384722053
train35 0.050283492987510496 greater than 0.050627065140634205
test35 0.04409390055186142
train40 0.04851498186210174 greater than 0.04887870169796629
test40 0.042754052156093465
train45 0.04664850773952768 greater than 0.04701907486573644
test45 0.04129376482092909
train50 0.044874626611252674 greater than 0.0452210730113841
test50 0.03981851429515129
train55 0.04315862609190993 greater than 0.0434998456210333
test55 0.038356279096544835
train60 0.041494998446098406 greater than 0.041821943515857614
test60 0.03690880447130966
train65 0.039862212375355345 greater than 0.04019406139747699
test65 0.0353612988822189
train70 0.03812300821536269 greater than 0.038477388977322065
test70 0.033829387910656504
train75 0.03616734621202678 greater than 0.03660881797409987
test75 0.032465624916512933
train80 0.0339538032831107 greater than 0.03430698954284348
test80 0.031441978203859446
train85 0.03264088917394848 greater than 0.03289264452281622
test85 0.030904557451233827
train90 0.03149908401768047 greater than 0.03171179708258491
test90 0.030662693285655785
train95 0.030538784646987793 greater than 0.030718554909314775
test95 0.030575376575064945
train100 0.02971533244162757 greater than 0.029870151477286614
test100 0.030586609601896776
train105 0.028990061882570615 greater than 0.02913480679030199
test105 0.030700553668767123
train110 0.028234744131626962 greater than 0.028378698498687634
test110 0.03085006735444652
train115 0.027576517316380136 greater than 0.027704727760976898
test115 0.03106698336758538
train120 0.026924708627998926 greater than 0.027051702040133598
test120 0.031433979374199804
train125 0.026458591882014666 greater than 0.02660105511493897
test125 0.031362725169368666
train130 0.025516373579083245 greater than 0.02571565785833162
test130 0.0313659668175637
train135 0.024619169361490355 greater than 0.024785421379285417
test135 0.03145366082316593
train140 0.023884710673691106 greater than 0.024015950341402536
test140 0.03166747552765588
train145 0.02333452466602815 greater than 0.023431777980116503
test145 0.031989655774328
train150 0.022948459188794183 greater than 0.023014334536544757
test150 0.03272584704802864
train155 0.022466906951430627 greater than 0.022598993888738345
test155 0.03366231823654534
train160 0.02170079861138728 greater than 0.021851449532039227
test160 0.03347647753410625
train165 0.021033509445235018 greater than 0.021158233998559834
test165 0.0324233457939308
train170 0.020426830199996325 greater than 0.02054827807554158
test170 0.03188949421568382
train175 0.0198286083147121 greater than 0.01994261128997737
test175 0.03161055874966349
train180 0.019326783800965266 greater than 0.019418804953837052
test180 0.031574929308144
train185 0.018900082280411176 greater than 0.018983222951825824
test185 0.03181711359248025
train190 0.018459507607756647 greater than 0.018549434403845738
test190 0.03232119865448588
train195 0.018249221523454193 greater than 0.018282895522203184
test195 0.032896853243741725
train200 0.01799976153829448 greater than 0.018049798211439143
test200 0.03341559237302027
train205 0.01738319443927854 greater than 0.01748696464890237
test205 0.033871698877864294
train210 0.016750883308446717 greater than 0.016931402567455782
test210 0.03356987660424599
train215 0.015962045053179972 greater than 0.016104685731266395
test215 0.03363730202544768
train220 0.015606146804830178 greater than 0.015664691701479393
test220 0.03405448421499695
train225 0.015446525599550332 greater than 0.015433025861657202
test225 0.03447621683609064
4 Accuracy:0.9141914191419142 Lower Bound:0.8984229805385732 Upper Bound:0.9299598577452551
Loss calculated for layer count:1 node count: 15 stepSize:0.15 Momentum: True
train0 0.12675159024640797 greater than 1
test0 0.12221110966078455
train5 0.07821925094197146 greater than 0.08376502845460133
test5 0.06495820397417967
train10 0.0642491645749865 greater than 0.06586656412959475
test10 0.05332449774390496
train15 0.05902503053946082 greater than 0.05983094633650782
test15 0.04966026720465221
train20 0.0558965351586596 greater than 0.056434972545505356
test20 0.04774908361080652
train25 0.053587374430843986 greater than 0.05401113974327808
test25 0.046236722353257466
train30 0.051589804603113375 greater than 0.051982667140089214
test30 0.044823804140135495
train35 0.04959042197429157 greater than 0.049997203461108454
test35 0.043383498933121274
train40 0.04752837140934706 greater than 0.04793589954431075
test40 0.0418754652392102
train45 0.045629070386310235 greater than 0.04599355826723779
test45 0.0404224210913472
train50 0.04383668503579376 greater than 0.0441918987962629
test50 0.03898655631510433
train55 0.04214296442996616 greater than 0.0424710487824902
test55 0.03759030872933813
train60 0.040522660565899246 greater than 0.04084633580162728
test60 0.0361453256733139
train65 0.038902348682613164 greater than 0.03923065631456718
test65 0.034585499966232544
train70 0.03724593868209853 greater than 0.03756586862480683
test70 0.033209286957683444
train75 0.03550581740500326 greater than 0.03591094034132494
test75 0.032197293388177624
train80 0.03339876752681781 greater than 0.033804406491933094
test80 0.03144595152907051
train85 0.03218520020980042 greater than 0.03238755445072712
test85 0.03102977941890804
train90 0.031108243152475706 greater than 0.03132313344766625
test90 0.03075764019714294
train95 0.03009468687550443 greater than 0.030287937388449468
test95 0.030641727524567102
train100 0.029231603025478472 greater than 0.029389242287650347
test100 0.03070602791679055
train105 0.028520012833391846 greater than 0.02865650698309769
test105 0.030911646501825672
train110 0.027852129479217995 greater than 0.027984090135906366
test110 0.03121967662024125
train115 0.027166894572366727 greater than 0.027301949494864877
test115 0.031622322869625234
train120 0.026491500722194448 greater than 0.026676845437634676
test120 0.03209378205708835
train125 0.025911229324375517 greater than 0.02605246474732276
test125 0.03247515438626218
train130 0.025009824533886494 greater than 0.0251424909299598
test130 0.03272065435342798
train135 0.024804852233487353 greater than 0.024825833164569162
test135 0.03297909795716226
train140 0.02373305318763023 greater than 0.02402979307311871
test140 0.03265624928742043
train145 0.023118384414100503 greater than 0.02321732410156054
test145 0.03262462865468473
train150 0.0227030125330234 greater than 0.0227139303312065
test150 0.033026554971411465
train151 0.022706161262175048 greater than 0.0227030125330234
test151 0.03325622722593881
4 Accuracy:0.9141914191419142 Lower Bound:0.8984229805385732 Upper Bound:0.9299598577452551
