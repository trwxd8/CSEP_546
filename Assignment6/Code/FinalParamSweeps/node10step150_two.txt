Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: False
train0 0.12056891716967082 greater than 1
test0 0.10445303904709985
train5 0.061814491071784584 greater than 0.06494852461632136
test5 0.06857510203700393
train10 0.053788356446908596 greater than 0.054873300225837904
test10 0.055266250190021834
train15 0.049916148033605806 greater than 0.05054682021220856
test15 0.04785434615620631
train20 0.04739973496458756 greater than 0.04784059892611551
test20 0.043784133544621524
train25 0.04549472219023749 greater than 0.045844065874092496
test25 0.041580011793468145
train30 0.04393577915250266 greater than 0.04421250159618023
test30 0.03993529956611108
train35 0.04268085009249494 greater than 0.04292198772167494
test35 0.03759306679196117
train40 0.04139405507512485 greater than 0.04167817294618064
test40 0.03542219387164298
train45 0.03958420092460461 greater than 0.03997842586483827
test45 0.03411690229379913
train50 0.03773609107712675 greater than 0.03807284567291015
test50 0.03334646890185175
train55 0.03605267462444045 greater than 0.036360561469571846
test55 0.03279084389779234
train60 0.034769250146244865 greater than 0.03501694909377799
test60 0.03264583414985258
train65 0.03361506190100105 greater than 0.03383092218938167
test65 0.03265302874802329
train70 0.032588995442728245 greater than 0.03279094583031664
test70 0.03273414167749421
train75 0.03167511580092488 greater than 0.03184496895003266
test75 0.03301970312255147
train80 0.030869489061434014 greater than 0.03100699754218212
test80 0.0336921752138446
train85 0.030089956556797085 greater than 0.030259493247280833
test85 0.034592304054332515
train90 0.029344215416818392 greater than 0.029476583680097264
test90 0.03499955653127656
train95 0.028709389568350514 greater than 0.028839682873074077
test95 0.035115350218415976
train100 0.028016209751041438 greater than 0.02815617388038723
test100 0.03532763625693034
train105 0.027439073669752616 greater than 0.027528832096466786
test105 0.03547907482296606
train110 0.026956741514173398 greater than 0.027064008796081306
test110 0.03565047893410775
train115 0.026408058223829592 greater than 0.02651661576342587
test115 0.035749781993994
train120 0.025880622537336157 greater than 0.025993790932563175
test120 0.035657247554646145
train125 0.025524539265867636 greater than 0.025588832784878043
test125 0.03567458182874813
train130 0.02497896466824182 greater than 0.025103050942666613
test130 0.03514981719574116
train135 0.024247651688102716 greater than 0.024367752145370996
test135 0.03597763577720824
train140 0.023785083345201966 greater than 0.02386944158909861
test140 0.037635679720300796
train145 0.023316122315665898 greater than 0.023416199102022042
test145 0.0388950787604066
train150 0.0228218465221721 greater than 0.022911259996547723
test150 0.03969243214369737
train155 0.02243563608848506 greater than 0.022511944738094685
test155 0.03971780532117324
train160 0.021985687970888693 greater than 0.02208381343466502
test160 0.03958690968197687
train165 0.021508904673171042 greater than 0.021604027342771982
test165 0.039349698434582824
train170 0.021024150692618836 greater than 0.021122797255648848
test170 0.03906728633734535
train175 0.020588470320036568 greater than 0.020661671254685893
test175 0.03904295431159587
train180 0.02027830350456684 greater than 0.02033571475490958
test180 0.03895132426414727
train185 0.01999323427203556 greater than 0.020052072336693306
test185 0.038925558852852656
train189 0.019887626364156228 greater than 0.01987971521068137
test189 0.03948514844006347
0 Accuracy:0.9051155115511551 Lower Bound:0.8886166349039726 Upper Bound:0.9216143881983376
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: True
train0 0.12056891716967082 greater than 1
test0 0.10445303904709985
train5 0.07113676494635597 greater than 0.07467360426390984
test5 0.08332215311780411
train10 0.06532059528185191 greater than 0.06595526924460265
test10 0.06379091933803883
train15 0.061475642533281735 greater than 0.06296566396195294
test15 0.05383902223920062
train20 0.05551876356997118 greater than 0.05620221087556811
test20 0.055332435783934965
train25 0.052885771152220186 greater than 0.052851964075172266
test25 0.055081015764704665
0 Accuracy:0.8539603960396039 Lower Bound:0.8340784473027425 Upper Bound:0.8738423447764654
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: False
train0 0.12368914776672844 greater than 1
test0 0.11569718577015929
train5 0.06569613525823112 greater than 0.06983396736881795
test5 0.07211347351200419
train10 0.056383436185552055 greater than 0.05755290596026348
test10 0.062025175470576614
train15 0.052347658166101804 greater than 0.053001320062256274
test15 0.0541740582511081
train20 0.04969806340816189 greater than 0.05015733843836356
test20 0.04903814272900857
train25 0.04777787456017643 greater than 0.04812086554863454
test25 0.045530326658671634
train30 0.04628790176966316 greater than 0.0465596400785351
test30 0.04304867376640079
train35 0.04508022251488041 greater than 0.04530463334798784
test35 0.04127677048303464
train40 0.04402930285230947 greater than 0.04423517273062801
test40 0.040017981665333775
train45 0.04293071593394223 greater than 0.043165072701746666
test45 0.03903840869415843
train50 0.04162188909197357 greater than 0.04190014408959455
test50 0.0377494445908347
train55 0.040125840864910486 greater than 0.04043691043771385
test55 0.03623852795488177
train60 0.03851303264734439 greater than 0.0388422358124804
test60 0.0347878793684552
train65 0.036881023334524296 greater than 0.037195935827685836
test65 0.03357621849113003
train70 0.035511698059588905 greater than 0.03576382515345952
test70 0.032499062596900066
train75 0.03435065263129412 greater than 0.034573439184748096
test75 0.03153239594674639
train80 0.03325548379450217 greater than 0.033474370643165385
test80 0.030940683936783935
train85 0.0321591047789411 greater than 0.03237609962232377
test85 0.030746349977921417
train90 0.031087347320106266 greater than 0.03130282084309768
test90 0.030645898659651147
train95 0.0301361180701116 greater than 0.0303068950495924
test95 0.030382340456586977
train100 0.02940207030047735 greater than 0.02952836824960906
test100 0.030093453866783228
train105 0.028788736333585533 greater than 0.028916713304939334
test105 0.030044994151592515
train110 0.027924096277113827 greater than 0.02810101571106206
test110 0.029811560065878612
train115 0.02704428073701865 greater than 0.027228963201413655
test115 0.029561187304173366
train120 0.02618234435740768 greater than 0.026349333584834704
test120 0.029550895715363377
train125 0.025404985196946183 greater than 0.025591888408955325
test125 0.029559583703978036
train130 0.024791346650276838 greater than 0.024901593238800902
test130 0.02937358748511284
train135 0.02426668082250241 greater than 0.024367092499514388
test135 0.029416797884677215
train140 0.023809730738955873 greater than 0.023895110974263896
test140 0.029512519658590076
train145 0.02341292819595783 greater than 0.023489819145596405
test145 0.029498793925429912
train150 0.02303086097368261 greater than 0.023107162579510333
test150 0.029373352580609868
train155 0.022658855813132744 greater than 0.02273143045649819
test155 0.029181843588229193
train160 0.022310616168468075 greater than 0.022378664787786405
test160 0.029010111705868768
train165 0.021976194955634878 greater than 0.022042575248800017
test165 0.029007411335831717
train170 0.02164107060809687 greater than 0.021708901542943547
test170 0.02921426970054264
train175 0.021294746938269226 greater than 0.021365465462731548
test175 0.029511315537246223
train180 0.02094871969410543 greater than 0.021013617821528916
test180 0.029844652214963803
train185 0.020638128748472626 greater than 0.020704331754153267
test185 0.030349054079082227
train190 0.020233272172971842 greater than 0.020320182745164825
test190 0.03080473747560025
train195 0.019820178243878005 greater than 0.019901067298239053
test195 0.0313308633898223
train200 0.01945146564411157 greater than 0.01952273083543838
test200 0.03193190762648677
train205 0.01898201621903926 greater than 0.019088408034289985
test205 0.03265422863420919
train210 0.018508808990771834 greater than 0.01859681379148205
test210 0.032501783159628365
train215 0.01830577068565043 greater than 0.01832461475500458
test215 0.03321178870444992
train220 0.017997008216811074 greater than 0.018076950343660362
test220 0.03245247526965811
train225 0.017815159728764754 greater than 0.017821911646966827
test225 0.0326504801125714
train230 0.017580741232144776 greater than 0.017679992849975686
test230 0.032553460935553676
train235 0.0172722276413594 greater than 0.01731339522115777
test235 0.0325623328626031
train240 0.017115066067608275 greater than 0.017139646537483646
test240 0.032874830219947
train241 0.01739170720352045 greater than 0.017115066067608275
test241 0.035388349143324
1 Accuracy:0.9108910891089109 Lower Bound:0.8948513036132367 Upper Bound:0.9269308746045851
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: True
train0 0.12368914776672844 greater than 1
test0 0.11569718577015929
train5 0.0654709617200121 greater than 0.06981429680816373
test5 0.07478749316459556
train10 0.05604312055292195 greater than 0.05722893513803485
test10 0.06241841371089128
train15 0.05190798953572919 greater than 0.05259353014280533
test15 0.053706596073965586
train20 0.0490156029968138 greater than 0.049531002816381724
test20 0.047870863029238424
train25 0.046839508991062144 greater than 0.04723307129076592
test25 0.04410078157570289
train30 0.04495513295218979 greater than 0.04532993373820956
test30 0.04104290853976953
train35 0.043064188192175466 greater than 0.04345468799654817
test35 0.038058962306266125
train40 0.040854388019267746 greater than 0.0413438908384555
test40 0.03568996160429706
train45 0.038534494086055415 greater than 0.038944648029757487
test45 0.03426952379428394
train50 0.0368767820978412 greater than 0.03717888364924776
test50 0.033339948426988786
train55 0.035210299346944994 greater than 0.03555622658228817
test55 0.03274767622815993
train60 0.03358005708453755 greater than 0.03389174646161063
test60 0.03232277733191769
train65 0.03216450887394323 greater than 0.03243863394232788
test65 0.03221704669843391
train70 0.03088805960753505 greater than 0.031137563250916954
test70 0.03266412489778886
train75 0.0296404637218781 greater than 0.029881023425291912
test75 0.032866498721909235
train80 0.028544190389854607 greater than 0.028748226608338895
test80 0.0328493904040292
train85 0.02755185972882917 greater than 0.027744648208428473
test85 0.032710440961993124
train90 0.02648857988585374 greater than 0.02670064830655486
test90 0.032837113643725206
train95 0.02593029370552983 greater than 0.026011001607127725
test95 0.03373515743487177
train100 0.025183392940418145 greater than 0.02533605673184699
test100 0.03428824472772221
train105 0.02447809342948757 greater than 0.02461741477867794
test105 0.03478873262904814
train110 0.023763924891807665 greater than 0.023893487557277344
test110 0.036500148968125465
train115 0.02316027135075762 greater than 0.0232703642854084
test115 0.03742309721533982
train120 0.022653851954954004 greater than 0.022732495497773002
test120 0.03820465718639895
train125 0.022586116492176404 greater than 0.022566329544780002
test125 0.04001939157420313
1 Accuracy:0.9001650165016502 Lower Bound:0.8832875520445865 Upper Bound:0.9170424809587138
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: False
train0 0.12441442974363452 greater than 1
test0 0.11769202521394789
train5 0.06815171220867688 greater than 0.07290515350178665
test5 0.07901252015902242
train10 0.057734774237831514 greater than 0.058987601718364514
test10 0.06983977910458897
train15 0.05353202941436642 greater than 0.05418467893243616
test15 0.063482536816223
train20 0.05092596183309938 greater than 0.05138442175633601
test20 0.0581662730139785
train25 0.04893148502195014 greater than 0.04929670949615908
test25 0.05411694277639588
train30 0.047308944620968484 greater than 0.047608562378801386
test30 0.05111517610575794
train35 0.04596877613071159 greater than 0.04621773767079352
test35 0.048852698059943925
train40 0.04483064773699695 greater than 0.0450464800144176
test40 0.04699453155065812
train45 0.043825770882192236 greater than 0.04401619215655239
test45 0.04542923371744711
train50 0.042969423505711556 greater than 0.04312742027393452
test50 0.04420504909814106
train55 0.04226911393424272 greater than 0.0423986378214846
test55 0.043266795109209305
train60 0.0416720002731639 greater than 0.04178674503952211
test60 0.04252108846460884
train65 0.04110772320785019 greater than 0.041220446118174166
test65 0.04187748567684517
train70 0.04053989507608466 greater than 0.040654074545733145
test70 0.041280138523366296
train75 0.03995982501584623 greater than 0.04007766081334094
test75 0.04078253278810013
train80 0.039341359813279984 greater than 0.03947041079327788
test80 0.04046615782627441
train85 0.03860101266056871 greater than 0.03877160335457704
test85 0.04036886356261576
train90 0.03755621526394868 greater than 0.037769536552754224
test90 0.040414684815197964
train95 0.03658597396604206 greater than 0.03676663447560099
test95 0.040083594746532135
train100 0.03572931846490018 greater than 0.03590263044688719
test100 0.03846462210088815
train105 0.034716610722653994 greater than 0.03493664609542508
test105 0.03650290839700374
train110 0.03355229710523064 greater than 0.03379206720917647
test110 0.03495288524271378
train115 0.0323672727538347 greater than 0.03259698336805829
test115 0.033533890661133894
train120 0.031238673251898507 greater than 0.031460545160253595
test120 0.03294243004991485
train125 0.030266699780628795 greater than 0.030440329740181613
test125 0.03215632107447814
train130 0.029459103447666023 greater than 0.029617579674950652
test130 0.03126466917505845
train135 0.028676115072827735 greater than 0.028827440156671293
test135 0.030582918931953817
train140 0.027985017955817034 greater than 0.028114788045090884
test140 0.03025513598807946
train145 0.027188993898403728 greater than 0.02736714505802157
test145 0.03020522696520522
train150 0.02629625526976518 greater than 0.026472256534355358
test150 0.030533351810938903
train155 0.025637837016536633 greater than 0.025737703836815777
test155 0.031242740172540178
train160 0.025227227094538174 greater than 0.025301838288493902
test160 0.03283350145744996
train165 0.024869442901702676 greater than 0.024940889845654523
test165 0.03470140915376026
train170 0.024448036176951054 greater than 0.024548776391699223
test170 0.036378114880181986
train175 0.023920710590934563 greater than 0.024003543936839674
test175 0.037776283482179714
train180 0.02331878943564634 greater than 0.023432833397779337
test180 0.03882573763861341
train185 0.02285145410241232 greater than 0.022937211726494888
test185 0.038911023776871605
train190 0.02242840629680432 greater than 0.02251419379370778
test190 0.03870054154040603
train195 0.02198933961748259 greater than 0.022077257425854497
test195 0.03824505250548814
train200 0.02156455541926005 greater than 0.021647251415926314
test200 0.037742200649191
train205 0.021169247775844655 greater than 0.02124523620223749
test205 0.037610523914370385
train210 0.020834828770081738 greater than 0.02089337990884546
test210 0.03793438724375121
train215 0.020682970134290147 greater than 0.020694723959475775
test215 0.03898549785999918
train220 0.020406671737284422 greater than 0.02044861749843203
test220 0.0398412898664119
train225 0.020233233899226773 greater than 0.02028016870171733
test225 0.040307979343477074
train227 0.020233316531697333 greater than 0.020218248557512277
test227 0.04052788192721226
2 Accuracy:0.8976897689768977 Lower Bound:0.880627867419156 Upper Bound:0.9147516705346393
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: True
train0 0.12441442974363452 greater than 1
test0 0.11769202521394789
train5 0.0828515022657131 greater than 0.08623264812989812
test5 0.08026088898614665
train10 0.07348195328111647 greater than 0.07430008117257428
test10 0.1098959779965484
train15 0.06336579345490963 greater than 0.06557965413742065
test15 0.07316779675586378
train18 0.06313042563619531 greater than 0.06304411489307572
test18 0.07525011599852206
2 Accuracy:0.7912541254125413 Lower Bound:0.7683732990275121 Upper Bound:0.8141349517975704
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: False
train0 0.12551382387441537 greater than 1
test0 0.1196161742553889
train5 0.07009769455387814 greater than 0.07497359846400786
test5 0.08511841299095463
train10 0.05931372765886201 greater than 0.06053425917574993
test10 0.07290655136049419
train15 0.05516739936898313 greater than 0.05586319090492283
test15 0.06517259651998014
train20 0.05246132352351386 greater than 0.052927467533674305
test20 0.05886943488892317
train25 0.050463097650568285 greater than 0.050826235081508214
test25 0.05448012456123687
train30 0.048842389524077476 greater than 0.04914447342446195
test30 0.05135940037006404
train35 0.04745352840875607 greater than 0.04771738640718432
test35 0.049110859324224994
train40 0.04621106844192048 greater than 0.04645087674135089
test40 0.04746389260404397
train45 0.045058423993897666 greater than 0.04528388685984584
test45 0.046206636936465796
train50 0.043951586381892456 greater than 0.04417146471827995
test50 0.04522980899590337
train55 0.04284081594969014 greater than 0.04306627435301379
test55 0.04440003319727065
train60 0.041655864405414765 greater than 0.041902989274079996
test60 0.04343206703162424
train65 0.040330124539016604 greater than 0.04060608564541604
test65 0.04210030374067024
train70 0.03887978309411681 greater than 0.039179143403264324
test70 0.04060249732959271
train75 0.0373366935920608 greater than 0.03764877982363276
test75 0.03878104546090374
train80 0.03580440490234049 greater than 0.03610353075878282
test80 0.03661066744677936
train85 0.03440256438909668 greater than 0.03467101604524999
test85 0.03482626254152728
train90 0.033087307903955726 greater than 0.03335009889029628
test90 0.03369171036589126
train95 0.031774544287997294 greater than 0.032034181999816565
test95 0.03296964180488126
train100 0.03054664191091766 greater than 0.03078142371524762
test100 0.03243074033751679
train105 0.029476103257710325 greater than 0.029678355216627293
test105 0.03188070609295649
train110 0.02849821854597457 greater than 0.028688312245889153
test110 0.03161549634055625
train115 0.027616261085752884 greater than 0.027783161551625038
test115 0.032256807674127363
train120 0.02682588303302963 greater than 0.026982042009787696
test120 0.03278994761497865
train125 0.026017913622574284 greater than 0.026182797669474702
test125 0.03307251885818293
train130 0.02524520104570732 greater than 0.025388564898416
test130 0.033306518946016585
train135 0.024599562460265716 greater than 0.024721284148427957
test135 0.03350813115696291
train140 0.024026982642268865 greater than 0.02413748751448972
test140 0.033736086696250896
train145 0.0234972540176674 greater than 0.02360038704169643
test145 0.03395901313700712
train150 0.02300485369811698 greater than 0.023100038635840425
test150 0.03410365711928345
train155 0.02254572576638376 greater than 0.02263630604241118
test155 0.034136127749576486
train160 0.02209146297180504 greater than 0.02218299718163228
test160 0.034120478589709353
train165 0.02162575471762878 greater than 0.021719726645256836
test165 0.034105973221387365
train170 0.02115249392527303 greater than 0.021247896945192
test170 0.03409257576883993
train175 0.020661081668687353 greater than 0.02076168578300105
test175 0.03407474098974364
train180 0.020144097222125866 greater than 0.02024859692694202
test180 0.034087272203442806
train185 0.01963033425275898 greater than 0.019730708556688657
test185 0.03414835538674032
train190 0.019160710684682492 greater than 0.019249678531133303
test190 0.034238354136047575
train195 0.018757418354188465 greater than 0.01883246725212707
test195 0.03432081199306319
train200 0.0184241678416956 greater than 0.018485196453877053
test200 0.034284615403070734
train205 0.018158007404621833 greater than 0.018206709538876247
test205 0.03413249846486721
train210 0.0179382812045129 greater than 0.017979354259260647
test210 0.034022585094624774
train215 0.017752629301849003 greater than 0.017787343438083456
test215 0.034096488812172525
train220 0.01758658474315993 greater than 0.01761913431430015
test220 0.03430314900422615
train225 0.017449516624695988 greater than 0.017471107485245236
test225 0.034488250385684184
train230 0.01739604495492188 greater than 0.017406322615205246
test230 0.034682730063451905
train235 0.01729563937021259 greater than 0.017319884128047847
test235 0.034276606672478664
train239 0.017272685185303456 greater than 0.01723551655276534
test239 0.03386760667087171
3 Accuracy:0.91996699669967 Lower Bound:0.9046904411913306 Upper Bound:0.9352435522080094
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: True
train0 0.12551382387441537 greater than 1
test0 0.1196161742553889
train5 0.08192543615338442 greater than 0.08398333457806671
test5 0.09179214148258116
train10 0.07321205690459054 greater than 0.0736313450790637
test10 0.12128752621490083
train15 0.06693610652300892 greater than 0.0691125064824552
test15 0.08537211877240877
train20 0.0632136890092392 greater than 0.06305535624590976
test20 0.07245687740705482
3 Accuracy:0.7945544554455446 Lower Bound:0.771807935678725 Upper Bound:0.8173009752123642
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: False
train0 0.1254695390573664 greater than 1
test0 0.1211873265670387
train5 0.07795412533797512 greater than 0.08323387830297252
test5 0.06624271118530138
train10 0.06409631936545278 greater than 0.06573708602155312
test10 0.053984201441507125
train15 0.05897615900293998 greater than 0.05975188727837814
test15 0.04991407181177828
train20 0.0559397843145016 greater than 0.05646345435891677
test20 0.047962896060362636
train25 0.053754340918850056 greater than 0.054143150702323437
test25 0.04657872509567827
train30 0.05206000996045563 greater than 0.05237158860319223
test30 0.04543215320545608
train35 0.05063984674811283 greater than 0.05090876476101064
test35 0.04442782894590197
train40 0.0493756808230527 greater than 0.04961944829806378
test40 0.04354964019871546
train45 0.048206920390876984 greater than 0.04843472613316908
test45 0.04279820346836708
train50 0.047108698381821856 greater than 0.047323077394833504
test50 0.04215726513634968
train55 0.046066746932232915 greater than 0.04627207764086981
test55 0.04155742170604235
train60 0.045051881091762815 greater than 0.04525397943967403
test60 0.040857550754689755
train65 0.04403426145994356 greater than 0.04423994909283854
test65 0.039944795153349845
train70 0.04295687368719149 greater than 0.04318091977371057
test70 0.03883212791623498
train75 0.041759932112971014 greater than 0.04201094789707062
test75 0.03762530732788161
train80 0.04044458624523716 greater than 0.040713349216251325
test80 0.03638406864390703
train85 0.03901029140087238 greater than 0.03931267836407743
test85 0.03509654952270127
train90 0.03742037933543928 greater than 0.03774149225022118
test90 0.03385583162605669
train95 0.03590762697223629 greater than 0.03619164631371605
test95 0.032790739911047716
train100 0.03457790372034808 greater than 0.034842471236284075
test100 0.03194195733587547
train105 0.0332010186285516 greater than 0.03348773894382511
test105 0.03131776794573071
train110 0.031745893481216626 greater than 0.03202394311559307
test110 0.03089642444911951
train115 0.03056039645499861 greater than 0.03077226553850469
test115 0.030655760813900684
train120 0.029637352446510216 greater than 0.02980727384643598
test120 0.030542562181968852
train125 0.02883754441647862 greater than 0.028994788024433425
test125 0.030532047200752484
train130 0.027997764257122474 greater than 0.02817582393929991
test130 0.030652950243175436
train135 0.02714183533631685 greater than 0.0272991943135417
test135 0.030769923588779845
train140 0.026486961783871724 greater than 0.026600609316903678
test140 0.031003666738725604
train145 0.02601689773586481 greater than 0.02613265210823043
test145 0.03089019040272471
train150 0.02552445827771525 greater than 0.02562305337715193
test150 0.030961587649680367
train155 0.024994281249056487 greater than 0.025113226413117872
test155 0.031154659486419153
train160 0.02432981964604039 greater than 0.02448827888633224
test160 0.03164715918344622
train165 0.023404402177201657 greater than 0.02363338000574563
test165 0.031232691496407634
train170 0.022467713578849142 greater than 0.02263086880172426
test170 0.03177837586068188
train175 0.022010527534008232 greater than 0.02210437612103308
test175 0.03227896303896589
train180 0.0214382990616686 greater than 0.021566106967464745
test180 0.0325176934536945
train185 0.02084807752681814 greater than 0.02095482579820464
test185 0.03308469220911418
train190 0.02022081147067166 greater than 0.020332117329511235
test190 0.033587226780998625
train195 0.019684923236423846 greater than 0.019793881732739153
test195 0.03405924735544127
train200 0.019194517964841038 greater than 0.019246770897434037
test200 0.035577180154673545
train203 0.01939704647250501 greater than 0.018994048948601128
test203 0.03666615338339453
4 Accuracy:0.905940594059406 Lower Bound:0.8895061227583358 Upper Bound:0.9223750653604761
Loss calculated for layer count:1 node count: 10 stepSize:0.15 Momentum: True
train0 0.1254695390573664 greater than 1
test0 0.1211873265670387
train5 0.07726680952053545 greater than 0.08287436816435756
test5 0.06181356853362026
train10 0.06361557646068758 greater than 0.0651259760284079
test10 0.052126634696438155
train15 0.058613703650373244 greater than 0.05940360274310873
test15 0.04897079579981164
train20 0.055595350590844764 greater than 0.056106689070280144
test20 0.047141232405758945
train25 0.05344271186285301 greater than 0.05383408259399532
test25 0.045680038463687636
train30 0.05163276224195704 greater than 0.05198122130372862
test30 0.04440257401828258
train35 0.04996388669033915 greater than 0.050286407412560394
test35 0.04314726817598108
train40 0.04845380576005794 greater than 0.04874422195266291
test40 0.041799634799199875
train45 0.046998415646384496 greater than 0.04729769464778331
test45 0.040290546225736155
train50 0.04534715066105625 greater than 0.04569944256096886
test50 0.03859879064480678
train55 0.04353859393479916 greater than 0.04389478156595108
test55 0.037062208168175875
train60 0.0417139799839354 greater than 0.042086579783407345
test60 0.035576063154609296
train65 0.039858732997518576 greater than 0.040257250805615916
test65 0.03401234497674195
train70 0.03810731225532881 greater than 0.0384242844698055
test70 0.032716536197351986
train75 0.036179838491218655 greater than 0.036620411342355594
test75 0.031984248724357275
train80 0.034174309818790764 greater than 0.03452683208890594
test80 0.031389538299615646
train85 0.0325195023129611 greater than 0.03279633483141985
test85 0.03097557982810004
train90 0.031249640333701544 greater than 0.03154224395584711
test90 0.030805118770733164
train95 0.029820882318293468 greater than 0.030106639820785833
test95 0.030810140028425397
train100 0.028455990625096447 greater than 0.028785798034040997
test100 0.031013442525767338
train105 0.027197117189454705 greater than 0.02735662891786656
test105 0.03142484193787558
train110 0.02634344771295473 greater than 0.026467410587596593
test110 0.03209456176493943
train115 0.025654117502042446 greater than 0.025809019112321807
test115 0.03275905840836678
train120 0.02531381731897724 greater than 0.025305128644943133
test120 0.032829559763567745
4 Accuracy:0.9133663366336634 Lower Bound:0.897529421057252 Upper Bound:0.9292032522100748
