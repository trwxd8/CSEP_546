Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: False
train0 0.12480997962785685 greater than 1
test0 0.11827897472012952
train5 0.06263984694604098 greater than 0.06582567381090891
test5 0.07131082174195742
train10 0.05495044313630436 greater than 0.05592410563953691
test10 0.0586330173017138
train15 0.05154470486398297 greater than 0.0520600812694229
test15 0.050709480569763016
train20 0.04950643397600673 greater than 0.04986689110224207
test20 0.04684927081527167
train25 0.047972207276295156 greater than 0.048243777345702406
test25 0.045179697664465064
train30 0.04680749285984854 greater than 0.047022150500120014
test30 0.043965932339629726
train35 0.04577657540225207 greater than 0.04598373949513232
test35 0.042844914812993
train40 0.04447231993163179 greater than 0.044787827577745666
test40 0.041242497830483095
train45 0.042754348591963275 greater than 0.04313886642628327
test45 0.039835960672745335
train50 0.040204926888559554 greater than 0.04071632654515557
test50 0.034634360570490616
train55 0.03784711982077959 greater than 0.0382303554653374
test55 0.03228279995389245
train60 0.036318244546947584 greater than 0.036594082743798106
test60 0.032184059387659524
train65 0.035059425361893694 greater than 0.035298827770489614
test65 0.031154175563661318
train70 0.034191901516177875 greater than 0.03432571371674425
test70 0.03369564080274097
train75 0.03361702732749586 greater than 0.033732796895760135
test75 0.035472829753893306
train80 0.03299888698787777 greater than 0.033118127095268644
test80 0.034683219834571934
train85 0.032461520634338464 greater than 0.03257753848520673
test85 0.03355272414763595
train90 0.03185326348298533 greater than 0.03204067317412337
test90 0.033777047501904606
train95 0.0313619424774499 greater than 0.031455337121592736
test95 0.03266349441841468
train100 0.030963309748329556 greater than 0.031037314359277417
test100 0.03174841060892488
train105 0.03061707624531151 greater than 0.03070773158010528
test105 0.031430241040998695
train110 0.0301119580826795 greater than 0.03021339917645799
test110 0.031548928104219144
train115 0.029769036210450874 greater than 0.029817995037865135
test115 0.03176776440337585
train120 0.02944987840894523 greater than 0.029523223147474735
test120 0.03197377381904099
train125 0.02894447505720564 greater than 0.029034048492295236
test125 0.03169144703166298
train130 0.028712180526323787 greater than 0.028749196336179
test130 0.03145867820747177
train135 0.028475291088148377 greater than 0.028536968784680556
test135 0.031303030644465046
train140 0.02808645472396524 greater than 0.0281439107867419
test140 0.031202268407664654
train145 0.027790696891649366 greater than 0.027831266400723823
test145 0.031154606284332902
train150 0.027600178866803462 greater than 0.02763822076920345
test150 0.030977625403495742
train155 0.02744408092385751 greater than 0.02745513674591012
test155 0.03094264821874343
train160 0.027393576840849455 greater than 0.027408959912619046
test160 0.031086447918802832
train165 0.0272811097394736 greater than 0.027295516791824164
test165 0.03126325706807641
train168 0.027247420482006974 greater than 0.02721737662018569
test168 0.03134236787100004
0 Accuracy:0.9249174917491749 Lower Bound:0.9100811904648197 Upper Bound:0.9397537930335301
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: True
train0 0.12480997962785685 greater than 1
test0 0.11827897472012952
train5 0.06543010065763163 greater than 0.07062853487032003
test5 0.08282377940119065
train10 0.05575846461899924 greater than 0.05701291701365677
test10 0.05801350939385283
train15 0.0502102373891379 greater than 0.051316713981344866
test15 0.04326276685904684
train20 0.048356962719507526 greater than 0.04847574450981214
test20 0.03925902252252316
train25 0.04586319837109832 greater than 0.046517482695763945
test25 0.03719338614882557
train30 0.04418164274709318 greater than 0.04440523870291523
test30 0.03457408518463387
train35 0.04241578156875274 greater than 0.04277855336194739
test35 0.03425687193527589
train40 0.04145996149060356 greater than 0.04136975010935309
test40 0.03635701391143373
0 Accuracy:0.9084158415841584 Lower Bound:0.8921769160428032 Upper Bound:0.9246547671255136
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: False
train0 0.12542079452325525 greater than 1
test0 0.12208609700969549
train5 0.0658300819302893 greater than 0.06977572747659631
test5 0.07555885802098947
train10 0.05693225816845653 greater than 0.058061399626126615
test10 0.06611480131453437
train15 0.05326140179121565 greater than 0.053833803101604086
test15 0.057603597984044665
train20 0.050913727605891884 greater than 0.05132088299198211
test20 0.051501540509363085
train25 0.04916689729852863 greater than 0.049499285887835316
test25 0.04804324287201727
train30 0.04752960075931036 greater than 0.04785971591634059
test30 0.04576435918565095
train35 0.04583905233953708 greater than 0.04617162614230888
test35 0.044114891873376694
train40 0.04418073341084387 greater than 0.04452645014098235
test40 0.04257004192940958
train45 0.04235007710443441 greater than 0.042723612991755505
test45 0.04034810235197845
train50 0.04026580699857165 greater than 0.04075590350620624
test50 0.03678920009091879
train55 0.03756305613911396 greater than 0.0379999240176871
test55 0.033916611624068255
train60 0.03577689654760806 greater than 0.036138137998308645
test60 0.03284108063314975
train65 0.03412922540263599 greater than 0.03438346808765585
test65 0.032750056651374794
train70 0.033346649280399 greater than 0.03348555203956425
test70 0.03307603108633448
train75 0.03268917508188263 greater than 0.032859142908450704
test75 0.03325837091939398
train80 0.03188491403967974 greater than 0.03199556716661222
test80 0.0337624974589429
train85 0.03143626738522396 greater than 0.03152686456650093
test85 0.03424967567736494
train90 0.030970700616505503 greater than 0.031056359949365668
test90 0.034797445117733534
train95 0.030605187128448664 greater than 0.03067896470675046
test95 0.035494271137955134
train100 0.030184633165098305 greater than 0.030275665769268427
test100 0.0362167760420826
train104 0.030071410879878427 greater than 0.029970689298542345
test104 0.036656652780301106
1 Accuracy:0.9001650165016502 Lower Bound:0.8832875520445865 Upper Bound:0.9170424809587138
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: True
train0 0.12542079452325525 greater than 1
test0 0.12208609700969549
train5 0.06663003656068166 greater than 0.07138121087833073
test5 0.08401347543542462
train10 0.05702181942919016 greater than 0.05821475700873184
test10 0.06550554827444957
train15 0.05275968684286272 greater than 0.053604456519903025
test15 0.05162486439639856
train20 0.04811762664602101 greater than 0.04891614272729548
test20 0.04341712592250108
train25 0.044295327991561426 greater than 0.04495728116814088
test25 0.03553338450836882
train30 0.04150200015381043 greater than 0.04225762973321973
test30 0.03245219141081546
train35 0.03984143646719766 greater than 0.04005497253472982
test35 0.03131611659825674
train40 0.03845513856482236 greater than 0.03890512805304067
test40 0.031210451440522726
train45 0.0374225266555389 greater than 0.037670048886205294
test45 0.032939217339885196
train47 0.03734458037345633 greater than 0.03727645293415214
test47 0.03397937785685503
1 Accuracy:0.9158415841584159 Lower Bound:0.9002114136085113 Upper Bound:0.9314717547083204
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: False
train0 0.12548117140424575 greater than 1
test0 0.12315390545643715
train5 0.07001613918098512 greater than 0.07541869490042499
test5 0.08038399347040588
train10 0.058608432460365095 greater than 0.05993227672375476
test10 0.07122052770255406
train15 0.05424781339283453 greater than 0.05489149534387197
test15 0.06571408485099936
train20 0.05189805495445005 greater than 0.05229283600211992
test20 0.060637339243518246
train25 0.05015044685135171 greater than 0.05047977796777953
test25 0.056523819064147954
train30 0.048634528214730244 greater than 0.0489160499570555
test30 0.05349554258847127
train35 0.04740639960180446 greater than 0.04762945634719328
test35 0.051445412777455506
train40 0.04641539244097147 greater than 0.046599325117977
test40 0.04997372346871771
train45 0.045583216666264186 greater than 0.04573920204600308
test45 0.04880139289149747
train50 0.04486515597686788 greater than 0.04500149336640036
test50 0.0478347278922024
train55 0.044223204132753755 greater than 0.044347341744489344
test55 0.04701306843887902
train60 0.04361694795485974 greater than 0.04373774088668356
test60 0.04616246553139511
train65 0.04297198377282587 greater than 0.043111204819262075
test65 0.04498291893444952
train70 0.04211450040532246 greater than 0.04231148592041209
test70 0.04361379239246546
train75 0.040931752780055036 greater than 0.041196442981776264
test75 0.04273071233606558
train80 0.039084857093247806 greater than 0.03953765786189599
test80 0.041305486396117204
train85 0.03690328113485733 greater than 0.037324046704004375
test85 0.0389401532398005
train90 0.03503431050685344 greater than 0.03539047108211935
test90 0.03654056482133049
train95 0.03353821597193883 greater than 0.03380602776109623
test95 0.0354001177014906
train100 0.032353359861904796 greater than 0.03256269149466424
test100 0.03288470037521988
train105 0.03143651875841786 greater than 0.0315852829227095
test105 0.030854407101072637
train110 0.030732485968267814 greater than 0.030871805773925495
test110 0.03006118280557839
train115 0.030112210875444897 greater than 0.030224485194102046
test115 0.029971514212456352
train120 0.02959741531975042 greater than 0.029692678393996304
test120 0.03035106815103876
train125 0.029158700178338975 greater than 0.029241254399608264
test125 0.03094604094890355
train130 0.02875713667375174 greater than 0.028837942535090505
test130 0.031310761141885496
train135 0.028361976667455182 greater than 0.028438507425203447
test135 0.03152842658281092
train140 0.02801246742724545 greater than 0.028077034851428994
test140 0.03171030464908976
train145 0.027730199794203398 greater than 0.02778153733825618
test145 0.03179105270984788
train150 0.02752256763046988 greater than 0.027555278156884383
test150 0.03169152092891248
train154 0.027486408693578383 greater than 0.02748294979964666
test154 0.03141768089509033
2 Accuracy:0.9240924092409241 Lower Bound:0.9091814677315778 Upper Bound:0.9390033507502703
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: True
train0 0.12548117140424575 greater than 1
test0 0.12315390545643715
train5 0.07960293666171595 greater than 0.08398565973674017
test5 0.1207110387520135
train10 0.06702152113424428 greater than 0.06919754006149377
test10 0.08575763249461903
train15 0.058825815485677314 greater than 0.06005940532124741
test15 0.05746423569802505
train20 0.05353315157562546 greater than 0.054260228150292814
test20 0.050798505556462266
train23 0.05301379142030904 greater than 0.05292345198191715
test23 0.05398231699205017
2 Accuracy:0.8597359735973598 Lower Bound:0.8401853568724996 Upper Bound:0.8792865903222199
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: False
train0 0.1252183744771738 greater than 1
test0 0.12264303922535572
train5 0.07236970961512615 greater than 0.07796834115316434
test5 0.0860561430766625
train10 0.05989405927606785 greater than 0.06122471312594329
test10 0.07302351404349224
train15 0.055686872083160976 greater than 0.05634768390204308
test15 0.06652060937055218
train20 0.053097934218396904 greater than 0.05354355228224708
test20 0.060964714861627176
train25 0.051221615759908155 greater than 0.05155767382120824
test25 0.056547685241901076
train30 0.049768560654559334 greater than 0.05003273021320707
test30 0.053123920271995785
train35 0.04858756409066306 greater than 0.04880842878462653
test35 0.05058586228896635
train40 0.04756111613709993 greater than 0.04775793684742971
test40 0.04862451555655387
train45 0.04661993254832905 greater than 0.046803505332867514
test45 0.04698932530382039
train50 0.04572142855259669 greater than 0.04589984288650151
test50 0.04551297670652737
train55 0.044806959461346454 greater than 0.0449956303551718
test55 0.04391141179947855
train60 0.04379280174933037 greater than 0.04400516803177741
test60 0.042089854480131174
train65 0.04264240864212231 greater than 0.042889332684412966
test65 0.04006318602688403
train70 0.041217140512477955 greater than 0.04152810911933704
test70 0.0383833746882033
train75 0.039537782059136495 greater than 0.039878615297248586
test75 0.037186908518695425
train80 0.03789436959966103 greater than 0.038216613725045805
test80 0.036170029829522624
train85 0.03635760092530292 greater than 0.036651771655382574
test85 0.03534478314003411
train90 0.03502732586116003 greater than 0.035272628447350914
test90 0.034891932684343444
train95 0.03397206648788529 greater than 0.03415459946857552
test95 0.03476829140071957
train100 0.033252729777563585 greater than 0.033395772648047814
test100 0.03464888738436492
train105 0.03214872119197544 greater than 0.03241437595050794
test105 0.03419388213618247
train110 0.030914690869625765 greater than 0.03111447453814361
test110 0.03357433225253758
train115 0.030170477728100352 greater than 0.030297651896894366
test115 0.033164474512058104
train120 0.02957170787614657 greater than 0.029689583429000795
test120 0.03288122160326773
train125 0.029012119524597497 greater than 0.029117626292688915
test125 0.032678984926601935
train130 0.028507655310533175 greater than 0.02860803581058061
test130 0.03252662918322284
train135 0.028007496952953945 greater than 0.028105436289458136
test135 0.03241901017087976
train140 0.027563281019396754 greater than 0.027645327573218765
test140 0.03236788702106511
train145 0.027188223893253444 greater than 0.027260474938215165
test145 0.032369404317273344
train150 0.02681375335853007 greater than 0.026892603183593766
test150 0.032407584210241945
train155 0.02638442029596136 greater than 0.026473717453667306
test155 0.03246499506164712
train160 0.02591056912298664 greater than 0.0260070225070715
test160 0.03252408229960653
train165 0.025454651577612866 greater than 0.02554428509384651
test165 0.03257195599813201
train170 0.02503428510026747 greater than 0.025110470397349902
test170 0.03266629221481644
train175 0.024588482379909536 greater than 0.024722680853188017
test175 0.03279792075316333
train180 0.02413360179667274 greater than 0.024195439789512545
test180 0.032547283552114184
train185 0.02387622800685761 greater than 0.023924352502449706
test185 0.032416411081303866
train190 0.0236235299516189 greater than 0.023676653461180905
test190 0.03236583619777202
train195 0.02335046775960037 greater than 0.023405211792882234
test195 0.03233368451881047
train200 0.023081779675394643 greater than 0.02313460694537184
test200 0.032318758990915616
train205 0.022825761250040662 greater than 0.022875948535319275
test205 0.032314360132528856
train210 0.02258302017347975 greater than 0.022630136464232675
test210 0.03231190368799621
train215 0.02236504568172431 greater than 0.02240594661327459
test215 0.0323141291572042
train220 0.022181977075759904 greater than 0.02221609874798865
test220 0.032338814706043086
train225 0.022011785043339668 greater than 0.02204722033561358
test225 0.03238416998350277
train230 0.021814731328457862 greater than 0.02185664889804616
test230 0.0324369076536536
train235 0.021605136816203525 greater than 0.021644529177352195
test235 0.032501449731375115
train240 0.021468691923887208 greater than 0.021491011669136396
test240 0.032578195564623234
train245 0.02131472513115835 greater than 0.02134977106438896
test245 0.03265129210067508
train250 0.02115644621661475 greater than 0.02117088348130082
test250 0.03272167874076502
train252 0.021163052109901964 greater than 0.021154527062990813
test252 0.03273532735950215
3 Accuracy:0.9216171617161716 Lower Bound:0.9064853638655583 Upper Bound:0.9367489595667848
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: True
train0 0.1252183744771738 greater than 1
test0 0.12264303922535572
train5 0.07218804446507712 greater than 0.07792457874100173
test5 0.09648517198926397
train10 0.06037952990673456 greater than 0.06153415609454823
test10 0.07376519742486788
train15 0.0561865610635195 greater than 0.056885348281448464
test15 0.06637044311340101
train20 0.05329286274090417 greater than 0.05377529905235252
test20 0.06347967944356993
train25 0.05150990997713711 greater than 0.05181439043517311
test25 0.05992075515632191
train30 0.05008680915427571 greater than 0.050357648439911806
test30 0.05546554252949331
train35 0.048628558318573975 greater than 0.04896207196526966
test35 0.05143216844948011
train40 0.04695676048336042 greater than 0.04728685608976704
test40 0.048082000554996375
train45 0.04524137371436485 greater than 0.04560884141867204
test45 0.04538011115451659
train50 0.04322020509050556 greater than 0.043614089801278864
test50 0.04040300381102199
train55 0.04073201992508081 greater than 0.04120845907508383
test55 0.03743907108716777
train60 0.03893718200224732 greater than 0.039265584912587095
test60 0.03542790781719199
train65 0.037205485613965174 greater than 0.037580011722784926
test65 0.03431859002691092
train70 0.03573685508072509 greater than 0.03586203411012177
test70 0.03609151692544533
train75 0.03499095888049282 greater than 0.0351576418435938
test75 0.03732948041010152
train80 0.03407966359049595 greater than 0.034116513494403346
test80 0.036780837992366255
train81 0.034083411290925895 greater than 0.03407966359049595
test81 0.03686432485682299
3 Accuracy:0.9108910891089109 Lower Bound:0.8948513036132367 Upper Bound:0.9269308746045851
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: False
train0 0.12599425211618254 greater than 1
test0 0.12474204081927881
train5 0.08412703767093323 greater than 0.09161455003501354
test5 0.07258968123472427
train10 0.06623738383294415 greater than 0.06830221795202217
test10 0.05641547128640684
train15 0.06029275370219312 greater than 0.06115006510016094
test15 0.05120901926420198
train20 0.05715134747614615 greater than 0.05767263086515775
test20 0.04904018025190438
train25 0.05502660061256179 greater than 0.05540171068057974
test25 0.04784769995279911
train30 0.053375162042620564 greater than 0.05368156509044494
test30 0.04706371380564306
train35 0.05196003395928938 greater than 0.052230797660245534
test35 0.046372566965953085
train40 0.050678971493583864 greater than 0.05092546678711357
test40 0.04567450170052353
train45 0.04952501114555454 greater than 0.04974423302843995
test45 0.044995618407117244
train50 0.048535728103684415 greater than 0.048718960850114144
test50 0.0443388716529229
train55 0.047705745522540886 greater than 0.04786393735440003
test55 0.04368376628805274
train60 0.04690925287190542 greater than 0.047071120226637606
test60 0.042987230606120176
train65 0.046097346920995125 greater than 0.04625947485969527
test65 0.04226036609186108
train70 0.045308489162909436 greater than 0.04546096486267146
test70 0.04161554379927011
train75 0.044594628165011654 greater than 0.0447319862622009
test75 0.041102542095586454
train80 0.043917344043983265 greater than 0.04405368117218894
test80 0.04067189293456257
train85 0.04321255225674055 greater than 0.04335690564592367
test85 0.040249732895411484
train90 0.04244684272167624 greater than 0.04261037550013539
test90 0.03967091236495593
train95 0.0413389930698478 greater than 0.04161308928551728
test95 0.03862298221648763
train100 0.03960934910405914 greater than 0.03998659110958612
test100 0.0370263594224842
train105 0.03774067287407157 greater than 0.03809990985149251
test105 0.03537073690824731
train110 0.036162605043078035 greater than 0.036460183687202774
test110 0.0341144245082364
train115 0.03470066195866563 greater than 0.034991333104318095
test115 0.033336129972396426
train120 0.03335786459336134 greater than 0.03360721782341217
test120 0.03286307464459301
train125 0.03222276313185589 greater than 0.03243358191543181
test125 0.0325987574228828
train130 0.03131609303207148 greater than 0.03147859381149619
test130 0.032453922616096816
train135 0.030597801019383218 greater than 0.030731966092740363
test135 0.03240822711964493
train140 0.029978641654944425 greater than 0.030095301257711628
test140 0.032456695174918865
train145 0.029452798107696052 greater than 0.029551219601794236
test145 0.03259515225051617
train150 0.028998157428544193 greater than 0.029083402143182543
test150 0.03279839122544169
train155 0.028674735795040582 greater than 0.02871848427291722
test155 0.03301773462663159
train158 0.02863576320387513 greater than 0.028635178487171266
test158 0.03313179954485355
4 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
Loss calculated for layer count:1 node count: 2 stepSize:0.15 Momentum: True
train0 0.12599425211618254 greater than 1
test0 0.12474204081927881
train5 0.08593893876063516 greater than 0.09088176477750771
test5 0.06693241559255046
train10 0.0670369450650525 greater than 0.06927728740781866
test10 0.05555154108314924
train15 0.06164964034173468 greater than 0.06262025872911423
test15 0.052624959261823534
train20 0.05757198361773371 greater than 0.05825696557732475
test20 0.049756387992663055
train25 0.05514726757741893 greater than 0.055515067481323586
test25 0.047758326792207886
train30 0.05299619439536621 greater than 0.0533956118993242
test30 0.047784644915492405
train35 0.051358388427472514 greater than 0.05163265521125757
test35 0.04880763950458894
train40 0.05039228907700707 greater than 0.050549587115169936
test40 0.05060835205410021
train45 0.04960220666966039 greater than 0.049757816726428136
test45 0.05114989291400977
train50 0.0487777750583623 greater than 0.04895543103413053
test50 0.04997052223952926
train55 0.04780167252761427 greater than 0.04798807409223069
test55 0.04933814720863416
train60 0.04707865615440702 greater than 0.047206811103170675
test60 0.04924756597937217
train65 0.04627759587793755 greater than 0.04645800645188875
test65 0.04831021680172861
train70 0.04549177325514019 greater than 0.04564779543490964
test70 0.0472445104930368
train74 0.04510752072666307 greater than 0.04510085104241141
test74 0.045070949465390975
4 Accuracy:0.8861386138613861 Lower Bound:0.8682554718431963 Upper Bound:0.904021755879576
