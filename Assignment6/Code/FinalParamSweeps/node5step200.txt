Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: False
train0 0.11942531413167348 greater than 1
test0 0.10346979696863579
train5 0.06155732551991239 greater than 0.06437570206575646
test5 0.06862209619662211
train10 0.05425073069603435 greater than 0.0552715500903385
test10 0.05594459766947866
train15 0.05068217108882257 greater than 0.05124633031572599
test15 0.04929152385190184
train20 0.04854592407467469 greater than 0.04890243125799359
test20 0.04560952287664689
train25 0.04706279646898162 greater than 0.0473310619207841
test25 0.04308550321755147
train30 0.045798020245221234 greater than 0.046045471205287514
test30 0.041255901350037816
train35 0.044607732463835925 greater than 0.04483812950134409
test35 0.039768969373892016
train40 0.04353964816853082 greater than 0.04373991170891019
test40 0.03855920020581743
train45 0.04264258007987581 greater than 0.042808159949876155
test45 0.037656515157028864
train50 0.0418824860637762 greater than 0.04202782532133818
test50 0.037056070514092564
train55 0.041202708059935554 greater than 0.0413376821657744
test55 0.03679360072146817
train60 0.04045330496730164 greater than 0.04061303584105754
test60 0.03664413530431329
train65 0.03968440062279072 greater than 0.03982640048206993
test65 0.03640811620157535
train70 0.039029607130039314 greater than 0.03915646120355254
test70 0.03616015350515389
train75 0.03843443781246401 greater than 0.03854687701974492
test75 0.03595626670423071
train80 0.037903246275556406 greater than 0.038006431076046635
test80 0.03590645216494339
train85 0.03740917750317416 greater than 0.037499110591145156
test85 0.0359560784301219
train90 0.03706692257335195 greater than 0.03712436418491406
test90 0.03592323499425828
train95 0.036788919001027334 greater than 0.036847966595254904
test95 0.03593882173071204
train100 0.03653147629024822 greater than 0.036582187414737864
test100 0.036465953526993174
train105 0.03622915111032325 greater than 0.036295613423424425
test105 0.03663198579025589
train110 0.03591569210754954 greater than 0.035973837528929097
test110 0.03613670199632778
train115 0.03559165223897729 greater than 0.0356627139483985
test115 0.03593915992688604
train120 0.03518711847368926 greater than 0.03527687954062765
test120 0.035733582783313005
train125 0.03486267031201273 greater than 0.0349188022268796
test125 0.03546415262388699
train130 0.034615184111562745 greater than 0.034665316020187985
test130 0.03535104743799906
train135 0.03434317317139289 greater than 0.03440579236161259
test135 0.03526746300483152
train140 0.034053065276745245 greater than 0.03409028366948703
test140 0.0351810730589661
train145 0.03361224515937365 greater than 0.033687617185286095
test145 0.03508677355188216
train150 0.03328471186803767 greater than 0.03334730118970892
test150 0.03501006419302696
train155 0.032961595077831404 greater than 0.033029018711240504
test155 0.034889941360474845
train160 0.03264832822272293 greater than 0.03271755923861829
test160 0.034785002698410936
train165 0.03230882395801487 greater than 0.032371982828122536
test165 0.03480093744866245
train170 0.031949913483605694 greater than 0.0320299335560211
test170 0.03486667330992361
train175 0.031526518783500566 greater than 0.03161290370341127
test175 0.035002860172284184
train180 0.031082495390465703 greater than 0.031170724525892587
test180 0.035169009831822216
train185 0.03068994467691702 greater than 0.030761566473838632
test185 0.03533099929679777
train190 0.030409838621188463 greater than 0.0304549892487705
test190 0.03578429688287375
train195 0.030217249786426283 greater than 0.03028022913816296
test195 0.036555482599569665
train198 0.030140876195434554 greater than 0.03010301871517132
test198 0.03679670383746837
0 Accuracy:0.9051155115511551 Lower Bound:0.8886166349039726 Upper Bound:0.9216143881983376
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: True
train0 0.11942531413167348 greater than 1
test0 0.10346979696863579
train5 0.061550817466039645 greater than 0.06436524283013757
test5 0.06880821313510578
train10 0.05423641591645767 greater than 0.05525860742475804
test10 0.05602792907343937
train15 0.05067753423300068 greater than 0.05124064369283416
test15 0.04937448865315774
train20 0.04854350196099973 greater than 0.048899928969364584
test20 0.04558598970754965
train25 0.04705213342136061 greater than 0.047324160812043735
test25 0.04300839321086135
train30 0.04575662970781484 greater than 0.04600985271704278
test30 0.04115252469817652
train35 0.0445457700775383 greater than 0.04477902723754038
test35 0.039644184531273226
train40 0.04348337295926459 greater than 0.04368108620950227
test40 0.038438270502673885
train45 0.04258703151096814 greater than 0.04275458431780921
test45 0.037568926631612355
train50 0.04184750450556456 greater than 0.04198353873273298
test50 0.037018242635076595
train55 0.04115184282616877 greater than 0.04129867672634045
test55 0.03681238277216051
train60 0.040339766445411485 greater than 0.04050793281970088
test60 0.036601049743558864
train65 0.039590652799823454 greater than 0.03972648361199128
test65 0.03635015085291889
train70 0.0389606610857617 greater than 0.03908276880030594
test70 0.03610643521724062
train75 0.038382856965098376 greater than 0.03849240982689832
test75 0.035899714126341395
train80 0.03786923589783255 greater than 0.03797037293648634
test80 0.035826126430022205
train85 0.03740043902945132 greater than 0.03748723335973671
test85 0.03585366476103101
train90 0.037074710786836405 greater than 0.03713384532726988
test90 0.03580228515318377
train95 0.036767143074702184 greater than 0.03681838206457016
test95 0.035997439117112534
train100 0.03655995455229273 greater than 0.036602796840254456
test100 0.03713067352090244
train105 0.036235381162747445 greater than 0.0363082542366701
test105 0.03681419567101911
train110 0.035902519880108126 greater than 0.035966808949696616
test110 0.036333156385915034
train115 0.03558094243593393 greater than 0.03564887446384364
test115 0.03615022912152647
train120 0.035103762216684466 greater than 0.0352190938280712
test120 0.035918148327808534
train125 0.03469769826703928 greater than 0.03476001472946643
test125 0.03565985220109748
train130 0.03457764589213023 greater than 0.034584316426620514
test130 0.03564044324474005
train135 0.034468451890826354 greater than 0.03450643635775429
test135 0.035640279862539474
train140 0.03418500958865919 greater than 0.03429752768696801
test140 0.03558551846256333
train145 0.03378194356047159 greater than 0.03383856643225459
test145 0.03544490379003004
train150 0.03329344816482816 greater than 0.03339886601654697
test150 0.035258677480118496
train155 0.03296549201724466 greater than 0.0330172546564061
test155 0.0351656461152271
train160 0.032711350482713134 greater than 0.03276191368272578
test160 0.035142989672669364
train165 0.03243876673833007 greater than 0.032496665978623475
test165 0.035140736856982574
train170 0.0321241980393205 greater than 0.03219166800400991
test170 0.035145091182535636
train175 0.031680868034006826 greater than 0.031790250801714
test175 0.035187194730741535
train180 0.03127593189549254 greater than 0.031359414538241086
test180 0.035283980874785424
train185 0.030881229550425222 greater than 0.03094944665233971
test185 0.03557536277959827
train190 0.030381137250395908 greater than 0.030504977156771303
test190 0.03584244722361409
train195 0.029952708732273788 greater than 0.030009057098763635
test195 0.03592551935136831
train200 0.02976388482650701 greater than 0.029778403708871345
test200 0.03630509000599063
train201 0.02976697596292005 greater than 0.02976388482650701
test201 0.03639112596964727
0 Accuracy:0.9067656765676567 Lower Bound:0.8903959958211952 Upper Bound:0.9231353573141182
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: False
train0 0.12195456742176077 greater than 1
test0 0.10955182729797638
train5 0.06466816043701493 greater than 0.06809373123553751
test5 0.07451149428058532
train10 0.05659231866703506 greater than 0.05763031753853777
test10 0.06467460135975812
train15 0.053094508876891836 greater than 0.053649312388840645
test15 0.056758335266342406
train20 0.050789681065886425 greater than 0.051202316828985635
test20 0.05104072998426801
train25 0.04910769684471422 greater than 0.04939994264538411
test25 0.04743344983807667
train30 0.04775022394076038 greater than 0.048009617419273475
test30 0.0450215573128606
train35 0.04647778182983013 greater than 0.046733368775876374
test35 0.04317108672780323
train40 0.045162785801835255 greater than 0.045429490170002876
test40 0.0414131057553069
train45 0.043756374588074985 greater than 0.04405163630344103
test45 0.03965252801010825
train50 0.04234747125657005 greater than 0.04261302839132933
test50 0.03812188810804517
train55 0.04095405681739704 greater than 0.04124672364486663
test55 0.036912277903250086
train60 0.039457516573204676 greater than 0.03974867798309331
test60 0.035664857242278594
train65 0.037905553296288176 greater than 0.03823137235075883
test65 0.03407500853012845
train70 0.03630033329289131 greater than 0.03661413832629867
test70 0.03274095501584612
train75 0.03483448945005577 greater than 0.035106711164545626
test75 0.031904110329705644
train80 0.033568983283380996 greater than 0.03382229268813388
test80 0.03151703500669617
train85 0.03247046493360128 greater than 0.03266663571307717
test85 0.03138500155381042
train90 0.03171052029716693 greater than 0.03183472610638557
test90 0.03142660288683384
train95 0.031221607637686697 greater than 0.03130900821955985
test95 0.03162673205007864
train100 0.03077933754871541 greater than 0.030873113440802805
test100 0.03196837230858441
train105 0.030264519473053683 greater than 0.030359979779031147
test105 0.032466670768836976
train110 0.02987130133966949 greater than 0.029999635941679387
test110 0.03316228694592277
train115 0.02925283093630264 greater than 0.029344293850771187
test115 0.03356073928841331
train120 0.028993162895067644 greater than 0.02897482874330725
test120 0.03339393584066998
1 Accuracy:0.9158415841584159 Lower Bound:0.9002114136085113 Upper Bound:0.9314717547083204
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: True
train0 0.12195456742176077 greater than 1
test0 0.10955182729797638
train5 0.07668755197281617 greater than 0.07950356062122088
test5 0.13321356688069821
train10 0.06753285879388224 greater than 0.06992623656398841
test10 0.08198067553225762
train15 0.061306794338716435 greater than 0.062124789990269415
test15 0.06740682433621042
train20 0.05755556762600743 greater than 0.05894377300503856
test20 0.052947563776760895
train25 0.05344785015371556 greater than 0.05445629903514212
test25 0.05032566922744095
train28 0.05188505858969999 greater than 0.05156848617440088
test28 0.044854051958923224
1 Accuracy:0.8968646864686468 Lower Bound:0.8797419993802978 Upper Bound:0.9139873735569958
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: False
train0 0.1229007436351592 greater than 1
test0 0.11298833998326652
train5 0.06640786503256316 greater than 0.07048947452647099
test5 0.07974490099118438
train10 0.05726678223466366 greater than 0.05844873190759186
test10 0.07028818428494882
train15 0.05326972821774582 greater than 0.053893140004636055
test15 0.06365870492264872
train20 0.05070790990859363 greater than 0.05116692018569979
test20 0.05837053894433012
train25 0.048681458981023676 greater than 0.049053881215574045
test25 0.05446497661899232
train30 0.04706031046937913 greater than 0.04735354873856876
test30 0.0515494609680959
train35 0.04576303072043904 greater than 0.0460059902295284
test35 0.049320768989854755
train40 0.044608273132186856 greater than 0.04483158418223914
test40 0.047619020532670774
train45 0.04359965310320591 greater than 0.04378443666234641
test45 0.04631486092517291
train50 0.042770368537598955 greater than 0.04292806461009761
test50 0.045034627684948064
train55 0.04201295425371715 greater than 0.04215711113072325
test55 0.04368525525226754
train60 0.04113046309639964 greater than 0.041364025903849105
test60 0.04287072467303866
train65 0.039880385369028604 greater than 0.04013621691644608
test65 0.0422309647665576
train70 0.038577107663749104 greater than 0.038826685815998414
test70 0.04036972189549559
train75 0.037183932590887536 greater than 0.037490554945111554
test75 0.039003464643604095
train80 0.035649156236677484 greater than 0.03594667724407292
test80 0.038280368727455195
train85 0.03424283105074917 greater than 0.03452441875624194
test85 0.03759959582985543
train90 0.03307332740452735 greater than 0.033282040423531424
test90 0.03709397484383367
train95 0.03201535903484292 greater than 0.03222991675134261
test95 0.03673376046802901
train100 0.030764868607368365 greater than 0.031044566765443576
test100 0.03605977995320486
train105 0.029645401264880654 greater than 0.02985477614696317
test105 0.03525278947473188
train110 0.028671171681834223 greater than 0.028852555428679753
test110 0.034576988093493785
train115 0.027824420326754723 greater than 0.027992164974007496
test115 0.03411186724943114
train120 0.026957522398986314 greater than 0.027119618233942468
test120 0.033767636139445056
train125 0.02631169250154306 greater than 0.026425862116737663
test125 0.03351979291460627
train130 0.025814168003344063 greater than 0.02590573782740176
test130 0.03332642373357946
train135 0.02538521838008783 greater than 0.025468452678585964
test135 0.0332042524413086
train140 0.0249941824619981 greater than 0.025067982210736597
test140 0.03317691376718239
train145 0.02464044066081871 greater than 0.024712033548705218
test145 0.033244281026231615
train150 0.024251885125001732 greater than 0.024332551608702017
test150 0.033340675029739995
train155 0.02392393332642929 greater than 0.02398046188177427
test155 0.03334595390208072
train159 0.023778559417114428 greater than 0.023746040990577016
test159 0.033380569542541566
2 Accuracy:0.9108910891089109 Lower Bound:0.8948513036132367 Upper Bound:0.9269308746045851
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: True
train0 0.1229007436351592 greater than 1
test0 0.11298833998326652
train5 0.07204185921536875 greater than 0.0769545890873072
test5 0.11364537105752785
train10 0.05938387812823322 greater than 0.06095373886568291
test10 0.07888641782934108
train15 0.0525891903243287 greater than 0.05393773708385766
test15 0.05171081909932917
train20 0.04731579884400022 greater than 0.0481574798119162
test20 0.04502228666995528
train25 0.04416757463752209 greater than 0.04482373421396729
test25 0.03989686749810211
train30 0.04208959491863203 greater than 0.042359286388566335
test30 0.03789678822190087
train35 0.04074910886416596 greater than 0.041081767002736656
test35 0.03572526185858608
train40 0.03740638366146018 greater than 0.03800238961630302
test40 0.03373060992093909
train45 0.03531951234641635 greater than 0.035698419012959816
test45 0.03303143651362244
train50 0.03413411483457009 greater than 0.03403942138949981
test50 0.03611577703034749
2 Accuracy:0.9075907590759076 Lower Bound:0.8912862586847559 Upper Bound:0.9238952594670593
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: False
train0 0.12485702841102926 greater than 1
test0 0.11796515564268202
train5 0.06854938575404382 greater than 0.07291248285120364
test5 0.08629362952642186
train10 0.05919124282576638 greater than 0.06030053105057704
test10 0.0740951501842868
train15 0.05527023959867295 greater than 0.05590897022467969
test15 0.06655370396329673
train20 0.05270649091089559 greater than 0.05315177890256911
test20 0.06034051029488557
train25 0.05080596139886258 greater than 0.05115206745889814
test25 0.05564490882920393
train30 0.049203711050743094 greater than 0.049516135095481965
test30 0.05129606043727177
train35 0.04756705744881522 greater than 0.04791106156211805
test35 0.04745894898916197
train40 0.045682908825967906 greater than 0.046084775660139785
test40 0.04461446444045407
train45 0.043442754986168695 greater than 0.04390773262612889
test45 0.041987403822031005
train50 0.04119851286352736 greater than 0.04162813042579558
test50 0.039573947410942896
train55 0.03914466475404621 greater than 0.03955098698724503
test55 0.037403531249010434
train60 0.03709427345406763 greater than 0.03750604608457711
test60 0.035242962217038015
train65 0.03530292538342315 greater than 0.03563335370246303
test65 0.03389105504459658
train70 0.03368284688113438 greater than 0.033959292739266865
test70 0.032799816831569024
train75 0.032476860528231094 greater than 0.03269776949343908
test75 0.031903940336625795
train80 0.03140852288445875 greater than 0.031611166233605506
test80 0.0315877066648444
train85 0.030620473424262874 greater than 0.030743657675781078
test85 0.03145435526883643
train90 0.030003557247922977 greater than 0.03016028065811188
test90 0.0314422846550741
train95 0.029289167886905502 greater than 0.029375953004501666
test95 0.03141981705494785
train100 0.028544772753072132 greater than 0.028718321954760033
test100 0.03145235953270373
train105 0.027747497880043707 greater than 0.027895535318730715
test105 0.0315131322697914
train110 0.027097850588379574 greater than 0.027217053492517247
test110 0.031552066970728494
train115 0.026557058028612687 greater than 0.026658054257938397
test115 0.03162128443291554
train120 0.026180033309603296 greater than 0.02624350747102174
test120 0.031737756794732175
train125 0.025812330452621046 greater than 0.025891922030178538
test125 0.03185140928388799
train130 0.025355489131791562 greater than 0.025463384959827213
test130 0.031920110506987405
train135 0.024703978601980477 greater than 0.02484649676029009
test135 0.03191458473241283
train140 0.023875328755952697 greater than 0.02404276382063175
test140 0.03199914785659053
train145 0.02320793855672169 greater than 0.023326024386145026
test145 0.03232429416970506
train150 0.022835694815039918 greater than 0.022890185938604583
test150 0.03311378012274923
train155 0.022415455492301384 greater than 0.022500085814346656
test155 0.034202906421415116
train159 0.022458785663781716 greater than 0.02229820191027317
test159 0.03441262301437447
3 Accuracy:0.915016501650165 Lower Bound:0.8993169761328473 Upper Bound:0.9307160271674827
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: True
train0 0.12485702841102926 greater than 1
test0 0.11796515564268202
train5 0.06964935059849389 greater than 0.07507811080760961
test5 0.101598263497618
train10 0.05883706604982649 greater than 0.05997979999898729
test10 0.07360453619290974
train15 0.053963888164117686 greater than 0.054854715485686496
test15 0.06519485608823584
train20 0.049876848302603555 greater than 0.050787733018953646
test20 0.06468368804486346
train25 0.046632449092085186 greater than 0.04714524313473364
test25 0.05125967649219902
train30 0.04423744320793793 greater than 0.04465169170019032
test30 0.04414997574658818
train35 0.04233080837957409 greater than 0.04294293046646381
test35 0.04339982644610411
train40 0.04102110146126193 greater than 0.041196941160249016
test40 0.04061354196645618
train45 0.039017171140943496 greater than 0.039663252514219975
test45 0.04182484329889977
train50 0.03704868378617379 greater than 0.03739114629892884
test50 0.03996157138232533
train55 0.0357802750814142 greater than 0.03595092407088228
test55 0.03638885620634609
train56 0.03620606861194763 greater than 0.0357802750814142
test56 0.036162293636925066
3 Accuracy:0.900990099009901 Lower Bound:0.8841748197670671 Upper Bound:0.9178053782527349
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: False
train0 0.1261086545115085 greater than 1
test0 0.12332567661910702
train5 0.07667022269851854 greater than 0.08208926122328375
test5 0.06265017473306829
train10 0.06389397227878432 greater than 0.06537452125731867
test10 0.052447110368934804
train15 0.0590158764624511 greater than 0.0597748751674962
test15 0.04932058969128454
train20 0.056077983812536175 greater than 0.05657582696362936
test20 0.04767335589782974
train25 0.05399764830321875 greater than 0.05437370574467824
test25 0.04630570572623184
train30 0.052251482196711066 greater than 0.05259109612565983
test30 0.04497602760696662
train35 0.050540749845520525 greater than 0.05088757234385631
test35 0.04356009426655182
train40 0.04877663226227034 greater than 0.04913079748323963
test40 0.041859286513325225
train45 0.047059978205339616 greater than 0.047394532277939015
test45 0.03989707917562435
train50 0.04528992444795821 greater than 0.04566979949350744
test50 0.0377593115125685
train55 0.043035287631816806 greater than 0.043540679616079464
test55 0.035589748392674876
train60 0.04037998105455921 greater than 0.04091778710678
test60 0.03392881614252427
train65 0.037869139338238544 greater than 0.03832082622075297
test65 0.03284075822397415
train70 0.036054841147586565 greater than 0.03633485990874238
test70 0.03225358166715336
train75 0.03472339909694785 greater than 0.034995311774568244
test75 0.03180004665606014
train80 0.033271542481398055 greater than 0.033559033835790876
test80 0.03142665866712698
train85 0.0319693477254425 greater than 0.03220530030423992
test85 0.03154993912081883
train90 0.031017485592553354 greater than 0.031179354059126394
test90 0.03183899501629627
train95 0.030333907635741147 greater than 0.03046179793316926
test95 0.032106445550200216
train100 0.02964963644771727 greater than 0.029797723971489228
test100 0.03201556588391691
train105 0.028959420540175827 greater than 0.02906886154034642
test105 0.03159562561028252
train110 0.02837520161060474 greater than 0.028514615927674878
test110 0.03142378515168897
train115 0.027873191675647415 greater than 0.027946900594418803
test115 0.031599169301116566
train120 0.02751362145773266 greater than 0.02759460534218537
test120 0.03139088617270071
train125 0.027151482269324553 greater than 0.027229293259447344
test125 0.031390350337593194
train130 0.02689059736181221 greater than 0.02692732711201545
test130 0.03154273052977558
train135 0.026390433539184657 greater than 0.026519462310172614
test135 0.031914933990091744
train140 0.02570855437048953 greater than 0.0258321986914696
test140 0.0322164071852091
train145 0.02524233783730884 greater than 0.025234359467537576
test145 0.032496626473146044
4 Accuracy:0.91996699669967 Lower Bound:0.9046904411913306 Upper Bound:0.9352435522080094
Loss calculated for layer count:1 node count: 5 stepSize:0.2 Momentum: True
train0 0.1261086545115085 greater than 1
test0 0.12332567661910702
train5 0.0846048751430682 greater than 0.08957488112751702
test5 0.06435750111738259
train10 0.06583353145017214 greater than 0.06828885476055896
test10 0.05435980351146122
train15 0.06010147947615093 greater than 0.06078110167675358
test15 0.050124448582046593
train20 0.05721249598612718 greater than 0.0577357944403413
test20 0.05002022448099031
train25 0.055434929676741754 greater than 0.05582645509846574
test25 0.047715598255196265
train30 0.05311072496679037 greater than 0.05359104857947548
test30 0.046082406089063076
train35 0.05013058355912952 greater than 0.05090272373236408
test35 0.044687021127225945
train40 0.04699424538600545 greater than 0.047392092129508095
test40 0.044957910350941294
train45 0.04405019487122244 greater than 0.04457355322952681
test45 0.04554657868075461
train50 0.04195050433719786 greater than 0.042432417770205975
test50 0.045930834200627504
train52 0.04221094480767078 greater than 0.04194755847902879
test52 0.04588540347388103
4 Accuracy:0.8811881188118812 Lower Bound:0.8629714478483704 Upper Bound:0.899404789775392
