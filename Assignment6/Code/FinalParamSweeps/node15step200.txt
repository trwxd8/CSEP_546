Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: False
train0 0.11584909037250365 greater than 1
test0 0.10036907470047506
train5 0.0607727192902663 greater than 0.06353118731639178
test5 0.0705990072852281
train10 0.0531644965212002 greater than 0.05420224314371541
test10 0.054904878138603015
train15 0.04935186449549565 greater than 0.04997719779139637
test15 0.04614602014035057
train20 0.04676115071378875 greater than 0.04723178752042673
test20 0.04204458819977501
train25 0.04479954693090212 greater than 0.04512529016769077
test25 0.03982221602342779
train30 0.043343954043359 greater than 0.043620465043489785
test30 0.03813309640650255
train35 0.041921519591319095 greater than 0.042207382801520255
test35 0.035910088925425694
train40 0.0400370496468288 greater than 0.040506506448697834
test40 0.03452093991178781
train45 0.037973601654474054 greater than 0.038376594828135026
test45 0.03374534074052014
train50 0.03633853543852146 greater than 0.03664666150614774
test50 0.033228493076934304
train55 0.03483836045838841 greater than 0.035132468542469965
test55 0.033151229104152746
train60 0.03340365666021902 greater than 0.033665259592212574
test60 0.03327170463372713
train65 0.0323462684875597 greater than 0.032606946935013426
test65 0.0338909998861156
train70 0.03112590057347213 greater than 0.03136075314140548
test70 0.03414251141904378
train75 0.029766567515976904 greater than 0.030018268609271186
test75 0.03342865364138683
train80 0.028663631692996277 greater than 0.02883497117956617
test80 0.03319821163369946
train85 0.0276202677592779 greater than 0.02783348281817504
test85 0.033359740658933996
train90 0.026920719162736688 greater than 0.02699504871733313
test90 0.03276621209254533
train95 0.02607490863289231 greater than 0.02627849407262901
test95 0.034209045141961096
train100 0.025440674393301246 greater than 0.02552343391415158
test100 0.03478921917687504
train105 0.025047292126718107 greater than 0.025109401266009006
test105 0.038610521224122285
train106 0.025061113944463066 greater than 0.025047292126718107
test106 0.038019652319380134
0 Accuracy:0.9067656765676567 Lower Bound:0.8903959958211952 Upper Bound:0.9231353573141182
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: True
train0 0.11584909037250365 greater than 1
test0 0.10036907470047506
train5 0.06202664432160015 greater than 0.06561961491051883
test5 0.07781889024194606
train10 0.052771057202416136 greater than 0.05397855560451821
test10 0.04670147159635414
train15 0.04813071310264172 greater than 0.04901562068293883
test15 0.040190695763098305
train20 0.043635398515248176 greater than 0.04429254477607281
test20 0.03779934851971353
train25 0.041286873658108895 greater than 0.04185876612268012
test25 0.03784955298737084
train30 0.037386942703970666 greater than 0.038105318267744805
test30 0.03515077374686976
train35 0.03631985623226293 greater than 0.03650498928653392
test35 0.03473291137273625
train40 0.03402165731638195 greater than 0.034558391246926445
test40 0.037266566648923936
train45 0.032500788878709705 greater than 0.0326627629617474
test45 0.03193843782167573
train48 0.03272379121407745 greater than 0.03244374055623012
test48 0.034115885624692796
0 Accuracy:0.9075907590759076 Lower Bound:0.8912862586847559 Upper Bound:0.9238952594670593
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: False
train0 0.11932187790600138 greater than 1
test0 0.1034462098053331
train5 0.063697147380593 greater than 0.0668569782094007
test5 0.07490215075381719
train10 0.05522294721416926 greater than 0.056377232429033294
test10 0.06059312159897996
train15 0.05105968832533224 greater than 0.05174095003140026
test15 0.052659343619305216
train20 0.048324442597005725 greater than 0.0488008440594448
test20 0.048007905157614654
train25 0.046245261659235586 greater than 0.046636236406721716
test25 0.04513619375511957
train30 0.04424967265576155 greater than 0.04465732817384645
test30 0.04287229380311927
train35 0.0423504628905644 greater than 0.042695147353788605
test35 0.04064406956353487
train40 0.040622419770977414 greater than 0.04100295246318065
test40 0.0375981396052821
train45 0.03860148402016259 greater than 0.039035960810336966
test45 0.03488435205012055
train50 0.03649302724864287 greater than 0.03686967035217439
test50 0.033934358709734706
train55 0.034796881294156694 greater than 0.03509558640132852
test55 0.03469108995056603
train60 0.03349578068261425 greater than 0.033726727701622876
test60 0.0359740385798962
train65 0.03255820446369882 greater than 0.03275551499097365
test65 0.03659189040710834
train70 0.03117841859797771 greater than 0.03137040232025626
test70 0.03558517344895064
train75 0.030286073911015482 greater than 0.030446707160026225
test75 0.034482292069058985
train80 0.029542705838936593 greater than 0.029684500653048975
test80 0.03377135949241467
train85 0.028912923590645984 greater than 0.029025388374046387
test85 0.0332930584506768
train90 0.028355157999895034 greater than 0.028471452555417432
test90 0.033034016714176224
train95 0.027645825503402714 greater than 0.027799431625764533
test95 0.032924077785005855
train100 0.02680586668451838 greater than 0.02697350995859508
test100 0.03302927816546943
train105 0.026057942385287823 greater than 0.026195609169593702
test105 0.03329117510301664
train110 0.025594152707422376 greater than 0.025656225460770034
test110 0.03265632311920204
train115 0.025162160502045835 greater than 0.0252810356442948
test115 0.03222159177751388
train120 0.02453115739087425 greater than 0.024637421671653694
test120 0.03409279140413328
train125 0.023806794029373485 greater than 0.02395893746843399
test125 0.03439495196380405
train130 0.02351459515736748 greater than 0.023577384068962624
test130 0.0327263577693011
train135 0.023141790799348125 greater than 0.02322611834132577
test135 0.033139767158216836
train140 0.022705040925551763 greater than 0.022779625373614464
test140 0.031635510503113466
train145 0.022371904767458626 greater than 0.02243011928440061
test145 0.0320390905725783
train149 0.022746716544476867 greater than 0.022036549751680987
test149 0.032036177333770455
1 Accuracy:0.9158415841584159 Lower Bound:0.9002114136085113 Upper Bound:0.9314717547083204
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: True
train0 0.11932187790600138 greater than 1
test0 0.1034462098053331
train5 0.0636279932099042 greater than 0.0672863102070097
test5 0.08588619320549198
train10 0.0544015370990977 greater than 0.05578144474389351
test10 0.05775272549403242
train15 0.04897382054186318 greater than 0.04990880443906809
test15 0.04440855766453306
train20 0.04418107494648923 greater than 0.04507888377059162
test20 0.038665043430803896
train25 0.040371586974003185 greater than 0.041050084809435165
test25 0.03669761974677506
train30 0.037933002796082145 greater than 0.03827730915809783
test30 0.03507533078925294
train35 0.03684265912811606 greater than 0.03709434596767901
test35 0.03220588208469472
train40 0.03523214505601167 greater than 0.035433912550804636
test40 0.032344814517651074
train45 0.03373372227690872 greater than 0.033973363421319944
test45 0.03052105981930383
train50 0.03225972508130718 greater than 0.03258395535223792
test50 0.030490427574491472
train55 0.03138362591837388 greater than 0.031279923676438504
test55 0.031001187162754896
1 Accuracy:0.9224422442244224 Lower Bound:0.907383561948374 Upper Bound:0.9375009265004709
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: False
train0 0.12394976329906994 greater than 1
test0 0.11462762620995873
train5 0.06810744707141009 greater than 0.07287617778962151
test5 0.07850342978397269
train10 0.05747171339568073 greater than 0.05876856704278294
test10 0.06882650701936764
train15 0.052980391308811284 greater than 0.053694630141472285
test15 0.06216924863389466
train20 0.05018185094866442 greater than 0.05066328464950434
test20 0.05758929046085063
train25 0.0481605472410037 greater than 0.0485212634304191
test25 0.05415999370536136
train30 0.04660637163312538 greater than 0.04688760278438472
test30 0.051329253232756555
train35 0.04537344347675404 greater than 0.04560088114190331
test35 0.04901488474497118
train40 0.04432337404421356 greater than 0.04452366404782278
test40 0.0470850443530615
train45 0.04339633023852301 greater than 0.043571853855573835
test45 0.045582484449310734
train50 0.04257330275142801 greater than 0.04273201224440747
test50 0.044352399184548454
train55 0.04181234966933703 greater than 0.04196065400365541
test55 0.043236448181342375
train60 0.04107276285458874 greater than 0.04122989722187692
test60 0.042373894460307114
train65 0.039967165809936846 greater than 0.040215061988908805
test65 0.042427408555356476
train70 0.0386970757962526 greater than 0.03896120367750894
test70 0.04183184995253622
train75 0.03724721686745752 greater than 0.037564606536160144
test75 0.04109868130391992
train80 0.03553367692155534 greater than 0.03588341103227369
test80 0.040622888405300005
train85 0.033772807927936466 greater than 0.034116470747056545
test85 0.03924775082671799
train90 0.03224920231710873 greater than 0.032528786543811375
test90 0.03741353169323797
train95 0.030990408988857323 greater than 0.03122689271170191
test95 0.035942929226065845
train100 0.02987530926046419 greater than 0.030090828856167668
test100 0.0343842805393576
train105 0.028830520900747256 greater than 0.02903672635142627
test105 0.03284966876132908
train110 0.027843568521455055 greater than 0.02803391652130957
test110 0.031542039531401465
train115 0.02692057143866989 greater than 0.027100655276000498
test115 0.03086629672646499
train120 0.026084812090105998 greater than 0.026243999638810626
test120 0.030703049028239976
train125 0.02534221853665699 greater than 0.025483586025610676
test125 0.031126579372280617
train130 0.024679412218758443 greater than 0.024807110138848013
test130 0.03242394799566397
train135 0.024077904888853474 greater than 0.02419338585003116
test135 0.03368471334218671
train140 0.02330482694684294 greater than 0.023508204685992604
test140 0.03482422166326674
train145 0.022506453815162728 greater than 0.022647099761421364
test145 0.03548093758412865
train150 0.02190212233339871 greater than 0.02200661069217142
test150 0.035429276530482154
train155 0.0215028954699472 greater than 0.021574378246525253
test155 0.033799556807646285
train160 0.021095103156770628 greater than 0.02118538244871252
test160 0.03180226994533261
train165 0.020644420048250543 greater than 0.02071771816081285
test165 0.03053394367851282
train170 0.020363924685372927 greater than 0.020424505577958545
test170 0.029879105903675107
train175 0.019818406256895297 greater than 0.019874597070151438
test175 0.029641386490673632
train180 0.019687350077500596 greater than 0.019729918934431667
test180 0.029475474935968273
train185 0.019325273183971124 greater than 0.0193483461182701
test185 0.029741090626075013
train186 0.019347551761812017 greater than 0.019325273183971124
test186 0.02949072537793119
2 Accuracy:0.9298679867986799 Lower Bound:0.9154908116842401 Upper Bound:0.9442451619131197
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: True
train0 0.12394976329906994 greater than 1
test0 0.11462762620995873
train5 0.0700199318579967 greater than 0.07518470618459064
test5 0.101230259014734
train10 0.05752866998931357 greater than 0.05901440225020437
test10 0.07434576891257683
train15 0.05166029252561254 greater than 0.05263046116725896
test15 0.05548259546394751
train20 0.047076871224799914 greater than 0.048009183684884854
test20 0.049723375838837666
train25 0.04304288354382798 greater than 0.043690860215516145
test25 0.04274168638854092
train30 0.03986315253379097 greater than 0.04040116944173141
test30 0.038598055429996524
train35 0.03797174376578408 greater than 0.03825448616458938
test35 0.03774892713167074
train40 0.035229091356902685 greater than 0.03571372155297809
test40 0.033390867388427926
train45 0.03399940682441682 greater than 0.03422755217813981
test45 0.032654061531275584
train50 0.03300437430336234 greater than 0.0332457445182726
test50 0.03281105446906937
train55 0.032113901509575674 greater than 0.03239296716344339
test55 0.03204243025124636
train60 0.031182828277484087 greater than 0.0313948868483628
test60 0.032811203436107966
train65 0.030669531760684456 greater than 0.030727167361005985
test65 0.03276972808875341
train70 0.029649737283090432 greater than 0.029885691426892118
test70 0.03250930112675405
train73 0.029437244663097165 greater than 0.029396869413577293
test73 0.03230696017828446
2 Accuracy:0.9166666666666666 Lower Bound:0.9011062988741652 Upper Bound:0.9322270344591681
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: False
train0 0.12389966535151081 greater than 1
test0 0.11236347318024892
train5 0.06821632513655924 greater than 0.07236987484752413
test5 0.08530906111938909
train10 0.05865782543798729 greater than 0.05982851694796382
test10 0.07271949252962433
train15 0.05448190024715701 greater than 0.055167500766679
test15 0.06473257477685021
train20 0.051721985854076565 greater than 0.05220447259058031
test20 0.058604337940597585
train25 0.049668965812835086 greater than 0.05003824221670241
test25 0.05437096838235247
train30 0.04805893368160413 greater than 0.048353681233857526
test30 0.051493537345571876
train35 0.04673419892425552 greater than 0.046982174771313084
test35 0.04947468441054828
train40 0.04559489286810953 greater than 0.04581083281903129
test40 0.047983703878784034
train45 0.04457041416260424 greater than 0.04477046839110065
test45 0.046851684853408924
train50 0.04359440162087284 greater than 0.04378567106207818
test50 0.04590582378476814
train55 0.04268325357114886 greater than 0.04285884012564557
test55 0.04491392925728486
train60 0.04184891975178904 greater than 0.042010908374069164
test60 0.04381702017533708
train65 0.04105613701564851 greater than 0.04121396394470174
test65 0.04281497428610054
train70 0.040250501291475144 greater than 0.04041386360240266
test70 0.0420230478900904
train75 0.039446811806956736 greater than 0.03960493096213603
test75 0.04136097808943521
train80 0.03864733351329221 greater than 0.038811887097969046
test80 0.04070978427019691
train85 0.03770941365906051 greater than 0.037919468494464356
test85 0.03983284174195447
train90 0.036303788141959906 greater than 0.03661725256210269
test90 0.038513527576333854
train95 0.034982576751121486 greater than 0.03523993140252579
test95 0.03733750808475094
train100 0.03378130171636539 greater than 0.034010760908679515
test100 0.03614027581896389
train105 0.032653327431620825 greater than 0.032883159550303714
test105 0.035002667413111525
train110 0.03150751448291098 greater than 0.03173177778964515
test110 0.03425690408350622
train115 0.03036850767468084 greater than 0.030602087490138898
test115 0.03390219513397129
train120 0.029133751094804806 greater than 0.029383688752660043
test120 0.033790785564097314
train125 0.02794206082564583 greater than 0.028174497977699147
test125 0.03358896631712559
train130 0.02686338584562867 greater than 0.027060791894815476
test130 0.033294516727668276
train135 0.026010098402874608 greater than 0.026175393384774263
test135 0.0334992597911603
train140 0.025228119625361815 greater than 0.025371968143262803
test140 0.03436148033911777
train145 0.024506873229717827 greater than 0.024668701839679716
test145 0.03532686711778283
train150 0.023573941882315676 greater than 0.023756245898572226
test150 0.03564458314430874
train155 0.022416779964267 greater than 0.022811040336031006
test155 0.035263684008136995
train160 0.021633905000315302 greater than 0.021794404481386396
test160 0.034572593513348314
train163 0.021503264539486155 greater than 0.02136357884060521
test163 0.03397128745334536
3 Accuracy:0.9100660066006601 Lower Bound:0.8939594335446445 Upper Bound:0.9261725796566757
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: True
train0 0.12389966535151081 greater than 1
test0 0.11236347318024892
train5 0.07777694851845605 greater than 0.08136554775036481
test5 0.0886162262139994
train10 0.06284374359254817 greater than 0.0649247298585547
test10 0.07684406533824993
train15 0.05651833928492136 greater than 0.057367305959033284
test15 0.06434470987760176
train20 0.05408285459662422 greater than 0.054441633734052135
test20 0.05855717928157817
train25 0.050953757089965086 greater than 0.05161756603365359
test25 0.06009491420236082
train30 0.04932165398851724 greater than 0.04960414459098959
test30 0.05016448577621422
train33 0.04840767111016325 greater than 0.04837709937034679
test33 0.05121185942068433
3 Accuracy:0.8754125412541254 Lower Bound:0.8568195910052603 Upper Bound:0.8940054915029905
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: False
train0 0.12594962796650244 greater than 1
test0 0.11991173749775348
train5 0.07636369482018068 greater than 0.08155290424018241
test5 0.06281257437532876
train10 0.06361246378595184 greater than 0.06512389463237787
test10 0.052218640404012
train15 0.058548545071072364 greater than 0.059346773794791786
test15 0.048764918871057016
train20 0.05541821768995843 greater than 0.05595241456801987
test20 0.04685463109696292
train25 0.05319926331393426 greater than 0.0535950649449409
test25 0.04540185924772569
train30 0.051445702361893636 greater than 0.051773044465203574
test30 0.04421284813468062
train35 0.04992879045498901 greater than 0.050217890387683775
test35 0.04321124693859127
train40 0.04857195635198358 greater than 0.0488329419605629
test40 0.04231429982834067
train45 0.04732645585250721 greater than 0.04756832896156311
test45 0.04140936240284662
train50 0.04616072763560659 greater than 0.04638954807426967
test50 0.04034449298252113
train55 0.044989872394616624 greater than 0.0452347921904343
test55 0.03897444584791284
train60 0.04357845916166267 greater than 0.04388149371426191
test60 0.037501514136351215
train65 0.042043445242132146 greater than 0.04235479032363211
test65 0.03611028910073706
train70 0.04052722480850557 greater than 0.040819947073659595
test70 0.03464025219243854
train75 0.039157506098510333 greater than 0.03941225903475534
test75 0.03336480044164136
train80 0.038063594013606525 greater than 0.03826552987945348
test80 0.03238891976178799
train85 0.036761043821025095 greater than 0.03708307880554286
test85 0.031607196460597356
train90 0.03487848966648089 greater than 0.03524529648763009
test90 0.03103217556806907
train95 0.03324488523631521 greater than 0.03354977330690947
test95 0.03061165998467483
train100 0.03183526662539667 greater than 0.032131373629308575
test100 0.030359316327787586
train105 0.030407611439252396 greater than 0.030676277956577642
test105 0.030286076243827516
train110 0.029027322137745424 greater than 0.029308590191675414
test110 0.030441255203091457
train115 0.027975770552096834 greater than 0.028165058471439144
test115 0.030707711226424933
train120 0.027066601781814897 greater than 0.027237312947940923
test120 0.03077596565873223
train125 0.026255157080209042 greater than 0.026406619585743415
test125 0.030550934028580775
train130 0.02531504833911473 greater than 0.02555251635631898
test130 0.030530421061016416
train135 0.02421866200881068 greater than 0.02440473642350383
test135 0.030420791136992576
train140 0.023398618081004206 greater than 0.023537675073236276
test140 0.0304984796359948
train145 0.022788224112916384 greater than 0.022927502837870598
test145 0.030972171272589756
train150 0.021908669990991246 greater than 0.02210818917171452
test150 0.031282983615172386
train155 0.02117873610857972 greater than 0.02128003717862694
test155 0.030630036199669343
train160 0.020568521631057744 greater than 0.02069361247413602
test160 0.0307771660299218
train165 0.020034497753678233 greater than 0.020131545644377583
test165 0.031197460829645345
train170 0.019675879922770957 greater than 0.0197233434472755
test170 0.03218408419339028
train174 0.02038860299056672 greater than 0.01946393271255654
test174 0.031880282979722764
4 Accuracy:0.9174917491749175 Lower Bound:0.9020016379834073 Upper Bound:0.9329818603664277
Loss calculated for layer count:1 node count: 15 stepSize:0.2 Momentum: True
train0 0.12594962796650244 greater than 1
test0 0.11991173749775348
train5 0.0912443558665071 greater than 0.09498533457109631
test5 0.07034760401085188
train10 0.08393680015486943 greater than 0.08402051596572493
test10 0.07860839189810524
train14 0.07738217349379936 greater than 0.07553466801778592
test14 0.06743278182567562
4 Accuracy:0.8143564356435643 Lower Bound:0.7924661223611679 Upper Bound:0.8362467489259607
