Train is 0.491469 percent closed.
Test is 0.493399 percent closed.
Calculating features...
<bound method MostCommonModel.predict of <MostCommonModel.MostCommonModel object at 0x000002481197F278>>
Most Common Accuracy: 0.5066006600660066 (0.4784533485794732, 0.53474797155254)
Loss calculated for layer count: 1  node count:  2
train 0   0.12493274793662469
test 0   0.12378329708417331
train 25   0.048698054075384876
test 25   0.042137287890268126
train 50   0.045173140914770225
test 50   0.0384284262086921
train 75   0.043397577290717734
test 75   0.037596887254173116
train 100   0.042013426270857815
test 100   0.037330702608297156
train 125   0.04058477183199129
test 125   0.037516052137472215
train 150   0.038400424850201405
test 150   0.03513137620641999
train 175   0.03462128228265868
test 175   0.03188485737135437
train 200   0.03210349766286209
test 200   0.03181919230904723
train 225   0.030465401971785152
test 225   0.032476048126897535
train 250   0.02943732796080692
test 250   0.03257542950495865
train 275   0.028601831137441497
test 275   0.03253529463060884
train 300   0.027945227716368357
test 300   0.03237368689120486
train 325   0.027425104789132574
test 325   0.03226954669887942
train 350   0.026996874707687343
test 350   0.03227647765913286
train 375   0.026626522331277054
test 375   0.032325224876659245
train 400   0.02629825034706094
test 400   0.03236114571916012
Loss calculated for layer count: 1  node count:  5
train 0   0.12480380268564838
test 0   0.1228772052573908
train 25   0.04851110428891578
test 25   0.041925795704432146
train 50   0.04195963466827055
test 50   0.035633567435931136
train 75   0.03559101805833216
test 75   0.030711096490266487
train 100   0.03234633457374208
test 100   0.030632713373487774
train 125   0.03043470684524994
test 125   0.030764464943395235
train 150   0.029239076327425985
test 150   0.03099880837764887
train 175   0.02832202698609886
test 175   0.031192725671985982
train 200   0.027539708464203474
test 200   0.031241430447713783
train 225   0.026889377132372303
test 225   0.031210292160659573
train 250   0.026312999404536883
test 250   0.03111361739558231
train 275   0.025769593156155488
test 275   0.03099783655740812
train 300   0.02525097255977529
test 300   0.030935391977059187
train 325   0.024726559841450998
test 325   0.03085742498597276
train 350   0.024116268924592576
test 350   0.030716051703085473
train 375   0.023349943264861235
test 375   0.030656720131157233
train 400   0.022586806417163707
test 400   0.030694573886383577
Loss calculated for layer count: 1  node count:  10
train 0   0.12482615381192816
test 0   0.1214858691427887
train 25   0.048461530715135454
test 25   0.04191825868229868
train 50   0.04292856847310473
test 50   0.03613505457566805
train 75   0.03584601610437846
test 75   0.03057258989047975
train 100   0.032266148306413195
test 100   0.030147983123613234
train 125   0.03025729999968176
test 125   0.02990158001893703
train 150   0.029007781779795124
test 150   0.029699525374991233
train 175   0.02797619733360112
test 175   0.02957177696790527
train 200   0.02708946881679647
test 200   0.02938567074166254
train 225   0.02632185396781492
test 225   0.029426609641032383
train 250   0.025635908226036216
test 250   0.02978245267224046
train 275   0.025007204322567478
test 275   0.030269678581562615
train 300   0.024405188795411134
test 300   0.03085992321554412
train 325   0.023797763248507207
test 325   0.03158730607285791
train 350   0.023143641183997976
test 350   0.03250384474018496
train 375   0.022421072389475907
test 375   0.033533758219043926
train 400   0.02168865942518146
test 400   0.03443087786821598
Loss calculated for layer count: 1  node count:  15
train 0   0.12376580055229301
test 0   0.11842653999753978
train 25   0.048373253819308966
test 25   0.0417444500553902
train 50   0.04372695279784135
test 50   0.03699228238525875
train 75   0.03902174559077883
test 75   0.03273962559885244
train 100   0.034382797548702104
test 100   0.03017974000957298
train 125   0.03140734823669119
test 125   0.030086018931340275
train 150   0.029482429110277313
test 150   0.02980918074394815
train 175   0.028165268258125785
test 175   0.029748456207054273
train 200   0.027195964280742443
test 200   0.02984430280071284
train 225   0.026351629252563392
test 225   0.02986125554126819
train 250   0.025534428384312937
test 250   0.02989427597064188
train 275   0.02470115365668949
test 275   0.02993963892903364
train 300   0.02383403783404051
test 300   0.03005959051354574
train 325   0.0230146234631043
test 325   0.03020343641211406
train 350   0.02221934221330063
test 350   0.030282155606232295
train 375   0.021369385915709298
test 375   0.03035886802271397
train 400   0.02055092031635845
test 400   0.030468284253650637
Loss calculated for layer count: 1  node count:  20
train 0   0.12403958856675458
test 0   0.11806981425808727
train 25   0.0481498789457324
test 25   0.04148010414562418
train 50   0.04371100028742967
test 50   0.037139044517970975
train 75   0.03959379797783913
test 75   0.03356578039686334
train 100   0.03518477842323217
test 100   0.03090600931552213
train 125   0.032200120286767185
test 125   0.030485082276150998
train 150   0.03021968042397488
test 150   0.03032686666096824
train 175   0.028662096141698654
test 175   0.02970067103368658
train 200   0.027490620197085033
test 200   0.02941406375743508
train 225   0.026457086237633803
test 225   0.029290542332413256
train 250   0.025410301439335942
test 250   0.029280190276577616
train 275   0.02426035501703019
test 275   0.029381438775603338
train 300   0.022943500930758892
test 300   0.029661685325100214
train 325   0.021512039894337535
test 325   0.030225062151638973
train 350   0.02027136456823837
test 350   0.029744898403182167
train 375   0.019172606301168595
test 375   0.02936778938498663
train 400   0.01803531939502523
test 400   0.029270976997855525
